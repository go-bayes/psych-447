---
title: "Samples, paramaters, and elements of a linear model"
description: |
  "What is a statistical model?" 
author:
  - name: Joseph Bulbulia
    url: https://josephbulbulia.netlify.app
    affiliation: Victoria University of Wellington
    affiliation_url: https://www.wgtn.ac.nz
    orcid_id: 0000-0002-5861-2056
date: 2021-MAR-23
output:
  distill::distill_article:
  self_contained: false
toc: true
bibliography: references.bib
---

```{r figopen, echo=F}
knitr::include_graphics("op.png")
```
```{r}
# packages
# ```{r install_rethinking}
# function for installing dependencies
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}

# usage
packages <- c("coda", "plyr", "mvtnorm", "scales", "dagitty")
ipak(packages)

# next install rethinking
if (!require(rethinking)) {
  devtools::install_github("rmcelreath/rethinking")
}

# libraries
library("tidyverse")
library("patchwork")
library("brms")
library("lubridate")
library("splines")
# set theme
# theme set
theme_set(theme_classic())
```


```{r echo = FALSE}
# Import data
# read data
nz_0 <- readr::read_csv2(url("https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv"))

# to relevel kessler 6 variables
f<-c("None Of The Time","A Little Of The Time","Some Of The Time",  "Most Of The Time", "All Of The Time")

# get data into shape
library("tidyverse")
nz <- nz_0 %>%
  dplyr::mutate_if(is.character, factor) %>%
  select(
    -c(
      SWB.Kessler01,
      SWB.Kessler02,
      SWB.Kessler03,
      SWB.Kessler04,
      SWB.Kessler05,
      SWB.Kessler06
    )
  ) %>%
  dplyr::mutate(Wave = as.factor(Wave)) %>%
  dplyr::mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%
  dplyr::mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%
  dplyr::mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%
  dplyr::mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%
  dplyr::mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%
  dplyr::mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%
  dplyr::mutate(Wave = as.factor(Wave)) %>%
  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) %>%
  dplyr::mutate(height_m = HLTH.Height * 100,
         weight_kg =  HLTH.Weight) # better height vars
```

## Overview

In part 1 we will:

-   learn how to use R to generate random numbers
-   use random numbers to simulate data

## Learning outcomes

By learning how to generate random numbers you will better understand how statistical models help us to guess how the world is (which is statistical inference)

## `rnorm`

`rnorm` is a r's random number generator. Within this function:

-   `n`specifies the number of observations
-   `sd` specifies the value of the standard deviation
-   `mean` specifies the value of the mean

```{r}
set.seed(12345)
# generate random numbers
ds <- rnorm(n = 1000, mean = 0, sd = 1)
dplyr::glimpse(ds)
```

We can create a histogram:

```{r}
p1 <- ggplot2::qplot(ds) + labs(title = "1st random number list")
p1
```

We use shorthand for generating numbers:

```{r}
set.seed(54321)
ds_0 <- rnorm(1000)
dplyr::glimpse(ds_0)
```

Note that the first and the second graphs differ:

```{r layout="l-body-outset", fig.width=10, fig.height=10}
p2 <- ggplot2::qplot(ds_0) + labs(title = "2d random number list")
p1 + p2 + plot_annotation("The two graphs differ", tag_levels = 'i')
```

Or more formally we can ask R to test the equivalence:

```{r message=TRUE}
identical(ds, ds_0)
```

Because we want to have reproducible code, we will use the `set.seed()` function in R to ensure the same random numbers are generated each time.

```{r warning=TRUE}
set.seed(123)
t1 <-stats::rnorm(100)
set.seed(123)
t2 <-stats::rnorm(100)

# test
identical(t1, t2)
```

## `runif`

We use r uniform to generate continuous data within a point range

```{r}
set.seed(123)
ds1 <- runif(n =100, min = 0, max = 50)
dplyr::glimpse(ds1)
hist(ds1)
```

Say we want to simulate a range of values between two endpoints. This is useful for simulating explanatory variables.

```{r}
set.seed(123)
exp <- runif(n =100, min = 130, max = 220)
dplyr::glimpse(exp)
hist(exp)
```

## `rep`

Frequently we'll need to generate random factors. For this, R's `rep` function, `letters` function, and `LETTERS` function make happy friends.

Here's how these functions work

Lower case letters:

```{r}
letters[1:3]
```

Upper case letters:

```{r}
LETTERS[4:10]
```

Creating sequences using `each`

```{r}
rep( letters[1:3], each = 3 )
```


Creating sequences using `times`


```{r}
rep( letters[1:3], times = 3 )
```


Createing a sequences  using `length.out`

```{r}
rep( letters[1:3], length.out = 5 )
```

Creating uneven sequences

```{r}
rep( letters[1:3], times = c(3, 1, 4) )
```

combining `each` + `times`

```{r}
rep(letters[1:3], each = 2, times = 3)
```


`length.out`

```{r}
rep(letters[1:3], each = 2, length.out = 17)
```

Note `length.out` take priority over `times` -- use `length.out` if you have a fixed vector length. 


## `seq`


## Fake Empire

We can build a model using fake data

```{r}
set.seed(123)
height_sim = rnorm(n = 100, mean = 170, sd = 40)
weight_sim = runif(n = 100, min = 40, max = 100)
```

```{r}
m0 <- lm(height_sim ~ weight_sim)
summary(m0)
plot(ggeffects::ggpredict(m0, terms = c("weight_sim")), add.data = TRUE, dot.alpha = .1)

```

We can use vectors within random number generation

```{r}
set.seed(123)
vdf<-rnorm(n = 20, mean = c(0, 500, 1000), sd = c(5,50,100))
qplot(vdf, binwidth=4)
```

# Use fake data to explore a small sample

Can appear to reveal relationships that are not there.

```{r}
set.seed(123)
x = rnorm(n = 10, mean = 0, sd = 1)
y = rnorm(n = 10, mean = 0, sd = 1)

df<-data.frame(x,y)
ggplot2::ggplot(df,aes(y,x)) + geom_point() + geom_smooth(method=lm)
```

```{r}
### Simulate a relationship between two variables 
summary(lm(height_m ~ weight_kg, data = nz))

weight <-runif(N, min = 50, max =100)
b <- rlnorm(N,.78,.1)
b
sigma <- runif(N, 0 , 10 )
height = weight * b

height
# simulated height/ weight data
df<-data.frame(height,weight)

m0<-lm(height ~ weight, data = df)
plot(ggeffects::ggpredict(m0, terms = c("weight")), add.data = TRUE, dot.alpha = .8) + labs(x = "simulated weight",
                y = "simulated height",
                title = "simulated linear relationship")
```


```{r}
a <-160
b <- c(2, 0.75)
x <- rnorm(100)
y <- rnorm(100, mean = b[1] * exp(b[2] * x))
dat1 <- data.frame(x, y)
plot(y ~ x , data = dat1)

#Polynomial
N<-1000
options(scipens(999))
nz$weight_kg_c = scale(nz$weight_kg, center = TRUE, scale = FALSE)
summary(lm(height_m ~ weight_kg_c + I(weight_kg_c^2), data = nz))



# simulate weights
weight <-runif(N, min = 60, max =120)
weight_c <-scale(weight, scale=FALSE)
# simulate coefficients
a = rnorm(N, mean = 170 , 10 )
b1 = rnorm(N, mean = 2.5, .1)
b1
b2 = - rnorm(N, mean = .05, .001)

height <- a + b1 * weight_c  +  b2 * weight_c^2
height

# simulated height/ weight data
df1<-data.frame(height,weight_c, weight)

plot(height ~ weight)

m1 <-lm(height ~ weight_c, data = df1)
summary(m1)
plot(ggeffects::ggpredict(m1, terms = c("weight_c")), add.data = TRUE, dot.alpha = .2) + labs(x = "simulated weight",
                y = "simulated height",
                title = "simulated linear relationship") 

m2 <-lm(height ~ weight_c + I(weight_c^2), data = df1)
summary(m2)
plot(ggeffects::ggpredict(m2, terms = c("weight_c")), add.data = TRUE, dot.alpha = .2) + labs(x = "simulated weight",
                y = "simulated height",
                title = "simulated linear relationship")

m3 <-lm(height ~ bs(weight_c), data = df1)
summary(m3)
plot(ggeffects::ggpredict(m3, terms = c("weight_c")), add.data = TRUE, dot.alpha = .2) + labs(x = "simulated weight",
                y = "simulated height",
                title = "simulated linear relationship")

```

Check linear model
```{r}
performance::check_model(m1)
```

Check quadradic model

```{r}
performance::check_model(m2)
```

Check splines model

```{r}
performance::check_model(m3)
```


## outliers

```{r}
b <- c(2, 0.75)
set.seed(12)
x <- rnorm(100)
set.seed(12)
y <- rnorm(100, mean = b[1] * exp(b[2] * x))
dat1 <- data.frame(x, y)

ot1 <-lm(y ~ x, data  = dat1)
performance::check_model(ot1)

plot(ggeffects::ggpredict(ot1, terms = "x"), add.data =TRUE, dot.alpha = .4)

e

ot2 <-lm(y ~ bs(x), data  = dat1)
performance::check_model(ot2)

plot(ggeffects::ggpredict(ot2, terms = "x"), add.data =TRUE, dot.alpha = .4)

```


## Use fake data to create a factor

Here we simulate no relationship between a group and an outcome
```{r}
group <- rep(letters[1:3], length.out = 100)
outcome <- rnorm(n = 100, mean = 0, sd = 1)
df <-data.frame(outcome,group)
dplyr::glimpse(df)


#model
ms<-lm(outcome ~ group, data = df)

# no difference
sjPlot::plot_model(ms)
```



## Probability

Suppose there is a test that is 99% accurate at detecting COVID if you have it. 

Very rarely it throws up a false positive,say one in a thousand. 

You just tested positve.  What is the probability that you have COVID? 
Our intuition is that we probably have COVID. However, let's assume COVID is rare. Currently in NZ, there are about 50 cases, so 1 in 100,000.  The background rate matters. 

Bayes rule says

\[ Pr(COVID|Positive) = \frac{Pr(Positive|COVID)\times Pr (COVID}{Pr(Positive)}
\]

We plug in the numbers:

Pr_Positive_COVID <- 0.99
Pr_Positive_Healthy <- 0.01
Pr_COVID <- 0.00001

# Calculate the background probability of testing positive
Pr_Positive <- Pr_Positive_COVID * Pr_COVID +
Pr_Positive_Healthy * ( 1 - Pr_COVID )

## Point of chapter

# Now calculated your probability of testing positive
Pr_COVID_Positive <- Pr_Positive_COVID * Pr_COVID / Pr_Positive 
Pr_COVID_Positive


## Random number generation

#### `rnorm`

`rnorm(n, mean = 0, sd = 1)`

 - n = number of observarations 
 - sd = vector of standard deviations
 - mean = vector of means


rnorm(n = 20, mean = 0, sd = 1)
```

## Linear relationships with one co-variate

## Non-linear relationships with one co-variate



```{r}
nz1 <- nz %>%
m0<-lm(height_m ~ weight_kg + I(weight_kg^2), data = nz )
m1<-lm(height_m ~ bs(weight_kg),data = nz )
m2<-lm(height_m ~ weight_kg + I(weight_kg^2),data = nz )
m3 <- lm(height_m ~ weight_kg + poly(weight_kg, 2),data = nz )


plot(ggeffects::ggpredict(m1, terms = c("weight_kg")), add.data = TRUE, dot.alpha = .1)
summary(m1)
```

## Model summaries

```{r}
hist(nz$Issue.Food.GMO)
model <- lm(Sepal.Length ~ Species, data = iris)
report_text(model)
report_table(model)
```

Let's make a report of our data:

```{r}
library(tidyverse)
tnz<-nz%>%
  select(Id, Wave, KESSLER6sum)%>%
  filter(!is.na(KESSLER6sum))%>%
  pivot_wider(values_from = KESSLER6sum, names_from = Wave) %>%
  rename(k2018 = `2018`,
          k2019 = `2019`)

#this won't run
ihateTtests<-t.test(tnz$k2019 , tnz$k2018,  paired = TRUE)
report::report(ihateTtests)
```

The future is now. I'm going to give you a glimpse of a better method. Below is a multi-level model. Here, we assess whether people changed in their Kessler6 distress scores between the 2018 and 2019 waves of the `nz` study.

```{r, warning=FALSE}
library(lme4)
library(report)
fm1 <- lme4::lmer(KESSLER6sum ~ Wave + (1|Id), data = nz, family ="poisson")
report(fm1)
```

## Statistical inference and Scientific inference

## Science begins with a question.

What do you want to know?

## Theories, models, hypotheses, statistical models

### Theory

### Model

### Hypothesis

### Statistical model

```{r}
# This is a comment
y <- lm(Sepal.Length ~ Sepal.Width, data = iris)
```

## Statistics is the logic of uncertainty

Mathematics is the logic of certainty.

### Not all questions make sense

### How you ask a question can be misleading?

### Measurement

## Samples and parameters: observation and inference

## Acknowledgments

The approach to simulation presented here owes to:

1.  Ariel Muldoon: <https://aosmith.rbind.io/>

Ariel has a bunch of resources on her website, please check them out.

2.  Richard Mcelreath's Statistical Rethinking [@mcelreath2020]
3.  Regression and other stories [@gelman2020]
