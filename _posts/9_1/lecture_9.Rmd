---
title: "Ordinal responses, monotonic predictors, mediation, and group-level intercepts"
description: 
author:
  - name: Joseph Bulbulia
    url: https://josephbulbulia.netlify.app
    affiliation: Victoria University of Wellington
    affiliation_url: https://www.wgtn.ac.nz
    orcid_id: 0000-0002-5861-2056
date: 2021-MAY-04 
output:
  distill::distill_article:
    self_contained: false
    toc: true
    code_folding: true
    highlight: kate
bibliography: bib.bib
---

```{r echo=F, include = F}
knitr::include_graphics("op2.png")
```


```{r setup, include=FALSE}
# setup
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  layout = "l-body-outset",
  fig.width = 12,
  fig.height = 10,
  collapse = TRUE,
  R.options = list(width = 60
  )
)
```

```{r  libraries, echo=FALSE}
### Libraries
library("tidyverse")
library("ggplot2")
library("patchwork")
library("lubridate")
library("kableExtra")
library("gtsummary")
library("lubridate")
library("equatiomatic")
library("ggdag")
library("brms")
library("rstan")
library("rstanarm")
library("bayesplot")
library("easystats")
library("kableExtra")
library("broom")
library("tidybayes")
library("bmlm")
if (!require(tidyLPA)) {
  install.packages("tidyLPA")
}
# rstan options
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores ())
theme_set(theme_classic())
```

```{r  nzdata, include=FALSE, cache  = TRUE}
# read data


nz_0 <- as.data.frame(readr::read_csv2(
  url(
    "https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nzj.csv"
  )
))

# to relevel kessler 6 variables
f <-
  c(
    "None Of The Time",
    "A Little Of The Time",
    "Some Of The Time",
    "Most Of The Time",
    "All Of The Time"
  )

# Relevel covid timeline longitudional
# ord_dates_class <- c(
#   "Baseline",
#   "PreCOVID",
#   "JanFeb",
#   "EarlyMarch",
#   "Lockdown",
#   "PostLockdown")

# Relevel covid timeline 2019
ord_dates_class_2019_only <- c("PreCOVID",
                               "JanFeb",
                               "EarlyMarch",
                               "Lockdown",
                               "PostLockdown")
# get data into shape
nz_cr <- nz_0 %>%
  dplyr::mutate_if(is.character, factor) %>%
  select(
    -c(
      SWB.Kessler01,
      SWB.Kessler02,
      SWB.Kessler03,
      SWB.Kessler04,
      SWB.Kessler05,
      SWB.Kessler06
    )
  ) %>%
  dplyr::mutate(Wave = as.factor(Wave)) %>%
  dplyr::mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%
  dplyr::mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%
  dplyr::mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%
  dplyr::mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%
  dplyr::mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%
  dplyr::mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%
  dplyr::mutate(Wave = as.factor(Wave)) %>%
  dplyr::mutate(male_id = as.factor(Male)) %>%
  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) %>%
  dplyr::mutate(
    FeelWorthless_int = as.integer(FeelWorthless),
    FeelNervous_int =  as.integer(FeelNervous),
    FeelHopeless_int =  as.integer(FeelHopeless),
    EverythingIsEffort_int =  as.integer(EverythingIsEffort),
    FeelRestless_int =  as.integer(FeelRestless),
    FeelDepressed_int =  as.integer(FeelDepressed),
    HLTH.Fatigue_int = as.integer(HLTH.Fatigue + 1)
  ) %>%
  dplyr::mutate(yearS = TSCORE - min(TSCORE, na.rm = TRUE)) %>%
  dplyr::mutate(KESSLER6sum = as.integer(KESSLER6sum))



nz <- nz_cr %>%
  dplyr::filter(YearMeasured == 1) %>%
  dplyr::filter(Wave == 2019) %>%
  dplyr::group_by(Id) %>%
  dplyr::ungroup(Id) %>%
  dplyr::mutate(Covid_Timeline_cr =
                  as.factor(ifelse(
                    TSCORE %in% 3896:3921,
                    # feb 29 - march 25th
                    "EarlyMarch",
                    ifelse(
                      TSCORE %in% 3922:3954,
                      "Lockdown",
                      #march 26- Mon 27 April 2020
                      ifelse(
                        TSCORE > 3954,
                        # after april 27th 20202
                        "PostLockdown",
                        ifelse(TSCORE %in% 3842:3895,
                               # jan 6 to feb 28
                               "JanFeb",
                               "PreCOVID")
                      )
                    )
                  ))) %>%
  dplyr::mutate(Covid_Timeline_cr = forcats::fct_relevel(Covid_Timeline_cr, ord_dates_class_2019_only))

dplyr::glimpse(nz)


## Long data #####################
ord_dates_class <- c("Baseline",
                     "PreCOVID",
                     "JanFeb",
                     "EarlyMarch",
                     "Lockdown",
                     "PostLockdown")

nzl <- nz_cr %>%
  dplyr::filter(YearMeasured == 1) %>%
  dplyr::filter(Wave == 2018 | Wave == 2019) %>%
  dplyr::group_by(Id) %>%
  dplyr::filter(n() > 1) %>%
  dplyr::filter(n() != 0) %>%
  dplyr::ungroup(Id) %>%
  dplyr::mutate(yearS = TSCORE - min(TSCORE, na.rm = TRUE)) %>%
  dplyr::mutate(WSCORE = as.factor(WSCORE)) %>%
  dplyr::mutate(Covid_Timeline =
                  as.factor(ifelse(
                    TSCORE %in% 3896:3921,
                    # feb 29 - march 25th
                    "EarlyMarch",
                    ifelse(
                      TSCORE %in% 3922:3954,
                      "Lockdown",
                      #march 26- Mon 27 April 2020
                      ifelse(
                        TSCORE > 3954,
                        # after april 27th 20202
                        "PostLockdown",
                        ifelse(
                          TSCORE %in% 3842:3895,
                          # jan 6 to feb 28
                          "JanFeb",
                          ifelse(TSCORE %in% 3665:3841 &
                                   Wave == 2019,
                                 "PreCOVID",
                                 "Baseline"  # 3672 TSCORE or  20 July 2019))))))))
                          )
                        )
                      )
                    ))))%>%
  dplyr::mutate(Covid_Timeline = forcats::fct_relevel(Covid_Timeline, ord_dates_class))
                
glimpse(nzl)
```


## Objectives

To develop the following skills:

  - modelling ordinal reponses
  - modelling monotonic predictors
  - estimating predictors of the variance as well as of the mean response.
  - estimating group-level (aka "random") intercepts. 
  - estimating multivariate outcomes (i.e. two or more)
  - estimating a mediation model 
  - interpret and report the results of these models using graphical methods.


## Ordinal responses 

An ordinal model is a cumulative logistic model. Instead of binary outcomes, we assume there are n > 2 outcomes \cite{Burkner2019-ie,Hadfield2012-hi}. 


<!-- In our NZ-jitter dataset, the Kessler-6 has five ordinal response options for indicating frequency of feeling distress during the past 30 days: "None Of The Time", "A Little Of The Time", "Some Of The Time", "Most Of The Time", "All Of The Time" hence six thresholds.  -->

<!-- The Kessler-6 distress indicators are: "Feeling Hopelessness;" "Feeling so Depressed Nothing Could Cheer You Up;" "Feeling Nervous or Fidgety;" "Feeling Everything is an Effort;" "Feeling Worthless;" "Feeling Nervous." We simultaneously modelled each of these six outcomes within a single model.  -->

Consider the `HLTH.Fatigue` indicator from the NZAVS jitter dataset.  The item reads:


> "During the past 30 days have you felt fatigue... "

and the repsonses are:

  > "None Of The Time", 

  > "A Little Of The Time", 

  > "Some Of The Time",

  > "Most Of The Time", 
  
  > "All Of The Time." 


A priori, we might think that Fatigue is better approached as an ordered categorical indicator rather than as a continuous indicator:

```{r}
hist(nzl$HLTH.Fatigue)
```



We can write the model equation for an ordinal (or cumulative logistic) model of Fatigue s follows

$$\begin{align}
y_{i}^c \sim \text{Ordered}(p_i) \\
\text{CumLogit}(\boldsymbol{p_i}) = \alpha^c +\beta X_i \\
\alpha^c \sim \text{StudentT}(3,0,10)\\
\boldsymbol{\beta}\sim \text{Normal}(0,1) 
\end{align}$$



For an ordinal response model, where there are $R$ responses for our outcome indiator, there are $C = R-1, c = 1\dots C$  thresholds. 

As with binary logistic regression, residuals for cumulative logistic models are not identified, so are fixed to 1. $\alpha^c$ denotes the $c^{th}$ intercept estimated in the model. Where the lowest response level is modeled as zero, hence four intercepts are estimated. $\mathbf{\beta}$ is the esitmate for the$X$ parameter measured on $i...N$ individuals.  In this case the time categories of the `Covid_Timeline` indicator.


```{r cache = TRUE}
system.time(
  m1 <- brm(
    bf(HLTH.Fatigue_int  ~
      Covid_Timeline_cr),
    family = cumulative(link = "logit"),
    data = nz,
    file = here::here("models", "ordinal_fatigue"),
    silent = FALSE
  )
)
par_m1 <-parameters::model_parameters(m1)
par_m1
plot(par_m1)
```

Hre we can see that the log-odds of Fatigue dropped during Lockdown.  But how did Fatigue drop? 

As with logistic regression, the coefficients are *not* obvious. We need to graph the predicted outcomes at specific values. 


### Interpreting results of an ordinal model using graphical methods

To understand what is happening, we plot all the individual intercepts by setting `categorical = TRUE`. Following McElreath's Statistical Rethinking, we will predict outcomes using the .89 probability credible interval:


```{r}
plot(
  conditional_effects(
    m1,
    categorical = TRUE,
    prob = 0.89,
    re_formula = NA,
  ),   # WE cannot graph points when the points arg = TRUE
  points = TRUE,
  point_args = list(
    alpha = 0.1,
    width = .1,
    size = .2
  )
) 
```

Here we see the separation occuring during `Lockdown` as compared with the baseline `PreCOVID`


This is another method for graphing, where the individual facets denote our expected response thresholds:


```{r eval = FALSE}
plot(ggeffects::ggpredict(
  m1, 
  effects = "Covid_Timeline_cr")
  )
```


Finally, for the purposes of exploration, we can obtain an average response using the BRMS `conditional_effects` function, with the setting `categorical = FALSE`


```{r}
plot(
  conditional_effects(
    m1,
    categorical = FALSE,
    prob = 0.89,
    re_formula = NA,
  ),
  points = TRUE,
  point_args = list(
    alpha = 0.1,
    width = .1,
    size = .2
  )
) 
```


## Ordinal (or monotonic) predictors

Reference, here (https://psyarxiv.com/9qkhj/)  also the BRMS documentation [here]( https://cran.r-project.org/web/packages/brms/vignettes/brms_monotonic.html)

> A predictor, which we want to model as monotonic (i.e., having a monotonically increasing or decreasing relationship with the response), must either be integer valued or an ordered factor. As opposed to a continuous predictor, predictor categories (or integers) are not assumed to be equidistant with respect to their effect on the response variable. Instead, the distance between adjacent predictor categories (or integers) is estimated from the data and may vary across categories.


### Model equation for a monotonic predictor model:


For an ordinal predictor model, where there are $R$ indicator responses, there are $C = R-1, c = 1\dots C$ thresholds to estimate for the the ordered reponses predictors

- $b^c$ locates the direction and size of effect, as with an ordinary regression solution. We consider  $b^c$ as the expected average difference between two adjacent categories of the ordinal predictor.
- $\zeta^c$ locates the normalized distances between consecutive predictor categories, producing the shape of the monotonic effect



$$g(y_i) = \alpha + b^c \zeta^c $$ 


Suppose we wanted to assess whether fatigue predicts psychological distress. We'll use a smaller sub-sample of the NZAVS jitter dataset to speed up computing time:


```{r cache = TRUE}
# Create small sample to improve computing time
snzl <- nzl %>%
  dplyr::filter(Wave == 2019) # Only one wave
set.seed(123)
nm <-
  sample(snzl$Id, size = 300) # randomly select a smaller sample of individuals.
sub_nzl <- snzl %>%
  filter(Id %in% nm)
```


First we estimate a model without a monotonic effect: 

```{r cacht = TRUE}
# ordinary predictor
mo_0 <- brm(
  bf(KESSLER6sum  ~ HLTH.Fatigue,
     family = "poisson"),
  data = sub_nzl,
  file = here::here("models", "monotonic_0"),
  silent = FALSE
)

# table
par_mo_0 <-parameters::model_parameters(mo_0)
par_mo_0

# coefficient plot
plot(par_mo_0) + labs(title = "Distress on Fatigue, no monotonic effects")
```


The expected increase in distress from a one unit increase in Fatige is .45 units of Kessler 6 distress on the log scale. 

To translate this expectation to the datascale recall that we must exponentiate our result

A one unit change in fatigue at the intercept leads to an expected change in distress of:

```{r}
exp(.75 + .45)
```

We plot the entire range of the response scale: 

```{r}
p_mo_0 <- plot(
  conditional_effects(mo_0),
  points = TRUE,
  point_args = list(alpha = 0.1,
                    width = .2), 
  ask = FALSE
)[[1]]
```


We next model the relationship of Fatigue on Distress by thinking of Fatigue as as a monotonic predictor, using the `mo` command, as follows:

```{r cache = TRUE}
mo_1 <- brm(
  bf(KESSLER6sum  ~ mo(HLTH.Fatigue),
     family = "poisson"),
  data = sub_nzl,
  file = here::here("models", "monotonic_1")
)

# table
par_mo_1 <-parameters::model_parameters(mo_1)
par_mo_1

# coefficient plot
plot(par_mo_1) + labs(title = "Distress on Fatigue, monotonic effects") 
```

Note that we have 4 x betas for Fatigue, which each threshold of Fatigue receiving its own beta. 

Graphing the results:

```{r cache = TRUE}
p_mo_1 <- plot(
  conditional_effects(mo_1),
  points = TRUE,
  point_args = list(alpha = 0.1,
                    width = .2)
)[[1]] + labs(title = "Predicted Distress by Fatigue: monotonic effects model")
p_mo_1
```

Compare our models, in this instance it appears that we do not require predicting Distress by Fatigue using monotonic effects:

```{r cache = TRUE}
compare_mo_0 <- add_criterion(mo_0, "loo")
compare_mo_1 <- add_criterion(mo_1, "loo")

compare_mo <-
  loo_compare(compare_mo_0, compare_mo_1, criterion = "loo")
compare_mo
```

We can visually inspect the posterior predictions, and they not look too different: 

Without monotonic predictors

```{r}
brms::pp_check(mo_0)
```


With monotonic predictors 

```{r}
brms::pp_check(mo_1)
```

The reason that we can get away without a monotonic predictor here is that for a poisson model, we are exponentiating the predictions of our regression coefficients. 

However in other instances a monotonic predictor might be preferred. There are no defaults.

Modelling requires a combination of reasoning and art. A modern data scientist knows there are no back-of-the-book answers instructing us what to do. Rather, we must be explicit about our decisions. 


## Distributional model

Paul Bruckner, the author of the BRMS package, offers the following distributional model (see the explanation {here](https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html))


```{r}
group <- rep(c("treat", "placebo"), each = 30)
symptom_post <-
  c(rnorm(30, mean = 1, sd = 2), rnorm(30, mean = 0, sd = 1))
dat1 <- data.frame(group, symptom_post)
head(dat1)
```

Note that you can fit a model in which the variation in reponse, and not merely the average response, is estimated. Paul does so as follows:

```{r cache = TRUE, message = FALSE, warnings = FALSE}
fit1 <- brm(bf(symptom_post ~ group,
               sigma ~ group),
            data = dat1,
            file = here::here("models", "distributional"),
            family = gaussian())

par_fit1 <-parameters::model_parameters(fit1)
par_fit1

# coefficient plot
plot(par_fit1) 
```



Here we have the prediction plot

```{r}
plot(conditional_effects(fit1), points = TRUE)
```

We can see the variance in the treatment group:

<!-- BRMS allows us to estemate whether both parameters (the mean) and the standard deviations of the mean, differ: -->

<!-- ```{r} -->
<!-- hyp <- c("exp(sigma_Intercept) = 0", -->
<!--          "exp(sigma_Intercept + sigma_grouptreat) = 0") -->
<!-- hypothesis(fit1, hyp) -->
<!-- ``` -->

<!-- Both the mens, and the variances, differ.  -->

<!-- We can graph these differences: -->

<!-- ```{r} -->
<!-- hyp <- "exp(sigma_Intercept + sigma_grouptreat) > exp(sigma_Intercept)" -->
<!-- (hyp <- hypothesis(fit1, hyp)) -->
<!-- plot(hyp) -->
<!-- ``` -->

When might this model apply? 

> Suppose we have two groups of patients: One group receives a treatment (e.g., an antidepressive drug) and another group receives placebo. Since the treatment may not work equally well for all patients, the symptom variance of the treatment group may be larger than the symptom variance of the placebo group after some weeks of treatment. (Paul Bruckner, BRMS vignette)



## Random intercept model (clustering)

See: https://m-clark.github.io/mixed-models-with-R/


Let's look at environmental concern as predicted by religious group membership:

```{r}
# 506 weekss betwen 530 June 2009 and 15 March 2019
nzl_11 <- nzl %>%
  dplyr::filter(Wave == 2019)

ri_relgroups <- brm(
  bf(Env.ClimateChgConcern    ~
       (1 | Religious_Group)),
  data = nzl_11,
  file = here::here("models", "ri_relgroups"),
  silent = FALSE
)

summary(ri_relgroups)
```

### Model equation for a random intercept model


$$ g(y_{ig}) \sim N(\mu_{ig}, \sigma)\\
\mu_i = \boldsymbol{\alpha} + \beta x_{ig}\\
 \boldsymbol{\alpha} = \alpha_o + \alpha_g\\
 \alpha_o \sim N(3.5,10)\\
 \alpha_g \sim N(0,\sigma_g)\\
 \sigma_g \sim Exp(1)\\
  \sigma \sim Exp(1)
$$




We can calculate the intraclass correlation coefficient for religious groups, which is the ration of group variance devited by the total variance or in this case:

$$\frac{\sigma^2_{religious groups}}{\sigma^2_{religious groups} + \sigma^2_{indiviuals}}$$

```{r}
performance::icc(ri_relgroups)
```

The authors of the performance package write (performance package documentation)

> While the adjusted ICC only relates to the random effects, the conditional ICC also takes the fixed effects variances into account (see Nakagawa et al. 2017). Typically, the adjusted ICC is of interest when the analysis of random effects is of interest. 


We can plot the group-level variances:

```{r}
par_ri_relgroups <-parameters::model_parameters(ri_relgroups,
                                             effects = "random")
plot(par_ri_relgroups,   sort = TRUE) 
````


## Random intercept: Fatigue

With longitudinal data, we have repeated measures within individuals. This leads to clustering. We can adjust for this clustering by including group-level intercept for individuals. 

Let's return to the Fatigue ~ Covid_Timeline model. 


$$\begin{align}
y_{ij}^c \sim \text{Ordered}(\mu_{ij}^c) \\
\text{CumLogit}(\mu_{ij}^c) = \boldsymbol{\alpha}^c +\beta X_i \\
\boldsymbol{\alpha}^c  = \alpha_{0}^c +\alpha_j\\
\alpha_{0}^c \sim N(0,10)\\
\alpha_j \sim N(0,\sigma_j)\\
\sigma_j \sim exp(1)\\
\boldsymbol{\beta}\sim \text{Normal}(0,1) \\
\end{align}$$




```{r}
system.time(
  m1_l <- brm(
    bf(HLTH.Fatigue_int  ~
      Covid_Timeline + (1|Id),
    family = cumulative(link = "logit")),
    data = nzl,
    file = here::here("models", "ordinal_fatigue_longitudinal"),
    silent = FALSE
  )
)
summary(m1_l)
```


```{r cache = TRUE}
par_m1_l <-parameters::model_parameters(m1_l)
par_m1_l
plot(par_m1_l)

plot(
  conditional_effects(
    m1_l,
   # spaghetti = TRUE,
  #  nsamples = 100,
    categorical = F,
    prob = 0.89,
    re_formula = NA,
  ),
  points = TRUE,
  point_args = list(alpha = 0.1,
                    width = .02)
) #  note this command controls which facet 

plot(ggeffects::ggpredict(m1_l), add.data= F, dot.alpha = .1)
```





## Multiple response models 




### Model equation

$$ g(y^n_{ig}) \sim N(\mu^n_{ig}, \sigma^n)\\
\mu^n_i = \boldsymbol{\alpha^n} + \beta^n x_{ig}\\
 \boldsymbol{\alpha} = \alpha_o + \alpha_g\\
 \alpha^n_o \sim N(3.5,10)\\
 \alpha^n_g \sim N(0,\sigma_g^n)\\
 \sigma_g^n \sim exp(1)\\
 \sigma^{n_1} \sim exp(1)\\
 \sigma^{n_2}\sim exp(1) \\
\begin{bmatrix}
\sigma^{n_1} \\
\sigma^n_2 
\end{bmatrix}
\sim 
\boldsymbol{RESCOR} \begin{pmatrix}
1 &  \sigma^{n_1}\sigma^{n_2} \rho    \\
\sigma^{n_1}\sigma^{n_2}\rho   &  1
\end{pmatrix}\\
\boldsymbol{RESCOR}\sim \text{LKJcorr}(2)
$$


```{r}

# Create smaller data frame
snzl <- nzl %>%
  dplyr::filter(Wave == 2019) # Only one wave

set.seed(123)
nm <- sample(snzl$Id, size = 300) # randomly select a smaller sample of individuals. 

# create the data frame
sub_nzl<- snzl%>%
 filter(Id %in% nm)


f_mv <- brm(
  mvbind(Warm.Immigrants, Warm.Muslims) ~  log(Hours.News + 1),
  data = sub_nzl,
  file = here::here("models", "multivariate_warmth"), 
)
summary(f_mv)
```

Coefficient plot


```{r}
brms::mcmc_plot(f_mv, 
               type = "areas",
               prob = .89)
```




##  Mediation

-   Manipulate X, measure M and Y
-   Regress M on X; Y on X and M

```{r, layout = "l-body-outset", cache =TRUE}
bmlm::mlm_path_plot(xlab = "Condition\n(X)",
              mlab = "Mediator\n(M)",
              ylab = "Distress\n(Y)")
```

## Assumptions

-   Y does not affect M
-   No 3rd variable on M to Y relationship
-   M is measured without error
-   Y and M residuals are not correlated [@vuorre2020multilevel]


## Set up

```{r echo = TRUE, eval = FALSE}
path_m <- bf(
  mF ~ x + (1 | c | id)
  )
path_y <- bf(
  y ~ x + mF_w + mF_b +
               (1 | c | id)
  )
m1 <- brm(
  path_m + path_y + set_rescor(FALSE),
  data = datF,
  file = here("models/mediation-k6-covid-fatigue")
)
```


## Model form 

```{r echo = FALSE}
x <-nzl$Covid_Timeline
mF <-nzl$HLTH.Fatigue_int
y<- nzl$KESSLER6sum
id <-nzl$Id
datF <-data.frame(x,mF, y,id)
```

```{r echo = FALSE}
path_m <- bf(
  mF ~ x + (1 | c |  id)
  )
path_y <- bf(
  y ~ x + mF +  (1 | c |  id)
  )
f1 <- brm(
  path_m + path_y + set_rescor(FALSE),
  data = datF,
  file = here::here("models/mediation-k6-covid-fatigue")
)

summary(f1)
```



```{r echo=FALSE, cache = FALSE}
post1F <- brms::posterior_samples(f1)
post_marF <- post1F %>% 
  transmute(
    a = b_mF_xLockdown,
    b = b_y_mF,
    cp = b_y_xLockdown,
    me = a * b,
    c = cp + me#,
   # pme = me / c
  )
# posterior_summary(post_marF)
```


```{r}
mcmc_intervals(post_marF)
```

## Hypothesis 

## Excercise: March vs. L4

```{r}
h1 <- c(
  a = "mF_xLockdown  = 0",
  b = "y_mF = 0",
  cp = "y_xLockdown = 0",
  me = "mF_xLockdown * y_xLockdown = 0",
  c = "mF_xLockdown * y_mF + y_xLockdown = 0"
)

plot(
  hypothesis(f1, h1)
)

```

## Bonus: Latent Profile Analysis

## Latent Profile Analysis

```{r  cache = TRUE}
library(tidyLPA)
# nzf<-nz %>%
#   select( FeelHopeless, 
#           FeelDepressed,
#           FeelRestless, 
#           EverythingIsEffort,
#           FeelWorthless, 
#           FeelNervous) %>%
#   mutate_if(., is.factor, ~ as.numeric(as.integer(.x)))

# convert to standard deviation units

out<-nzl %>%
  dplyr::select(Hours.Internet, 
          Hours.Exercise,
       #   Hours.CompGames, 
       #   Hours.News,
          Hours.Work)%>%
  dplyr::mutate_all(., scale)

out%>%
  single_imputation() %>%
  tidyLPA::estimate_profiles(3) %>%
    plot_profiles(add_line = TRUE)
```
