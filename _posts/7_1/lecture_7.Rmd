---
title: "Multiple regression (ANOVA, ANOVCA, MANOVA)"
description: 
author:
  - name: Joseph Bulbulia
    url: https://josephbulbulia.netlify.app
    affiliation: Victoria University of Wellington
    affiliation_url: https://www.wgtn.ac.nz
    orcid_id: 0000-0002-5861-2056
date: 2021-APRIL-20 
output:
  distill::distill_article:
    self_contained: false
    toc: true
---

```{r echo=F}
knitr::include_graphics("op_A.png")
```


```{r echo = TRUE, include = FALSE}
# packages
# ```{r install_rethinking}
# function for installing dependencies
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE) 
sapply(pkg, require, character.only = TRUE)
}
# usage
packages <- c("coda", "plyr", "mvtnorm", "scales", "dagitty")
ipak(packages)

# next install rethinking
if (!require(rethinking)) {
  devtools::install_github("rmcelreath/rethinking")
}

#next install ggdag
if (!require(rethinking)) {
  devtools::install_github("malcolmbarrett/ggdag")
}

if (!require(rethinking)) {
  devtools::install_github("larmarange/labelled")
}
# installed from previous lectures
library("equatiomatic")
library("tidyverse")
library("ggdag")
library("brms")
library("rstan")
library("rstanarm")
# library("tidybayes")
library("bayesplot")
library("easystats")
library("kableExtra")
library("broom")
# rstan options
rstan_options(auto_write=TRUE)
options(mc.cores=parallel::detectCores ())
theme_set(theme_classic())
```


## Learning objectives

Before the break, we introduced basic concepts in statistical regression, focusing on regression using a single covariate. 

Today, we will introduce regression using multiple co-variates. You will acquire the following skills: 

1. Understand how to write a regression model with multiple co-variates in R. 

2. Understand how to interpret a regression model with multiple co-variates in R. 

3. Understand methods for comparing simple and complex models. 

4. Understand how to create a table for your results. 

5. Understand how to graph your results. 

6. Understand how to write equivalent models in an ANOVA framework, in case someone forces you to do this. 


## What is a regression model?

In the first instance, linear regression is a method for predicting features of the world, or as we put it in lecture 5:

> [R]egression ...  is method for inferring the expected average features of a population, and the variance of a population, conditional on other features of the population as measured in a sample.

In week 9, we'll discussion how to use regression for explaining features of the world. However, for the next two weeks, we'll restrict our interests to prediction.


To referesh your memory about how the magic of prediction by regression works, let's look at an example. Consider data collected in the 1920s on the relationship between the speed at which a car is travelling and the distance that it takes for that car to stop. The data are found in the `cars` dataset, which is automatically loaded when you start R.

Here is our model:

```{r}
model_simple <- lm(dist ~ speed, data = cars)
```

Let's look at the output.  You can create a table like this: 

```{r}
parameters::parameters(model_simple)
```


Recall that we can create an html table in this way

```{r}
parameters::parameters(model_simple)%>%
    parameters::print_html(caption = "Breaking distance as predicted by speed")

```

## How do we interpret the results of a regression model?


In terms of workflow, recall the report package makes your life easy:

```{r}
report::report(model_simple)
```

However, before you go rushing to report your "signficant" p-value, what does any of this output mean?  

Recall that can write the output of a model as an equation. The `equatiomatic` package in R makes this work easy for you.

```{r eval = FALSE, include = FALSE}
# this is how to quickly generate the equation
equatiomatic::extract_eq(model_simple,  use_coefs = TRUE)
```

$$
\operatorname{\widehat{dist}} = -17.58 + 3.93(\operatorname{speed})
$$

The generic form of the a linear model is:

```{r eval = FALSE, include = FALSE}
# this is how to quickly generate the equation
equatiomatic::extract_eq(model_simple,  use_coefs = FALSE)
```
```{r eval=FALSE, include = FALSE}
emojifont::search_emoji('thinking')
```


The model says that the expected stopping distance for a car is -17.58 feet when the speed of the car is set to zero, plus an additional 3.93 feet for each additional unit of speed (here in miles per hour).

What strikes you about this model?

If you are like me you probably feel confusion when you see the number "-17.58" predicting speed `r emojifont::emoji('astonished')`. Did car manufacturers of the 1920s invent a method for traversing space and time as we know it `r emojifont::emoji('thinking')`? Or is regression a hopeless tool for understanding the world and should you demand your money back for this course `r emojifont::emoji('crying_cat_face')`?

Let's look at the data more carefully:

```{r}

cars%>%
  dplyr::arrange(speed)%>%
  tibble()
```

Here we find that the lowest speed measured in this dataset is 4 miles per hour, at which distance the car stopped in 2 feet. Another car travelling at 4 mph took 10 feet to stop.

Let's plug these two numbers into the regression equation that we just estimated.  We do not need to copy and paste coefficients from the output, you can recover the interecept as follows:

```{r eval}
coef(model_simple)[[1]]
```


And for each coefficient thereafter just add the relevant index as follows:

```{r}
coef(model_simple)[[2]]
```

So we can write the expected (or predicted) speed for a car travelling at the maximum speed in the dataset (`r max(cars$speed, na.rm = TRUE)` as follows:


```{r}
coef(model_simple)[[1]] + coef(model_simple)[[2]] * max(cars$speed, na.rm = TRUE)
```

and for the minimum speed (`r min(cars$speed, na.rm = TRUE)`) as follows:

```{r}
coef(model_simple)[[1]] + coef(model_simple)[[2]] * min(cars$speed, na.rm = FALSE)
```

Is this any better? We're still getting a negative speed.


Let's turn to the most useful tool in your R toolkit, your graph. We can plot the model as follows:

```{r}
ggplot2::ggplot(data = cars, 
                aes( x = speed,  y = dist )) +
  geom_smooth(method = "lm") + 
  geom_point()
```

Here, our linear model is minimising the average distance between between observed speed and observed distance in the sample. The model hits at most a few points in the dataset. Otherwise it estimates a response that is either too high or two low. 

Note that by allowing our predictor variable to start at 0 we render the intercept term `r coef(model_simple)[[1]]` in our model uninterpretable. There is no coherent concept we can attach to  an expected stopping distance from an expected speed of 0 mp, and the value 0mph never occurs in the dataset. 

For this reason, it is very helpful to center all your continuous predictors. In such a model, the intercept is interpretable as the expected response when all continuous predictors are set to their sample average, which in this case is `r mean(cars$speed)`mph.

Recall that we can center our variables using the following code

```{r}
# center and create new dataframe
schmars <- cars

# not that I am presently having trouble with dplyr output. The following code will allow me to graph results (which I found by searching stackoverflow.com)

schmars['speed_c'] = as.data.frame(scale(schmars$speed, scale = FALSE) )# work around for a bug - center and convert to decade units
schmars$speed_c
# model
model_simple2 <- lm(dist ~ speed_c, data = schmars)

# graph
parameters::parameters(model_simple2)%>%
  print_html()
```

Notice that the estimated coefficient in for speed is this second model is the same as the estimated coefficient for speed in the first model:

```{r}
# evaluate whether the two coefficients are the same at least to five decimal places
round(coef(model_simple)[[2]],5) == round(coef(model_simple2)[[2]],5)
```

And the graph is identical:

```{r}
ggplot2::ggplot(data = cars, 
                aes( x = speed,  y = dist )) +
  geom_smooth(method = "lm") + 
  geom_point()
```


However, the intercept is now interpretable as the expected (or predicted) outcome when speed is set to its sample average (25mph)

```{r}
round(coef(model_simple2)[[1]],5)
```

We interpret the coefficient for speed as the expected increase or decrease in stopping distance for a one-unit change in speed (which was measured in miles per hour in the population from which this sample was collected.  For a car travelling 3 mph faster than the sample average we would expect an average stopping distance of:


```{r}
coef(model_simple2)[[1]] + coef(model_simple2)[[2]] * 3
```


And for a car travelling 10.33 mph slower than the sampling average we would expect an average stopping distance of:

```{r}
coef(model_simple2)[[1]] + coef(model_simple2)[[2]] - 10.33
```


Recall that you can create a nice prediction plot using the `ggeffects` package:

```{r}
# graph espected means
plot(ggeffects::ggpredict(model_simple2, terms = "speed_c [all]"), 
     add.data = TRUE, 
     dot.alpha = .8, 
     jitter = .01, 
     ci.style = "errorbar") + 
  ylab("Distance in feet") + 
  xlab("Speed in MPH (centered at 25 mph") + 
  labs(title = "Predicted average stopping distance by speed for cars in the 1920s",
       subtitle = "Speed centered at 25 MPH") + theme_classic()
```

or another method:


```{r}
# graph espected means
plot(ggeffects::ggpredict(model_simple2, terms = "speed_c [all]"), 
     add.data = TRUE, 
     dot.alpha = .8, 
     jitter = .01) + 
  ylab("Distance in feet") + 
  xlab("Speed in MPH (centered at 25 mph") + 
  labs(title = "Predicted average stopping distance by speed for cars in the 1920s",
       subtitle = "Speed centered at 25 MPH") + theme_classic()
```






The essential take home messages so far:

- inspect your data
- center your continuous predictors (standardise? maybe...)
- graph your model
- generate a prediction plot





## Multiple regression 

We have already encountered multiple regression. Perhaps the most familiar case is prediction. Suppose we want to predict someones expected income based on their gender identification. Suppose we theory is true there might be an added income advantage from height.  We can include an additional term in our model and evaluate whether or theory is true.   Let's assess this question using the jittered NZAVS dataset.

```{r echo = FALSE, cache = TRUE}
nz_0 <- as.data.frame(readr::read_csv2(url("https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nzj.csv")))
```

```{r echo = FALSE, cache=TRUE}
df<- nz_0 %>%
  dplyr::filter(Wave == 2019) %>%
  dplyr::mutate(age = Age) %>%
  dplyr::select(age, HLTH.Height, Household.INC, Male, Employed, Edu, Id, Wave) %>% # collect variables
  dplyr::filter(Employed == 1) %>% # restrict inference to people who are employed
  dplyr::rename(income = Household.INC, # shorter names
                not_male = Male) 

df['height_cm_c'] = as.data.frame(scale(df$HLTH.Height, center = TRUE, scale = FALSE)*1000) # work around for a bug - center and convert meter units to cmd
# not working
df['age_10_c'] = as.data.frame(scale(df$age, center = TRUE, scale = FALSE)/10) # work around for a bug - center and convert to decade units

df['edu_s'] = as.data.frame(scale(df$Edu) )# work around for a bug - center and convert to decade units

```


Here is a regression model with two predictors. We have centered height to the population average. The outcome is expected household income for the NZ population simultaneously stratified by height and by male identification. (Question: can you think of why household income is not an ideal indicator in a setting where we are looking at gender differences...? We'll come back to this question in a few weeks.) 

```{r}
# model
model_theory <- lm(income ~ height_cm_c + not_male, data = df)
sjPlot::tab_model(model_theory)
```

How do we interpret this model?  First we need the average sample height (across all gender identifications):

```{r}
mean(nz_0$HLTH.Height, na.rm=TRUE)
```


The model tells us the following. The expected household income for the population of men who are at the sample average height is:  `r coef(model_theory)[[1]]`.The expected increase in income for each additional centimeter of height whether or not one is male or not-male identified is `r coef(model_theory)[[2]]`. The expected reduction in income for people who do not identify as male is `r coef(model_theory)[[3]]`.

Are men expected to make `r coef(model_theory)[[3]]` more than woman? 

No because on average non-male is shorter and we are stratifying across male and non-male gender identifications. If we focus only on the gender difference we can include male-id and remove height and this gives us: 

```{r}
model_theory2 <- lm(income ~ not_male, data = df)
sjPlot::tab_model(model_theory2)
```

Thus the expected not-male income is 153753.08	- 28349.98 = or about $NZD 125,400.

We can recover this expectation in the original model by adding the difference between the sample averages in male heights over not-male heights as follows: 

Find approx deviations in male height and female heights from sample:

```{r}
df%>%
  group_by(not_male)%>%
  summarise(genmean = mean(HLTH.Height, na.rm = TRUE))
# difference 
df%>%
  summarise(mean = mean(HLTH.Height, na.rm = TRUE))
```


Plug in expected values for male Id:

```{r}
coef(model_theory)[[1]] + # intercept at average height
  coef(model_theory)[[2]]* 83 + #increase in average male height over sample average in cm
  coef(model_theory)[[3]]* 0 # code for male id = 0
```

And for not-male Id:

```{r}
coef(model_theory)[[1]] + coef(model_theory)[[2]]* - 47  + coef(model_theory)[[3]]*1
```

These are the equivalent expectations to the stratified models.

Either way you cut it, we find that women (and others who do not identify as male) are are expected to make `r coef(model_theory2)[[2]]` less in household income than men. 

## Multiple regression with more than than two covariates


Note that regression allows us to stratify across other segments of a population by adding even more co-variates. 

Let's add education to the model. In the NZAVS education `Edu` is a 10-level factor.  However, for now, we'll think of it as numeric. A standard deviation of `Edu` is `r sd(df$Edu, na.rm = TRUE)` out of `r min(df$Edu, na.rm = TRUE)` to `r max(df$Edu, na.rm = TRUE)`.  Adding `edu_s` to the model we find:

```{r}
model_theory3 <- lm(income ~ height_cm_c + not_male + edu_s, data = df)
sjPlot::tab_model(model_theory3)
```

Do height differences predict difference greater income differences ?  To assess this we add an interaction term. **NOTE: we should always center our interaction terms for the same reason that we should center polynomials: otherwise the multi-collinearity renders the model unstable**


## Multiple regression with interactions

```{r}
# model
model_theory4 <- lm(income ~ height_cm_c * not_male, data = df)
```

Note that this model is equivalent to:

```{r eval = FALSE}
model_theory4 <- lm(income ~ height_cm_c + not_male + not_male:height_cm_c, data = df)
```


The `*` in the model is merely shorthand for the two main effects `height_cm_c + not_male` $+$ their interaction `not_male:height_cm_c`.   Note that if we merely wanted to estimate the interaction without also estimating the main effects we would write:

```{r}
model_theory5<- lm(income ~ not_male:height_cm_c, data = df)
```
```{r}
parameters::parameters(model_theory5)
```


Such a model tells us that the population household average is about NZD 131,000, and the slope difference for height by gender is three times greater for male-id's than for non-male-id's in the population.

```{r}
plot(ggeffects::ggpredict(model_theory5, terms = c("not_male", "height_cm_c")))
```

Generally you should include the main effects and the interactions of your indicator:

```{r eval = FALSE}
model_theory4 <- lm(income ~ height_cm_c * not_male, data = df)
```

Which gives us:

```{r}
sjPlot::tab_model(model_theory4)
```

Whoooah what just happened?! There's huge uncertainty about the main effect of male-id -- it is no longer a reliable predictor in this model. 

To assess the magnitude of the uncertainty let's plot the regression coefficients:

```{r}
sjPlot::plot_model(model_theory4)
```

The main effect of not_male is nearly 35k per year!  What's going on here? Recall that multi-collinearity -- or when two indicators are highly correlated, tend to result in large uncertainty in coefficient estimates. Is the interaction messing up the model?  Let's check:

```{r}
z<-performance::check_collinearity(model_theory4)
z ## looks ok
plot(z) ## looks ok
```
As indicated in the graph, the problem isn't multi-collinearity.  

Does this mean that income is really predicted by height, and gender is a red herring? I have seen regression models interpreted that way. 

It looks that way:


```{r}
modelbased::estimate_means(model_theory4)
```

However this is comparing male-ids and not-male ids at equivalent levels of height, but we know that non-male-id's tend to be shorter. 
 
However **wherever you have interactions it is essential to graph the results**.  This is so important I'm going to write it again: **wherever you have interactions it is essential to graph the results**.  Tables are notoreously difficult to interpret. So let's probe further by graphing the model predictions. Recall that we can do this using the `ggeffects` package.


```{r}
pp1 <- plot(ggeffects::ggpredict(model_theory4, terms = c("not_male","height_cm_c")))
pp1
```



The following graph makes it easier to see how male-id fails to predict income whereas the interaction of male-id x income does predict income:

```{r}
x <- ggeffects::ggpredict(model_theory4, terms = c("height_cm_c","not_male"))
pp2 <- plot( x )
pp2
```

Or consider this graph

```{r}
pp3 <- plot( x,
      add.data = TRUE, 
      jitter = 0.2, 
      dot.alpha =.05
      ) + scale_y_continuous(limits = c(0,5e+5))

pp3
```

This graph graphs height along the x-axis, and plots two different line. What we find is a non-linearity.  The model predicts that, on average, short men are expected to make much less than short women. Among non male-ids, height differences are not a reliable predictor of household income. By contrast among male-ids, height differences are a reliable predictor. non-male-ids tend to be shorter than men by about 13cm

Sample averages:

```{r}
df%>%
  summarise(mean(HLTH.Height, na.rm = TRUE))
```

Sample differences by gender-id:

```{r}
df%>%
  group_by(not_male)%>%
  summarise(mean(HLTH.Height, na.rm = TRUE))
```


Let's plug in the values for male-id and not male-id at the population average for each population:

```{r eval = FALSE, include = FALSE}
# this is how to quickly generate the equation
equatiomatic::extract_eq(model_theory4,  use_coefs = TRUE)
```

$$
\operatorname{\widehat{income}} = 134509.3 + 233.14(\operatorname{height\_cm\_c}) - 5311.89(\operatorname{not\_male}_{\operatorname{Not\_Male}}) - 161.81(\operatorname{height\_cm\_c} \times \operatorname{not\_male}_{\operatorname{Not\_Male}})
$$
To get the expected average for the male-id population we compute the regression equation as follows

```{r}
coef(model_theory4)[[1]] + # intercept about 170 cm
  coef(model_theory4)[[2]] * 80 + # male-id are 8 cm taller than sample average # note we include this even though the coefficient is unreliable
  coef(model_theory4)[[3]]* 0 + # male-id are coded as zero  + 
  coef(model_theory4)[[4]]* 80 * 0 # 8cm difference * male-id are coded as zero  (which zeros this out)
```

For the female-id population we compute the regression equation as follows 

```{r}
coef(model_theory4)[[1]] + # intercept about 170 cm
  coef(model_theory4)[[2]] * -50 + # not male-id are 5 cm shorter than sample average # 
  coef(model_theory4)[[3]]* 1 + # not male-id are coded as 1  + 
  coef(model_theory4)[[4]] * -50 # 5cm shorter * male-id are coded as zero  (which zeros this out)
```

Compare these estimates with the model in which we did not have the interaction but only the additive predictors of male-id and height. We can do this quickly using the `modelbased` package.

```{r}
modelbased::estimate_means(model_theory2)
```

We find that the estimates are similar. 


## How do we select a model?

Which model should we prefer?  Answer: it depends on your question.  We'll come back to this point when we focus on the uses of regression for causal inference (week 9).  For now, you should now that there are no canned methods for selecting a model. 

However because reviewers will ask, you can use the AIC or BIC information criteria to select a model. Recall that a decrease in the absolute values of either statistic offers a reason to prefer one model over the other.

```{r}
performance::compare_performance(model_theory, 
                                 model_theory2,
                                 model_theory3,
                                 model_theory4,
                                 model_theory5)
```

We can graph the models along different criteria:

```{r}

xx <- performance::compare_performance(model_theory, 
                                 model_theory2,
                                 model_theory3,
                                 model_theory4,
                                 model_theory5)

plot(xx)
```

We find that model three performed the best. Recall that this is the model that included education:

```{r eval = FALSE}
model_theory3 <- lm(income ~ height_cm_c + not_male + edu_s, data = df)
```

What if we were to include an interaction with height and male-id? 

```{r}
model_theory6 <- lm(income ~ height_cm_c * not_male + edu_s, data = df)
```

There's no improvement in this model. After adjusting for education, height, and male-id, we do not see an improvement from including a non-linear adjustment for the height effect for the different male-id factors: 

```{r}
performance::compare_performance(model_theory3,model_theory6)
```

What about if we were to include age?  REcall that a linear model can (and often should) include non-linear terms. We encountered this point when we were discussing polynomials and splines.  We can include a spline term for age using the `bs` command from the `splines` package. 


```{r}
library("splines")
model_theory7 <- lm(income ~ bs(age_10_c) + height_cm_c * not_male + edu_s, data = df)
parameters::parameters(model_theory7)
```


As judged by the AIC, this model has an improved fit. However as judged by the BIC, this model has reduced fit.  This happened because the BIC penalises the extra parameters.

We can graph this outcome as follows:

```{r}
za<-performance::compare_performance(model_theory3, model_theory7) # age, benefit but only if polynomial
plot(za)
```

Why might we include age?  We might be interested in the predicted comparisons for age among, here for the population that is set to zero in our model (male-id's who are shorter than average male height by about 83 cms, and have an sample average education). Age here has been converted to 2 x decade units.

```{r}
plot( ggeffects::ggpredict(model_theory7, terms = c("age_10_c")) ) 
```


We can see here that the interaction term has shifted because have added more variables. However, this is because the population we are predicting here is slightly different from the population before ... the change might have occurred merely because we have added more terms or, as we shall come to understand in Week 9, the change might have occurred because adding indicators produce new understanding. 

```{r}
plot( ggeffects::ggpredict(model_theory7, terms = c("height_cm_c","not_male")) ) 
```



## Can we estimate multiple outcomes at the same time? 

Yes. I do this using the BRMS package. 




## ANOVA

Don't use it, but if you must. The following code can be useful.

### ANOVA and T-tests

You can write these as linear models.

This is a one-way anova in which the grouping variable "not_male" is the condition and "income" is the outcome.

Anova
```{r}
#anova
m2 <- aov(income ~ not_male, data = df)
parameters::parameters(m2)
```

This is how the report package says you should report. I've tweaked the wording because I cannot write "statistically significant" without gagging:

```{r include = FALSE}
report::report(m2)
```

The ANOVA (formula: income ~ not_male) suggests that:

  - The main effect of not_male is statistically significant (F(1, 1734) = 23.52, p < .001; Eta2 = 0.01, 90% CI [5.87e-03, 0.02])

Effect sizes were labelled following Field's (2013) recommendations.

I recommend the `modelbased` package, which comes as part of `easystats` to explore your model

These are the estimated means

```{r}
modelbased::estimate_relation(m2)
```

Compare this against the linear model:

```{r}
m2l <- lm(income ~ not_male, data = df)
parameters::parameters(m2l)
```

This is how the report package says you should report as follows

```{r include = FALSE}
report::report(m2l)
```

We fitted a linear model (estimated using OLS) to predict income with not_male (formula: income ~ not_male). The model explains a statistically significant proportion of variance (R2 = 0.01, F(1, 1734) = 23.52, p < .001, adj. R2 = 0.01). The model's intercept, corresponding to not_male = 0, is at 1.54e+05 (95% CI [1.45e+05, 1.63e+05], t(1734) = 33.03, p < .001). Within this model:

  - The effect of not_male [Not_Male] is significantly negative (beta = -28349.98, 95% CI [-39814.07, -16885.89], t(1734) = -4.85, p < .001; Std. beta = -0.24, 95% CI [-0.34, -0.14])

Standardized parameters were obtained by fitting the model on a standardized version of the dataset.


These are the estimated means, which are identical to the ANOVA. 

```{r}
modelbased::estimate_relation(m2l)
```

I'm not goint to spend any more time with ANOVA.  This isn't because I'm dimissive of ANOVA. It can be a useful framework for certain questions. However, it is never going to produce different answers than you would obtain in a linear regression framework. Moreover the linear regression framework is much more flexible, and can be extending However in case you are required to formulate your regression model, I've included some R syntax for you here.


## One-way ANOVA

Syntax

```{r eval = FALSE}
aov(Y ~ Grp, data = data)
```

Assumptions:

- normal distribution of the of the DV's within each group
- homogeneity of variances within each group
- random sample from the population
- independent observations

## Two-way ANOVA

Syntax

```{r eval = FALSE}
aov(Y ~ Grp * Male, data = data)
```

Or this model can be written

```{r eval = FALSE}
aov(Y ~ Grp + Male + Grp:Male, data = data)
```

- normal distribution of the of the DV's within each group
- homogeneity of variances within each group
- random sample from the population
- independent observations


## MANOVA

This model can be written: 

```{r eval=FALSE}
# MANOVA test
model<- manova(cbind(Y1, Y2) ~ GRP, data = data)
summary(model)
```


## Appendix A: LaTeX tables

For advanced users, if you wish to write in $\LaTeX$ you can create a table in many way, for example copying and/pasting the output of the following code into your $\LaTeX$ document:

```{r}
parameters::parameters(model_simple)%>%
 kable( booktabs = F, "latex")%>%
  print()
```

The package I tend to go to for $\LaTeX$ is the texreg package:

```{r}
texreg::texreg(list(model_simple),
           custom.model.names = c("Cars ..."),
           caption = "Breaking distance as predicted by speed",
           sideways = F,
           scalebox = .5,
           #fontsize= "footnotesize",
           label = "tab:model_simple",
           ci.force.level = 0.95, bold = 0.05,
           settingstars = 0,
           booktabs = TRUE,
           custom.note ="")
```




