---
title: "Consolidation of skills"
description: |
author:
  - name: Joseph Bulbulia
    url: https://josephbulbulia.netlify.app
    affiliation: Victoria University of Wellington
    affiliation_url: https://www.wgtn.ac.nz
    orcid_id: 0000-0002-5861-2056
  - name: Johannes Karl
    url: https://johannes-karl.com
    affiliation: Victoria University of Wellington
    affiliation_url: https://www.wgtn.ac.nz
    orcid_id: 0000-0001-5166-0728
date: 2021-MAR-16
output:
  distill::distill_article:
    self_contained: false
    toc: true
    highlight: tango
bibliography: bib.bib
---

```{r spss-old, echo=F, warning=FALSE, message=FALSE}
knitr::include_graphics("op.png")
# let's set our theme too
```



```{r, echo = FALSE, warning=FALSE, message=FALSE}
#libraries
if (!require(skimr)) install.packages('skimr')
if (!require(lubridate)) install.packages('lubridate')

if (!requireNamespace("devtools")) {
  install.packages("devtools")
}
if (!require(easystats)) devtools::install_github("easystats/easystats")
if (!require(ggthemes)) install.packages('ggthemes')
if (!require(pmdplyr)) install.packages("pmdplyr")
if (!require(kableExtra)) install.packages("kableExtra")
# this should be part of easystats but in case not:
if (!require(report)) install.packages('report')
if (!require(brms)) install.packages('brms')
if (!require(lme4)) install.packages('lme4')
if (!require(table1)) install.packages('table1')
if (!require(modelsummary)) install.packages("modelsummary")
if (!require(naniar)) install.packages("naniar")
if (!require(ggraph)) install.packages("ggraph")


# load tidyverse
library("tidyverse")

# themeset
theme_set(theme_classic())

# uncomment below and run this code
# easystats::install_easystats_latest()
```

```{r echo = FALSE, message = FALSE, warning=FALSE,cache=TRUE}

nz_0 <- readr::read_csv2(url("https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv"))

# take all characters and make them factors
# also get rid of duplicate rows
# Note the convention of renaming dataframe when creating a new one:
# ` nz <-nz_0 %>%... `



f<-c("None Of The Time","A Little Of The Time","Some Of The Time",  "Most Of The Time", "All Of The Time")


nz <-nz_0 %>%
  dplyr::mutate_if(is.character, factor) %>%
  select(-c(SWB.Kessler01,SWB.Kessler02,SWB.Kessler03,SWB.Kessler04,SWB.Kessler05, SWB.Kessler06))%>%
  dplyr::mutate(Wave = as.factor(Wave))%>%
  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless,f))%>%
  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed,f))%>%
  mutate(FeelRestless = forcats::fct_relevel(FeelRestless,f))%>%
  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort,f))%>%
  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless,f))%>%
  mutate(FeelNervous = forcats::fct_relevel(FeelNervous,f))

# not used
# nz <- haven::zap_formats(nz)
# nz <- haven::zap_label(nz)
# nz <- haven::zap_widths(nz)
# nz <- haven::zap_labels(nz)
```

## Preamble

One of the advantages of R is that allows us to create highly effective workflows. Today, we'll reinforce and extend the workflow skills that you've started to develop in previous weeks. Below we'll be working with the `nz` dataset, which is a reduced, truncated, and jittered version of waves 10 and 11 of the New Zealand Attitudes and Values Study. This dataset is for teaching only, if you'd like to learn more about the study to which it belongs, go [here](https://www.psych.auckland.ac.nz/en/about/new-zealand-attitudes-and-values-study.html) or [here](https://go-bayes.github.io/reports/posts/nzavs/). 

## Data carpentry continued

### Different methods for selecting columns

Suppose we want to select all variables that start with `Believe`. We can do this in a number of ways.

First there is explicit selection:

```{r explicitselection}
# explicit selection 
nz %>%
  select("Believe.God", "Believe.Spirit")%>%
    glimpse()
```

We can select all instances of a column that start with a certain name. For this you by using `starts_with`

```{r startswith}
nz %>%
  select(starts_with("Believe"))%>%
    glimpse()
```

By the same token, we can select all instances of a variable that ends with a certain string by using `ends_with`

```{r endswith}
nz %>%
  select(ends_with("conditions"))%>%
    glimpse()
```

We can cast a broader net and select all instances of a variable within a string by using `contains`

```{r contains}
nz %>%
  select(contains("Believe"))%>%
    glimpse()
```

As we can see, the net that we cast using `contains` was too broad. We don't want the `Religion.Believe.Cats`.

In R, you can programme your way out of this corner as follows:

```{r containshack}
nz %>%
  select(contains("Believe") &  -  Religion.Believe.Cats)%>%
   glimpse()
```

However, that's inelegant; better to drop `contains` altogether and revert to another method.

### Re-leveling a factor

Death, taxes, and factors are consequence of living. Let's look at the `BigDoms` variable in the nz, which is a factor identifying large religious denominations

```{r selectbdoms}
nz %>%
  dplyr::select(BigDoms)%>%
  table(useNA ="ifany")
```

Note the use of `ifany` to print the NAs in this table. It's almost never sensible to ignore missing values!

Suppose we wanted to make "Not Rel" our base category for this factor. We could do so as follows:

```{r fctrelevelbigcoms}
## suppose we want "Not_Rel" as the base category, and rearrange the other levels
library(forcats) # this is part of the tidyverse package. 
nz1<-nz %>%
  dplyr::select(BigDoms, KESSLER6sum) %>%
  dplyr::mutate(BigDoms =  
                  forcats::fct_relevel(BigDoms, c("Not_Rel","Christian","Buddhist","Muslim","TheOthers")))

#inspect data
nz1%>%
  group_by(BigDoms)%>%
  count()
```

The reordering makes for a more sensible model because the base category is now `Not_Rel` or not-religious. Hence comparisons are to this category.

```{r bdommodeltable}
m0<- glm( KESSLER6sum ~ BigDoms, data = nz1 )
parameters::model_parameters(m0)  %>%
  print_html(caption = "Model of Distress by Denomination with the base category is `No Religion'")
```

We can see the results better using a coefficient graph, which visualises the information presented in the table. 

```{r bdommodelgraph}
plot(parameters::model_parameters(m0) ) + labs(title = "Comparison of Religious groups to secular people", 
subtitle = "Christians are a little more chilled out, \n Other denoms are less chilled out")
```

The base category is the comparison class. Should we infer that "The Others" denomination causes greater distress? We'll return to this, and related questions, in the upcoming weeks. For now let's just leave it at "probably not."

### Creating factors from numerical indicators

It is almost never a good idea to transform continuous data into categorical data. However, occassionally, you will need to do so. For example, we might want to break the KESSLER6 distress indicator into its medically diagnostic components for "mild distress", "moderate distress", and "severe distress."  We may achieve this task using the `cut` function as follows:

```{r}
nz <-nz %>%
  dplyr::mutate(k6cats = cut(
    KESSLER6sum,
    breaks = c(-Inf, 5, 13, Inf),   # create Kessler 6 diagnostic categories
    labels = c("Low Distress", "Moderate Distress", "Serious Distress"), 
    right = TRUE
  ))
table(nz$k6cats, useNA = "ifany")
```

### Using `ifelse` to create factors

I prefer to maintain control over how I am making the categories. For example, in the previous example, I didn't remember whether `cut` includes a value to the left or to the right. I had to look this up. However, I can use `ifelse` function to explicitly create the relevant categories:

```{r}
nz %>%
  dplyr::mutate(k6cats1 =  as.factor(ifelse(
    KESSLER6sum <= 5,
    "Low Distress",
    ifelse(KESSLER6sum <= 13,  "Moderate Distress", "Serious Distress")
  ))) %>%
  group_by(k6cats1) %>%
  count()

#check this is the same as the previous method
nz %>%
  group_by(k6cats) %>%
  count()
```

We can see that this method returns the same values as the `cut` method above.

### Transformations of indicators: scaling, centering, and logs

Throughout this course, we'll be standardising and centering indicators. Occasionally, we'll need to perform log transformations. You'll need to know how to do this.

Suppose we want to standardise the `Relid` indicator. This will transform the `Relid` indicator into standard deviation units. In later seminars, we'll explain why this transformation is useful. For now, this is how you do it:

```{r}
nz1 <- nz %>%
  select(Relid)%>%
  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE))
head(nz1)
```

What happened? The variable name for our standardised variable looks weird: `religious_s[ ,1]`

This isn't a worry. We use the variable as we would any other and all is fine. [^1]

[^1]: Notice, the intercept here is zero. This because we centered the new indicator at zero, and we wrote a model that is estimating the population average for this outcome (an intercept-only model). Don't worry if you don't know what an intercept is, we'll get to regression in a few weeks.

```{r}
sjPlot::tab_model(lm(religousid_s ~ 1 , data = nz1))
```

### Pro tip 1: 

Transform your data as the **last** step in your pipe workflow.

This is because if you filter cases, you'll end up with a variable that isn't measured standard deviations units

```{r}
nza <- nz %>%
  select(Relid, BigDoms)%>%
  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE)) 
nzb <- nz %>%
  select(Relid, BigDoms)%>%
  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE)) %>%
  filter(BigDoms !="Not_Rel")

# compare
summary(nza$religousid_s)

# with
summary(nzb$religousid_s)
```

When we filter last, the mean value in the dataset is `r round(mean(nzb$religousid_s,na.rm=TRUE),2)` -- everything has changed!

```{r}
nz1 <- nz1 %>%
  select(Relid)%>%
  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE))
head(nz1)
```

or simply:

```{r eval = FALSE}
nz1 <- nz1 %>%
  select(Relid) %>%
  mutate(religousid_s = scale(Relid))

head(nz1)
```

To center a variable we set `scale = FALSE, center = TRUE`

```{r}
nz1 <- nz %>%
  mutate(religousid_c = scale(Relid, scale = FALSE, center  = TRUE))

# inspect new indicator
nz1%>%
  select(Relid,religousid_c)%>%
    glimpse()
```

We use the log transformation for extreme values. We can create a new indicator by combining `mutate` and `log` as follows:

```{r}
nz1 <- nz %>%
  mutate(charitydonate_log = log(CharityDonate + 1))

# inspect new indicator
nz1 %>%
  select(CharityDonate,charitydonate_log)%>%
    glimpse()
```

Note that we have to add $$+1$$ to the log transformation, as you will recall that the log of zero is undefined. You cannot obtain zero by raising it to the power of another number.

### Create and work with dates a date

```{r}
nz <- nz %>%
  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE)  # first data of data collection in this study
```

We can analyze dates, for example, for how many minutes were data collected?

```{r}
nz %>%
  select(date)%>%
  summary()

int<-lubridate::interval(ymd("2018-01-02"), ymd("2020-10-06"))

#time in years
time_length(int, "year")

#time in minutes
time_length(int, "minutes")
```
Fun! So much so you have some homework that will work with dates.

### Create a timeline

Here we're going to graph the number of responses each day for the years of data collection.

```{r}
library(lubridate)
library(ggplot2)


datrep <- nz %>%
  count(day = floor_date(date, "day"))%>%
  dplyr::mutate(Year = factor(ifelse(
    day < "2018-01-01",
    2017,
    ifelse(day < "2019-01-01", 2018,
           ifelse(day < "2020-01-01", 2019, 2020))
  ))) %>%
  arrange(day)

# create the graph
ggplot(datrep, aes(day, n)) +
  geom_col(aes(fill = Year)) +
  scale_x_date(date_labels = "%b/%Y")  +
  xlab("Days") + ylab("Count of Responses") +
  ggtitle("Our Dataset's Daily Counts")  +
  theme_classic()  +
  scale_fill_viridis_d()

```

Note that we can use the `datrep` dataframe that we created to explore aspects of data collection. For example we can arrange the dataset by day in descending order of participants sampled:

```{r}
datrep%>%
  arrange(desc(n))
```

Take not of that code, you might need it for your workbook.


What might we do with dates? Well we might ask, were there any inherently stressful days?

To see this, we can take average stress levels by day, and then see where the high average stress days fall.

```{r}
tn<-nz %>%
  select(date,KESSLER6sum,Id) %>%
  group_by(date)%>%
  summarise(
   av_distress =  mean(KESSLER6sum, na.rm = TRUE),
   n = n_distinct(Id)
  ) %>%
  arrange(desc(av_distress))%>%
  glimpse()

```

Graphing the densities reveals the following

```{r}
tn%>%
  ggplot(., aes(date, av_distress)) + 
  geom_col(aes(fill =(n))) + scale_x_date(date_labels = "%b/%Y")  + theme_classic() + scale_fill_viridis_c()
```

Clearly the "stressful days" are an artifact of days with low numbers of participant respondents.

Let's see whether there are any stressful days of the week. We do this by creating a weekday variable using the `wday` function in the `lubridate` package. Let's graph our results using a pipe `%>%` workflow:

```{r}
nz %>%
  select(Id, date, KESSLER6sum) %>%
  mutate(weekdays = wday(date, label = TRUE)) %>%
  group_by(weekdays) %>%
  summarise(
    mn_k6 =  mean(KESSLER6sum, na.rm = TRUE),
    sd_k6 =  sd(KESSLER6sum, na.rm = TRUE),
    n_k6w = n()
  ) %>%
  mutate(
    se_k6 = sd_k6 / sqrt(n_k6w),
    lw_ci = mn_k6 - qt(1 - (0.05 / 2), n_k6w - 1) * se_k6,
    up_ci = mn_k6 + qt(1 - (0.05 / 2), n_k6w - 1) * se_k6
  ) %>%
  ggplot(., aes(x = weekdays, y = mn_k6, colour = mn_k6)) +
  geom_errorbar(aes(ymin = lw_ci, ymax = up_ci), width = .1) +
  geom_point(size = 3)  +
  scale_y_continuous(limits = c(0,7)) + 
  theme_classic() + scale_fill_viridis_d()
```

Despite the variability over the two years of data collection, the bars of the graph overlap: we don't find differences in distress by days.

"Ok Boomer", you ask, "what if we were to calculate distress by generational cohorts?"

My reply, I'm not a boomer, I'm a GenX-er. I'm keen to check it out:

```{r layout="l-body-outset", fig.width=10, fig.height=10}
nz %>%
  select(GenCohort, KESSLER6sum) %>%
  group_by(GenCohort) %>%
  summarise(
    mn_k6 =  mean(KESSLER6sum, na.rm = TRUE),
    sd_k6 =  sd(KESSLER6sum, na.rm = TRUE),
    n_k6w = n()
  ) %>%
  mutate(
    se_k6 = sd_k6 / sqrt(n_k6w),
    lw_ci = mn_k6 - qt(1 - (0.05 / 2), n_k6w - 1) * se_k6,
    up_ci = mn_k6 + qt(1 - (0.05 / 2), n_k6w - 1) * se_k6
  ) %>%
  ggplot(., aes(x = GenCohort, y = mn_k6, colour = GenCohort)) +
  geom_errorbar(aes(ymin = lw_ci, ymax = up_ci), width = .1) +
  geom_point(size = 3)  +
  scale_y_continuous(limits = c(0, 7)) +
  theme_classic() +
  geom_hline(yintercept = 5,
             colour = "red",
             linetype = "dashed") +
  scale_y_continuous(limits = c(0, 10)) +
  theme(
    legend.text = element_text(size = 6),
    legend.title = element_text(size = 8),
    axis.text.x = element_blank()
  ) +
  xlab("Birth Generation Cohort") +
  ylab("Kessler 6 Distress") +
  labs(title = "Average Distress by Birth Cohort",
       subtitle = "Red line indicates clinically moderate distress threshold") +
  scale_colour_viridis_d() 
```

Later, we'll ask why you're so stressed out. 

### `Slice`

Dplyr's slice function can be handy. Say we only want the first four rows

```{r}
datrep%>%
  arrange(desc(n)) %>%
  slice(1:4)
```

Say we only want the 1st row, the 3rd row, and the 20th row

```{r}
datrep%>%
  dplyr::arrange(desc(n)) %>%
  dplyr::slice(c(1,3,20))
```

### Lags and leads using timeseries data

Create a difference variable for change in Kessler 6

```{r}
library("pmdplyr")
df <-nz %>%
  dplyr::filter(!is.na(KESSLER6sum))%>%
  mutate(wave = as.numeric(Wave))%>%
   mutate(lag_k6 = tlag(KESSLER6sum,
    .i = Id, # id variable
    .t = wave # time series variable, needs to be numeric
  ))%>%
  mutate(diff_k6 = lag_k6 - KESSLER6sum) %>%
  select(Id,Wave,KESSLER6sum,diff_k6,Emp.JobSecure,Employed)%>%
  arrange(desc(diff_k6)) 

```

What to do with this new variable. Well, we might explore whether employment security relates to distress change:

```{r}
df %>%
  filter(Wave == 2019) %>%
  mutate(employed_employsecurity = as.factor(ifelse(Employed ==1, Emp.JobSecure,0)))%>%
  ggplot(data = ., aes(x = diff_k6, fill = employed_employsecurity) )+
   geom_histogram() + 
  xlab("Difference in K6 eleveation (cases above 5)") + 
  ylab("Counts of cases") + 
  labs(subtitle ="No clear relationship between unemployment insecurity and distress change")+
  scale_fill_discrete(name="Employment Security 1-7") + 
  scale_fill_viridis_d() + theme_classic() + 
  theme(legend.position = "bottom")
```

And remarkably we don't see much evidence in the cross-sectional analysis.

```{r}
# create data frame with new variable Zero is for the unemeployed. 
dfnew <- df %>%
  filter(Wave == 2019) %>%
  mutate(employed_employsecurity = as.numeric(ifelse(Employed == 1, Emp.JobSecure, 0)))%>%
filter(!is.na(employed_employsecurity))

head(dfnew)
# Graph
ggplot(dfnew, aes(y = diff_k6, employed_employsecurity)) +
  geom_jitter(alpha = .2) +
  geom_smooth(method = lm) +
  xlab("employed_employsecurity") +
  ylab("Kessler 6 distress jumps over 5") +
  ggtitle("Jumps in distress change not related to employement insecurity") +
  scale_fill_viridis_d() + theme_classic()
```

However, perhaps our indicator is misleading us. We can formally model the relationship between employment security and Kessler6 distress across two years

```{r}
# create dataframe with the variables we need
dfnew2 <- df %>%
  mutate(employed_employsecurity = as.numeric(ifelse(Employed == 1, Emp.JobSecure, 0))) %>%
  filter(!is.na(employed_employsecurity)) %>%
  dplyr::mutate(employsecurity_s = scale(employed_employsecurity))

# multi-level model 

m00a<-lme4::lmer(KESSLER6sum ~  employsecurity_s * Wave + (1|Id), data = dfnew2)
plot(ggeffects::ggpredict(m00a, terms=c("employsecurity_s", "Wave")),
     add.data = TRUE, jitter = 0.2, dot.alpha =.05) + geom_hline(yintercept = 5,
             colour = "red",
             linetype = "dashed") + 
  labs(title = "There is a relationship between employment security and Kessler6 distress")
```

This suggests a stable negative relationship between employment security and (low) distress. So is there are causal relationship? Not necessarily. Again, we return to casual inference in the upcoming weeks. For now, we want to alert you to an important lesson:

### Pro tip 2
Do not read too much into your descriptive analysis!

This is especially true when creating new variables. Just because you can make a variable doesn't mean you should use it, or interpret it!

Put differently, our workflow will require much more than descriptive statistics.

## Data summary

### Summarise all your data

#### The `skimr` package

The skimmer package can be helpful in detecting problems. A drawback note that it is interpreting all factors as numbers).

For example. ( I won't run the following code, you will do so for your homework).

```{r eval = FALSE}
library("skimr")
nz %>%
  select(-date) %>% #not useful
  dplyr::group_by(Wave) %>%
  skim()
```

However, I want to point out that `skimr` works with individual columns, and it accepts a tidy workflow.

```{r}
nz %>%
  dplyr::group_by(Wave) %>%
  select(KESSLER6sum,HLTH.SleepHours)  %>%
  skim() 
```
#### `Table1` & other canned tables

In earlier seminars, we encountered the `table1` package, which makes really great html tables:

```{r layout="l-body-outset"}
library(table1)

table1::table1(~Age  +
                 GenCohort +
                 Male + 
                 Edu +
                 Pol.Orient + 
                 Relid + 
                 BigDoms   | Wave, data = nz,
               overall = FALSE)
```

Unfortunately, the `table1` package only prints html tables.


For publications, I might use the `modelsummary` package

```{r}
library("modelsummary")
nnz<-nz %>%
  dplyr::select(Age, 
                Male,
                BigDoms,
                Edu,
                GenCohort,
                Relid,
                Pol.Orient,
                Wave)
modelsummary::datasummary_balance( ~ Wave, data=nnz, dinm=FALSE, output = 'table.tex')
```

I'll put the $LaTeX$ output into my document because I generally prefer to write in $LaTeX$

However if you want to print inline, you can simply use:


```{r}
library("modelsummary")
nnz<-nz %>%
  dplyr::select(Age, 
                Male,
                BigDoms,
                Edu,
                GenCohort,
                Relid,
                Pol.Orient,
                Wave)
modelsummary::datasummary_balance( ~ Wave, data=nnz, dinm=FALSE)
```



### Create a table using pipe functions

Above we saw how to create a cluncky table using `table(x)`. However, R has lots of functionality to enable better. 


```{r designtable,  tab.cap="\\label{tab:designtable}"}
library(kableExtra)
nz %>%
  select(k6cats, Wave) %>%
  filter(!is.na(k6cats))%>%
  group_by( Wave, k6cats) %>%
  summarise(n = n())%>%
  kbl(caption = "Distress by Year") %>%
   kable_classic_2(c("striped", "hover"), full_width = TRUE)%>%
  collapse_rows()
```

Note that we can use the `pivot_wider` function to spread the dataframe to enable a table that is easier to interpret. 

Credit where credit is due: I just learned about `pivot_wider` from Johannes and Thorven. I'm keen to get `pivot_longer` and `pivot_wider` into my vocabulary, and to do more things, like this:

```{r}
nz %>%
  select(k6cats, Wave) %>%
  filter(!is.na(k6cats))%>%
  group_by( Wave, k6cats) %>%
  summarise(n = n())%>%
 pivot_wider(names_from = Wave, values_from = n) %>%
   kbl(caption = "Distress counts by year") %>%
   kable_classic_2(c("striped", "hover"), full_width = TRUE)
```

Nice!


### Bar graphs

For categorical data, in place of tables we can use bar graphs

Here's the table:

```{r}
table(nz$BigDoms)
```

Here's the bar graph:

```{r}
ggplot(nz) + 
  geom_bar(mapping = aes(x = BigDoms))
```

Note that we can re-order the factor levels to produce a nicer output, using `fct_infreq`

```{r}
ggplot(nz) + 
  geom_bar(mapping = aes(x = fct_infreq(BigDoms))  )
```

### Missing data graphs 

What do you notice about the patterns of missingness in this graph? 

```{r cache = TRUE}
library("naniar")
naniar::vis_miss(nz) #What do you notice? 
```


Here, we find all the problem cases:

```{r}
gg_miss_upset(nz)
```

What explains these feature of missingess? 

### Boxplots

A box plot provides visual information for the following statistics:

-   Minimum -- (0p) min outlier
-   Maximum -- (100p) max outlier
-   Median -- (50th p)
-   First Quartile (Q1 or 25p)
-   Third Quartile (Q3 or 75p)
-   Interquartile range (IQR), whcih is the distance between Q1 and Q3\
-   Optional: the notch displays a confidence interval around the median. This is +/- 1.58 X IQR/sqrt(n). We use notches to compare differences between groups; overlap implies uncertainty about whether the medians differ.

There's a simple explanation [here](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)

We can use base R to investigate differences in distress among big denominations:

```{r}
# using base R
boxplot(KESSLER6sum ~ BigDoms, data = nz, notch = TRUE, col = c("cadetblue1","orange","red","darkblue","brown"))
```

Here's a ggplot boxplot:

```{r warning = FALSE}
ggplot(data = nz, aes(x = KESSLER6sum, y = BigDoms, fill = BigDoms)) + 
  geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + 
  ggtitle("If the notches don't overlap, there's likely a difference") + 
  geom_jitter(alpha = .05)
```

Here's a ggplot2 boxplot with points overlaid, and jittered. This allows us to se the differences in sample sizes

```{r}
ggplot(data = nz, aes(x = KESSLER6sum, y = BigDoms, fill = BigDoms)) + 
  geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + 
  ggtitle("If the notches don't overlap, there's likely a difference") + 
  geom_jitter(alpha = .07)
```

We could look at differences by wave:

```{r}
ggplot(data = nz, aes(x = KESSLER6sum, y = BigDoms, fill = BigDoms)) + 
  geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + 
   geom_jitter(alpha = .07) + 
  facet_grid(Wave ~ .) +
  ggtitle("If the notches don't overlap, there's likely a difference") 
 
```

### Correlation graphs

Johannes will describe a method for making a correlation plot. Here is another method.


```{r}
bzsec<-nz%>%
  select(
    Your.Future.Security,
    Standard.Living,
    NZ.Economic.Situation,
    NZ.Business.Conditions,
    Emp.JobSecure,
    CharityDonate,
    Id
  ) %>%
  mutate_all(., as.numeric) %>% #make numeric 
  mutate(Id = as.factor(Id),
         CharityDonate = log(CharityDonate + 1))# make Id a factor for the 

# make a correlation plot using the "correlation" package from easystates

library(correlation)
p1<-bzsec %>%
  correlation(partial = FALSE, multilevel = TRUE ) %>%
  plot()+ 
  theme_blackboard()
```

Print summary

```{r}
bzsec %>%
  correlation(partial = FALSE, multilevel = TRUE ) %>%
  summary()
```


Let's set multilevel to `FALSE`. 

```{r}
library(correlation)
p2<-bzsec %>%
  select(-Id)%>% # get rid of grouping variable
  correlation(partial = FALSE, multilevel = FALSE ) %>%
  plot()
#print summary
bzsec %>%
  select(-Id)%>% # get rid of grouping variable
  correlation(partial = FALSE, multilevel = FALSE ) %>%
  summary()
```

```{r layout="l-body-outset", fig.width=10, fig.height=10}
library(patchwork)
# create a two panel plot
p1 / p2 + 
  plot_annotation(title = "Plot of multilevel (a) and single-level (b) correlation", tag_levels = 'a')
```


We can see an even greater correlations between the variables. This is because the model does not adjust for the repeated measures, which create dependencies in the data. 


### The `report` package

The reports package from the `easystats` group is powerful tool for saving tame. Before extolling its virtues, I'd like to point out two major limitations.

First, the package is in development. Currently, it has lots of bugs.

Second, the package uses terminology that won't work for all contexts and purposes. For example, it uses the term "significant" to describe p values that are below the traditional p = .05 threshold.

If you learn nothing else from this course, you should learn never to use "significant" to describe a p value. You may, if you like, use "statistically signficant" however it would be better altogether if you simply dropped p-values from data analysis. We'll show you how. With those provisos in mind, consider some useful functionality from the `report` package.

```{r max.height='1000px', max.width ='1000px', layout="l-body-outset"}
# create a demographic dataframe
nz_demagraphics <- nz %>%
  select(Age, GenCohort, Male, Edu, Pol.Orient, Relid, BigDoms, Wave)

# now a nice way to save you time when reporting
paste(
  report::report_participants(
    nz_demagraphics, 
    group = "Wave", 
    age = "Age",
    sex = "Male",
    education = "Edu",
    spell_n = TRUE),
  "were recruited in the study by through enticement by lollipops. Those who did not volunteer were coerced."
  )
```

The table function of `report` isn't great yet. However it has some nice features. For example you should always report your session information, and doing so in tabluar form clarifies the elements

Try running the following code on your own:


```{r eval=FALSE}
r <- report_table(sessionInfo())
r
```

Here is another method, which you can try on your own

```{r eval= FALSE}
cite_packages()
```

Here's a demographic table (try on your own)

```{r eval=FALSE}
report_table(nz_demagraphics)
```

Here's a data summary

```{r}
library("report")
nz %>%
  group_by(Wave)%>%
  select(
    "Wave", 
    "Age",
    "Male",
    "Edu",
    "Relid",
    "Pol.Orient",
    "KESSLER6sum",
    "FeelHopeless",
    "FeelDepressed",
    "FeelRestless",
    "EverythingIsEffort",
    "FeelWorthless",
    "FeelNervous"
    )%>%
  report() %>% 
  summary()
```


Notes: 

More about the report package: [here](https://easystats.github.io/report/index.html)

This package is brought to you by [easystats](https://github.com/easystats/easystats)

### Measures

When reporting your study, it is extremely important to include information about your measure. For example:

We measure psychological distress using the Kessler-6 scale [@Kessler2002-ex], which exhibits strong diagnostic concordance for moderate and severe psychological distress in large, cross-cultural samples [@Kessler2010-xi, @Prochaska2012-dy]. Participants rated during the past 30 days, how often did... (a) “\dots you feel hopeless”; (b) “\dots you feel so depressed that nothing could cheer you up”; (c) “\dots you feel restless or fidgety”; (d)“\dots you feel that everything was an effort”; (e) “\dots you feel worthless”; (f) “\dots you feel nervous?” Ordinal response alternatives for the Kessler-6 are: “None of the time”; “A little of the time”; “Some of the time”; “Most of the time”;  “All of the time.” 

We report sample descriptive statistics for indicators of personal Kessler-6 distress in Table \@ref(fig:desckK6).


```{r fig.cap="(ref:desckK6)"}
library(table1)
table1::table1(
  ~ KESSLER6sum +
    FeelHopeless +
    FeelDepressed +
    FeelRestless +
    EverythingIsEffort +
    FeelWorthless +
    FeelNervous | Wave,
  data = nz,
  overall = FALSE
)
```

In addition to the packages already mentioned, the `gtsummary` package for nice functionality when creating tables. Here's an example:

```{r}
library(gtsummary)
nz %>%
  dplyr::select(
    KESSLER6sum,
    FeelHopeless,
    FeelDepressed,
    FeelRestless,
    EverythingIsEffort,
    FeelWorthless,
    FeelNervous,
    Wave,
  ) %>%
  gtsummary::tbl_summary(
    by = Wave,
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} / {N} ({p}%)"
    ),
    digits = all_continuous() ~ 2,
    missing_text = "(Missing)"
  )
```



Also, check out Johannes's mini-lecture on `papaja.` 


### Order of Method

The following is a brief guide to describing your method. We'll be returning to report writing in future weeks. For now, I just want to put this on the table for you. The advice is just a guide.

+--------------+-------------------------------------------------------------------------------------+
| **Heading**  | Include                                                                             |
+--------------+-------------------------------------------------------------------------------------+
| Participants | -   Participant or subject characteristics                                          |
|              |                                                                                     |
|              | ```{=html}                                                                          |
|              | <!-- -->                                                                            |
|              | ```                                                                                 |
|              | -   Sampling procedures                                                             |
|              |                                                                                     |
|              | -   Sample size and power                                                           |
+--------------+-------------------------------------------------------------------------------------+
| Materials    | -   Primary and secondary measures                                                  |
|              |                                                                                     |
|              | -   Quality of measurements                                                         |
+--------------+-------------------------------------------------------------------------------------+
| Procedure    | -   [Data collection methods](https://www.scribbr.com/methodology/data-collection/) |
|              |                                                                                     |
|              | -   Research design (e.g., experimental, correlational, or descriptive)             |
|              |                                                                                     |
|              | -   Data processing and diagnostics (e.g., outlier removal)                         |
|              |                                                                                     |
|              | -   Data analysis strategy (e.g., comparison or regression tests)                   |
+--------------+-------------------------------------------------------------------------------------+


Below are the sampling procedures from the New Zealand Attitudes and Values Study, from where the `nz` teaching dataset was drawn.


## Appendix 1A Sampling Procedure -- NZAVS Time 10 (2018; conducted from 18.06.2018-28.09.2019)

The Time 10 (2018) NZAVS contained responses from 47,951 participants (18,010 retained from one or more previous wave. The sample retained 2,964 participants from the Time 1 (2009) sample (a retention rate of 45.5%). The sample retained 14,049 participants from Time 9 (2017; a retention rate of 82.3% from the previous year). Participants who provided an email address were first emailed and invited to complete an online version if they preferred. Participants who did not complete the online version (or did not provide an email) were then posted a copy of the questionnaire, with a second postal follow-up two months later. We staggered the time of contact, so that participants who had completed the previous wave were contacted approximately one year after they last completed the questionnaire. We offered a prize draw for participation (five draws each for \$1000 grocery vouchers, \$5000 total prize pool). All participants were posted a Season’s Greetings card from the NZAVS research team and informed that they had been automatically entered into a bonus seasonal grocery voucher prize draw. Participants were also emailed an eight-page newsletter about the study. 

To boost sample size and increase sample diversity for subsequent waves, a booster sample was conducted by selecting people from the New Zealand electoral roll. As with previous booster samples, sampling was conducted without replacement (i.e., people included in previous sample frames were identified and removed from the 2018 roll). The sample frame consisted of 325,000 people aged from 18-65 randomly selected from the 2018 New Zealand Electoral Roll, who were currently residing in New Zealand (one can be registered to vote in New Zealand but living overseas). The electoral roll contained ~3,250,000 registered voters. The New Zealand Electoral Roll contains participants’ date of birth (within a one-year window), and we limited our frame to people who 65 or younger, due to our aim of retaining participants longitudinally. We concurrently advertised the survey on Facebook via a $5000 paid promotion of a link to a YouTube video describing the NZAVS and the large booster sample we were conducting. The advertisement targeted men and women aged 18-65+ who lived in New Zealand and ran for 14 days. This paid promotion reached 147,296 people, with 4,721 link clicks (i.e., clicking to watch the video), according to Facebook. The goal of the paid promotion was twofold: (a) to increase name recognition of the NZAVS during the period in which questionnaires were being posted, and (b) to help improve retention by potentially reaching previous participants who happened to see the advertisement. A total of 29,293 participants who were contained in our sample frame completed the questionnaire (response rate = 9.2% when adjusting for the 98.2% accuracy of the 2018 electoral roll). A further 648 participants completed the questionnaire, but were unable to be matched to our sample frame (for example, due to a lack of contact information) or were unsolicited opt-ins. Informal analysis indicates that unsolicited opt-ins were often the partners of existing participants. 

## Appendix 1B Sampling Procedure -- NZAVS Time 11 (2019; conducted from 29.09.2019-17.10.2020)

The Time 11 wave was conducted during COVID-19 pandemic. Procedures thus differed in that there was an increased focus on online deliver using email reminders and extensive Facebook advertising, no Christmas card, and incomplete phoning of non-respondents.

The Time 11 (2019) NZAVS contained responses from 42,684 participants (36,522 retained from one or more previous wave. The sample retained 2,506 participants from the Time 1 (2009) sample (a retention rate of 38.4%). The sample retained 34,782 participants from Time 10 (2018; a retention rate of 72.5% from the previous year). Participants who provided an email address were first emailed and invited to complete an online version if they preferred. Participants who did not complete the online version (or did not provide an email) were then posted a copy of the questionnaire, with a second postal follow-up two months later. We staggered the time of contact, so that participants who had completed the previous wave were contacted approximately one year after they last completed the questionnaire. A second reminder email was sent approximately four months following the first email attempt. We offered a prize draw for participation (five draws each for \$1000 grocery vouchers, \$5000 total prize pool). Participants were also emailed an eight-page newsletter about the study. As in past years, three attempts were made to phone non-respondents using each available cell and landline number. However, due to the university closure during COVID-19 lockdowns, phoning attempts were made for only 54% of the phoning pool (11,687 from a total of 21,636 non-respondents who provided at least one phone number). 

Two additional forms of recruitment were also introduced during Time 11. The first was a large  information box in the questionnaire (taking a full page on the paper version), which asked people: ‘Do you have a partner who would also like to join the NZAVS?’ with additional details for how partners might join the study (see questionnaire for the full text). The second was a Facebook advertisement. The advertisement targeted men and women aged 18-65+ who lived in New Zealand and ran from and 4th April 2020 – 4th July 2020 (overlapping with New Zeeland’s first lockdown period and recovery), and again from 18th August 2020 – 4th September (during the second Auckland lockdown). Given the unprecedented nature of the COVID-19 lockdowns, we thought it important to maximise sampling during these periods. The goal of the Facebook advertisement was threefold: (a) to increase name recognition of the NZAVS and remind people to complete the paper/online version already posted/emailed to them, (b) to help improve retention by potentially reaching previous lost participants who happened to see the advertisement, and (c) to recruit new participants (and also the partners of existing participants) while people were at home with some possibly having more free time during lockdown. This last goal was indirect and not explicitly stated it in the advertisement. 

The Facebook advertisement read as follows: “Participate in the New Zealand Attitudes and Values Study. Complete the 2020 Questionnaire online” with the body of text: "If you are part of the NZAVS, but have not heard from us in the last year, then please consider completing the 2020 questionnaire online. The study is more important than ever as we aim to understand the impact of COVID-19 on mental health, wellbeing and resilience in our communities. We wish you all the best at this time and hope you keep well and stay safe." This paid promotion reached 883,969 people, with 37,850 link clicks (i.e., clicking the link for the Qualtrics survey) according to Facebook. A total of 6106 people continued complete the questionnaire and provide full contact details, and were thus included in the dataset (4734 were new participants opting in to the study, and 1372 were previously ‘lost’ participants). 

## Appendix 2  Johannes's mini-lecture on the `papaja` package

[Lecture](https://www.dropbox.com/s/ap96nfnvs833zq1/yaml.pptx?dl=0)

[Papaja R markdown template](https://www.dropbox.com/s/bvdo6z55kgfr0ri/technology.Rmd?dl=0)

## Appendix 3

APA style advice [here](https://opentextbc.ca/researchmethods/chapter/american-psychological-association-apa-style/)
