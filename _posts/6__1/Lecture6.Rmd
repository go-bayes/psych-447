---
title: "An introduction to simulation"
description: 
author:
  - name: Joseph Bulbulia
    url: https://josephbulbulia.netlify.app
    affiliation: Victoria University of Wellington
    affiliation_url: https://www.wgtn.ac.nz
    orcid_id: 0000-0002-5861-2056
date: 2021-MAR-30
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
    code_folding: false
    highlight: kate
---

```{r setup, echo=FALSE}
# setup
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

```{r echo = FALSE, message = FALSE}
# libraries
library("tidyverse")
library("patchwork")
library("brms")
library("lubridate")
library("splines")
library("easystats")
library("simpleboot")
if (!require(broom)) {
  devtools::install_github("tidymodels/broom")
}

# useful functions
if (!require("tjmisc")) {
devtools::install_github("tjmahr/tjmisc")
}
# Wrapping functions : https://github.com/tjmahr/WrapRmd/commit/ff7ed3caf953971bccfc553df3b378cf5a093b7b
if (!require(broom)) {
devtools::install_github("tjmahr/WrapRmd")
}
# set theme
theme_set(theme_classic())
```

```{r echo = FALSE, include = FALSE}
md_df <- data.frame(read.table(url("https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt"), header=TRUE))
# Center mother's height for later example
```


```{r spss-old, echo=F}
knitr::include_graphics("bibpng1.png")
```

## Required readings

## Overview

This week we will:

-   Learn how to use R to generate random numbers
-   Use random numbers to simulate data
-   Relate lessons from simulation to lessons from regression.

## Learning outcomes

By learning how to simulate data you will better understand what a regression model is doing.

Additionally, you will need to understand simulation to follow the seminars on causal inference and multilevel modelling.

## Functions for simulation

### `rnorm`

`rnorm` is a R's random number generator. Within this function:

-   `n`: specifies the number of observations that you will generate
-   `sd`: specifies the value of the standard deviation
-   `mean`: specifies the value of the mean

```{r}
# seed
set.seed(123)
# generate random numbers
ds <- rnorm(n = 1000,
            mean = 0,
            sd = 1)
dplyr::glimpse(ds)
```

We can create a histogram:

```{r }
p1 <- ggplot2::qplot(ds) + labs(title = "1st random number list")
p1
```

Note that R users frequently shorten the above code, which can be written:

```{r }
set.seed(123)
# generate random numbers
ds2 <- rnorm(1000)

# check whether the abbreviated simulated vector is the same as the long form vector
all.equal( ds, ds2 )
```

We used `set.seed` to ensure that the same random vector will be generated for our audience. We can also use `set.seed` to ensure that a different vector will be generated.

Here we ise a different seed to produce a new random sample; we then check whether the new simulated sample differs from the previous random sample:

```{r }
set.seed(54321)
ds3 <- rnorm(1000)
# dplyr::glimpse( ds3 ) # uncomment to glimpse at the data
# better method
identical(ds, ds3)
```

We can assess the average by-row difference:

```{r }
#check equality
all.equal(ds , ds3)
```

Indeed, these differences are large enough to detect visually":

```{r layout="l-body-outset", fig.width=10, fig.height=10}

p2 <- ggplot2::qplot(ds3) + labs(title = "2d random number list")

# plot two graphs, each with different random samples
p1 + p2 + 
  plot_annotation("Random samples",  tag_levels = 'i')

```

### `runif`

We use r uniform to generate continuous data within a point range

```{r }
set.seed(123)
# 100 numbers between zero and 50
ds4 <- runif(n = 100, min = 0, max = 50)
dplyr::glimpse(ds4)

# how long is the vector?
length(ds4)

# visualise how are the data distributed?
hist(ds4, breaks = 100)
```

### `rep`

How can we generate random factors.

For this, R's `rep` function, `letters` function, and `LETTERS` function make happy friends. Here's how these functions may be combined:

Create lower case letters:

```{r  }
letters[1:3]
```

Create upper case letters:

```{r  }
LETTERS[4:10]
```

Create sequences using `each`

```{r  }
rep(letters[1:3], each = 3)
```

Create sequences using `times`

```{r  }
rep( letters[1:3], times = 3 )
```

Create uneven sequences:

```{r  }
rep( letters[1:3], times = c(3, 1, 4) )
```

Combine `each` + `times`:

```{r  }
rep(letters[1:3], each = 2, times = 3)
```

`length.out`

```{r  }
rep(letters[1:3], each = 2, length.out = 17)
```

Note `length.out` take priority over `times` -- use `length.out` if you have a fixed vector length.

### `seq`

Create a vector of numbers of a specific length:

```{r  }
seq(from = 1, to = 45, by = 1)
```

17 unit steps:

```{r  }
seq(from = 1, to = 45, by = 17)
```

17 steps:

```{r  }
seq(from = 1, to = 45, length.out = 17)
```

### Custom sequences

We can use vectors within random number generation

```{r  }
set.seed(123)
vdf<-rnorm(n = 20, 
           mean = c(0, 500, 1000, 10000), 
           sd = c(5,50,100,1000))

# we created a vector 
# dplyr::glimpse(vdf) # reveal to show structure

# quick graph
qplot(vdf, binwidth=4) # note the combined means + sds around them.
```

## Why simulate?

### Create noise to understand inference

Recall the relationship between mother's heights and daughters heights in the Pearson/Fox dataset:

```{r}
# recall this model
explore_md <- ggplot2::ggplot(data = md_df,
                              aes(y = daughter_height,
                                  x = mother_height)) +
  geom_jitter(alpha = .2) +
  labs(title = "The relationship between mothers height and daughter's height") +
  ylab("Daughter's height") +
  xlab("Mother's height") +
  theme_classic()

# plot
explore_md
```

What would a random relationship look like?

Simulation can help us to address this question.

```{r layout="l-body-outset", fig.width=12, fig.height=10, code_folding = FALSE}
# average daughter height
av_dh <- mean(md_df$daughter_height, na.rm = TRUE)

# sd of daughter height
sd_dh <- sd(md_df$daughter_height, na.rm = TRUE)

# average mother height
av_mh <- mean(md_df$mother_height, na.rm = TRUE)

# sd of mother height
sd_mh <- sd(md_df$mother_height, na.rm = TRUE)

# number of obs
N <- nrow(md_df) # 5524

# fake data
# simulate values for these parameters but do not relate the two parameters
sim_dh = rnorm(N, av_dh, sd_dh)
sim_mh = rnorm(N, av_mh, sd_mh)

# create a datframe of the simulations
sim_df_md <- data.frame(sim_dh, sim_mh)

# graph the data
fake_md <- ggplot2::ggplot(data = sim_df_md,
                           aes(y = sim_dh, x = sim_mh)) +
  geom_jitter(alpha = .2) +
  geom_smooth(method = lm) +
  theme_classic() +
  labs(title = "Fake data relationship between mothers height and daughter's height") +
  ylab("Daughter's height") +
  xlab("Mother's height")


# real data
explore_md <-ggplot2::ggplot(data = md_df, aes(y = daughter_height, x = mother_height)) + 
  geom_jitter(alpha = .2) + 
  labs(title = "The relationship between mothers height and daughter's height") +
       ylab("Daughter's height") +
       xlab("Mother's height") + theme_classic() + 
  geom_smooth(method = lm)

# plot uncorrelated data against data plot
library(patchwork)
fake_md + explore_md  + plot_annotation(tag_levels = "a")
```

What would a postive linear relationship look like?

Simulation can help us to address this question too.

```{r code_folding = FALSE}
N <- nrow(md_df)
# average mother height
set.seed(123)
md_df$daughter_height_c = scale(md_df$daughter_height, scale = FALSE) # center but not scale
md_df$mother_height_c = scale(md_df$mother_height, scale = FALSE) # center but not scale
mh_fake_c <- runif(N,
  min = min(as.numeric(md_df$mother_height_c), na.rm=TRUE),
  max = max(as.numeric(md_df$mother_height_c), na.rm = TRUE))

# let's take the intercept as the average height of daughters
a <- rnorm(N, mean = , mean(md_df$daughter_height), sd = 5)
# recall the beta for the model was .55,
b <- rnorm(N,mean = .55, sd = .1)

# now the outcome: this is just the linear model:
dh_fake <-  a + b * mh_fake_c

md_fake <- data.frame(mh_fake_c, dh_fake)

mod_fake <-lm(dh_fake ~ mh_fake_c, data = md_fake)

# Simulated relationship 
sim_plot <- plot(
  ggeffects::ggpredict(mod_fake,
                       terms = c("mh_fake_c [all]")),
  add.data = TRUE,
  dot.alpha = .2
)  + labs(title = "Simulated relationship") +
  xlab("simulated mother height") +  ylab("simulated daughter height")

# measured relationship
mod_real <-lm(daughter_height ~ mother_height, data = md_df)

# Simulated relationship 
real_plot <- plot(
  ggeffects::ggpredict(mod_real,
                       terms = c("mother_height [all]")),
  add.data = TRUE,
  dot.alpha = .2
)  + labs(title = "Actual relationship") +  ylab("simulated daughter height")

# graph both
library(patchwork)
sim_plot + real_plot +
  plot_annotation("Simulated Model and Data-based Model",
                  subtitle = "Daughters heights predicted by mother's heights")

```

What do we learn from the real data that we cannot obtain from the fake data?

Here's a shortcut we will occasionally use to simulate a dependency

```{r}
sim_dh2 = rnorm(N, sim_mh)
# quick plot
plot(sim_dh2 ~ sim_mh) + title(main = "Qucik simulation of a data dependency")
```

We can quickly generate a negative relationship

```{r}
sim_dh3 = rnorm(N, -sim_mh)
# quick plot
plot(sim_dh3 ~ sim_mh) + title(main = "Qucik simulation of a negative data dependency")
```

Increase the standard deviation from 1 to 5

```{r}
sim_dh4 = rnorm(N, -sim_mh, sd = 5)
# quick plot
plot(sim_dh4 ~ sim_mh) + title(main = "Qucik simulation of a data dependency with larger standard deviation")
```

### We can use simulation to explore potential problems with a sample

We might think there is a relationship when we know (owing to simulation) that there is no relationship.

Do you see a linear relationship?

```{r  }
set.seed(123)
# no relationship between x and y
x = rnorm(n = 10, mean = 0, sd = 1)
y = rnorm(n = 10, mean = 0, sd = 1)

bar_df <- data.frame(x,y)
# Barely significant linear model? 
summary(bad <-lm(y ~ x , data = bar_df))
ggplot2::ggplot(bar_df,aes(y,x)) + geom_point() + geom_smooth(method=lm)
```

Is this a one off?

<!-- Someone might try bootstrapping -->

<!-- ```{r} -->

<!-- # library -->

<!-- library(simpleboot) -->

<!-- bader<-simpleboot::lm.boot(bad, R = 1000) -->

<!-- sam<-simpleboot::plot(bader) -->

<!-- perc.lm(bader, p = c(0.1, 0.9)) -->

<!-- ``` -->

Go Bayesian? Note that the default implies a reliable association

```{r code_folding = FALSE}
bad_bayes <- brms::brm(y ~ x, data = bar_df, file = here::here("models", "bad_bayes"))
bayestestR::describe_posterior( bad_bayes )

# find the coefficient does not cross zero!
plot( parameters::model_parameters( bad_bayes ) )
```

We can replicate a result many times, without relying on one seeded draw.

The are two steps.

First we make our one off simulation into a function. We do this so that the simulation can be repeated many times.

Here is a function:

```{r}
# function for simulating a relationship
simple_sim = function(mn, sd) {
  # we will plug numbers in for 'mn' and 'sd'
  x_out = rnorm(mn, sd) # random x
  y_out = rnorm(mn, sd) # random y (uncorrelated)
  dat = data.frame(x_out, y_out) # bind into a dataframe
  lm(y_out ~ x_out, data = dat) # linear model
}

# try it out
set.seed(123)
m1<-simple_sim(10,1) # n = 10 
sjPlot::tab_model(m1) # almost significant! 
```

Second, we use `replicate` to generate many outcomes from this function:

```{r}
sms = replicate(100, simple_sim(10,1), simplify = FALSE ) # make 100 examples
# we set simplify to "FALSE" to recover a list
sms[[100]] # here is the 100th outcome
```

Combine `purrr` and `broom` to get the simulation

```{r}
library(purrr)
library(broom)
## all regressions
# map(sms, coef)

map_dfr(sms, broom::tidy)
```

How many p.values are less than or equalt to p = .05?

```{r}
mapped<-map_dfr(sms, broom::tidy)%>%
  filter(term == "x_out") # note we only want the coefficients not the intercept

# In 5 cases we find p <=.05
sum(mapped$p.value <=.05) / length(mapped$p.value) # we find 5% of the simulations yield "significant values"
```

Is this surprising?

Which proportion is negative and statistically significant?

```{r code_folding = FALSE}
sum((mapped$estimate < 0) &  mapped$p.value <=.05 )/ length(mapped$estimate) # we find 5% of the simulations yield "significant values"
```

And which proportion is positive?

```{r code_folding = FALSE}

sum((mapped$estimate > 0) &  mapped$p.value <=.05 )/ length(mapped$estimate) # we find 5% of the simulations yield "significant values"
```

**What does this simulation suggest to you about science?**

### Simulate a relationship between two variables

<!-- ```{r  } -->

<!-- ### Simulate a relationship between two variables  -->

<!-- library(ggplot2) -->

<!-- N = 1000 -->

<!-- weight <-runif(N, min = 50, max =100) -->

<!-- b <- rlnorm(N,.78,.1) # "rlnorm" is forcing b to be positive -->

<!-- sigma <- runif(N, 0 , 10 ) -->

<!-- height = weight * b -->

<!-- # simulated height/ weight data -->

<!-- df<-data.frame(height,weight) -->

<!-- m0<-lm(height ~ weight, data = df) -->

<!-- plot(ggeffects::ggpredict(m0, terms = c("weight")), -->

<!--      add.data = TRUE, -->

<!--      dot.alpha = .8) + -->

<!--   labs(x = "simulated weight", -->

<!--        y = "simulated height", -->

<!--        title = "simulated linear relationship") -->

<!-- ``` -->

### Simulate a non-linear relationship

<!-- #  -->

<!-- #  -->

<!-- # a <-160 -->

<!-- # b <- c(2, 0.75) -->

<!-- # x <- rnorm(100) -->

<!-- # y <- rnorm(100, mean = b[1] * exp(b[2] * x)) -->

<!-- # dat1 <- data.frame(x, y) -->

<!-- # plot(y ~ x , data = dat1) -->

```{r}
#Polynomial
N <- 1000

# simulate weights
weight <- runif(N, min = 60, max = 120)
weight_c <- scale(weight, scale = FALSE)

# simulate coefficients
a = rnorm(N, mean = 180 , 10)
b1 = rnorm(N, mean = 2.2, .01)
b2 = -rnorm(N, mean = .02, .001)

height <- a + b1 * weight_c  +  b2 * weight_c ^ 2

# simulated height/ weight data

df1 <- data.frame(height, weight_c, weight)

plot(height ~ weight)
```

Let's fit a linear model to the data and graph the results

```{r code_folding = FALSE}
m1 <- lm(height ~ weight_c, data = df1)

# graph model
plot(ggeffects::ggpredict(m1, terms = c("weight_c [all]")),
     add.data = TRUE,
     dot.alpha = .2)  + labs(title = "simulated linear relationship") +
  xlab("simulated weight") +  ylab("simulated height")

sjPlot::tab_model(m1)
```

Let's fit a non-linear model and graph the results

```{r code_folding = FALSE}
m2 <-lm(height ~ weight_c + I(weight_c^2), data = df1)

plot(ggeffects::ggpredict(m2, terms = c("weight_c [all]")),
     add.data = TRUE,
     dot.alpha = .2) + labs( title = "simulated linear relationship") + 
  xlab("simulated weight") +  ylab("simulated height")
```

Let's fit a non-linear model using splines and graph the results

```{r code_folding = FALSE}
m3 <-lm(height ~ bs(weight_c), data = df1)

plot(ggeffects::ggpredict(m3, terms = c("weight_c")),
     add.data = TRUE,
     dot.alpha = .2)  + labs(title = "simulated linear relationship") +
  xlab("simulated weight") +  ylab("simulated height")

```

Check the linear model and perform model checks:

```{r}
performance::check_model(m1)
```

The model looks OK.

Check quadratic model (polynomial = 2)

```{r}
performance::check_model(m2)
```

This model looks better

Check splines model.

```{r}
performance::check_model(m3)
```

The splines model also looks better.

We can compare model performance:

```{r}
performance::compare_performance(m1, m2, m3)
```

Which do we prefer?

### Create fake factors

The key thing to remember about regression with factors is that the intercept is intercept as the lowest category of the factor.

```{r code_folding = FALSE}
N <- 200 # number of observations
#group <- rep((0:1), length.out = 200) # 2 groups
group <- rep(c("m","n_m"), each = N/2) #equivalent:
a <- rnorm(N, 150, 3) # intercept
b1 <- rnorm(N, 20, 1) # coefficient of "b
sigma = rexp(N,1)# error term
outcome <- rnorm(N, mean = a + b1 * (group == "m"), sigma)

df <-data.frame(outcome,group)
#dplyr::glimpse(df)


#model removing the intercept to show the difference
ms<-lm(outcome ~ group, data = df)

# graph
sjPlot::plot_model(ms)

#table
sjPlot::tab_model(ms)
```

The key thing to remember about regression with factors is that the intercept is intercept as the lowest category of the factor.

To see this we can remove the intercept by including `-1` in the model. This recovers contrasts:

```{r code_folding = FALSE}
#model removing the intercept to show the difference
ms2<-lm(outcome ~ -1 + group, data = df)

# graph
sjPlot::plot_model(ms2)

# table
sjPlot::tab_model(ms2)
```

### Simulate a relationship between two factors

Simulate two groups with mean = 6 and mean = 12

```{r code_folding = FALSE}
cdf <- data.frame(group = rep(letters[1:2], length.out = 100), # 2 groups
                  response = rnorm(n = 100, mean = c(3, 12), sd = 1)) # mean of 3 mean of

head(cdf)
```

Model:

```{r code_folding = FALSE}
m_gr<- lm(response ~ -1 +  group, data = cdf)
```

Results

```{r code_folding = FALSE}
sjPlot::tab_model(m_gr)
```

### Is imbalance in my study causing a problem?

```{r code_folding = FALSE}
### Is imbalance wrecking my inference? 
set.seed(123)
N <- 120
cells <-rep( letters[1:2], times = c(15, 105))

a <- rnorm(N, 2, 1)
b1 <- rnorm(N, .2, .1)
sigma <- rexp(N,1)

out <- rnorm(N, mean = a + b1 * (cells == "b"), sigma)
dfc<-data.frame(out,cells)
sim_cells<-lm(out ~ cells, data = dfc)

sjPlot::tab_model(sim_cells)
```

This isn't too convincing. We need to replicate the model many times

```{r code_folding = FALSE}
# Make a function for the simulation
set.seed(12)
test_fun = function() {
  N <- 120
  cells <-rep( letters[1:2], times = c(110, 10))
  a <- rnorm(N, 2, 1)
  b1 <- rnorm(N, 1, .1)
  sigma <- rexp(N, 1)
  out <- rnorm(N, mean = a + b1 * (cells == "b"), sigma)
  dfc <- data.frame(out, cells)
  sim_cells <- lm(out ~ cells, data = dfc)
  sim_cells
}

r_lm = replicate(20, test_fun(), simplify = FALSE )
length(r_lm)
```

We can use the `purrr`and `broom` packages to generate many replicates of a model and summarise them, and ask: What percentage of simulations yield statistically significant results?

```{r code_folding = FALSE}
tab_sim<-purrr::map_dfr(r_lm, broom::tidy)
mout<-tab_sim %>%
  dplyr::filter(term =="cellsb")%>%
  dplyr::mutate_if(is.numeric, round, 5)

# calculate proportion
sum(mout$p.value <= .05) / length(mout$p.value)
```

Does increasing the cells the issue?

```{r}
# Make a function for the simulation
set.seed(12)
test_fun = function(cell1, cell2) {
  N <- 120
  cells <-rep( letters[1:2], times = c(cell1, cell2))
  a <- rnorm(N, 2, 1)
  b1 <- rnorm(N, 1, .1)
  sigma <- rexp(N, 1)
  out <- rnorm(N, mean = a + b1 * (cells == "b"), sigma)
  dfc <- data.frame(out, cells)
  sim_cells <- lm(out ~ cells, data = dfc)
  sim_cells
}

# balanced cells of 60 each
sim_lm = replicate(20, test_fun(60,60), simplify = FALSE )
length(sim_lm)
```

We can use the `purrr` package to generate many replicates of a model.

The simulation suggests that balances of 60/60 is good here.

```{r code_folding = FALSE}
# summarise coefficients 
tab_sim2<-purrr::map_dfr(sim_lm, broom::tidy)

# obtain only the treatment coefficient
mout2<-tab_sim2 %>%
  dplyr::filter(term =="cellsb")%>%
  dplyr::mutate_if(is.numeric, round, 5)

sum(mout2$p.value <= .05) / length(mout2$p.value)
```

## Acknowledgments

Much in my approach to teaching simulation owes to Ariel Muldoon. PLease check out Ariel's wonderful R webpage here: <https://aosmith.rbind.io>

Try out Ariel's tutorial for simulation functions [here](https://aosmith.shinyapps.io/tutorial_simulation_helper_functions/#section-simulating-datasets)
