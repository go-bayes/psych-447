[
  {
    "path": "workbooks/W_8_s/",
    "title": "Week 8 workbook and solutions",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-28",
    "categories": [],
    "contents": "\n\nContents\nAssessment\n\nImport the jittered NZAVS dataset, selecting the 2018 Wave\n\n\n### Libraries\nlibrary(\"tidyverse\")\nlibrary(\"patchwork\")\nlibrary(\"lubridate\")\nlibrary(\"kableExtra\")\nlibrary(\"gtsummary\")\nlibrary(\"lubridate\")\n#devtools::install_github(\"data-edu/tidyLPA\")\nif (!require(tidyLPA)) {\n  install.packages(\"tidyLPA\")\n}\n\n\n\n\n\n\n# read data\n\nnz_0 <- as.data.frame(readr::read_csv2(\n  url(\n    \"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nzj.csv\"\n  )\n))\n\n\n# to relevel kessler 6 variables\nf <-\n  c(\n    \"None Of The Time\",\n    \"A Little Of The Time\",\n    \"Some Of The Time\",\n    \"Most Of The Time\",\n    \"All Of The Time\"\n  )\n\n# get data into shape\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(male_id = as.factor(Male)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE)%>%\n  dplyr::filter(Wave == 2018)\n\n\n\n\n\ndplyr::glimpse(nz)\n\n\nRows: 2,918\nColumns: 82\n$ Id                          <dbl> 1, 4, 5, 6, 7, 9, 12, …\n$ Wave                        <fct> 2018, 2018, 2018, 2018…\n$ years                       <dbl> 9.921971, 9.900068, 9.…\n$ Age                         <dbl> 43.85489, 47.49350, 33…\n$ Male                        <fct> Not_Male, Male, Not_Ma…\n$ Gender                      <dbl> 0, 1, 0, 1, 0, 0, 0, 1…\n$ w.GendAgeEthnic             <dbl> 0.4022481, 4.1207264, …\n$ Edu                         <dbl> 7, 7, 7, 7, 7, 1, 4, 9…\n$ Partner                     <dbl> 1, 1, 1, 1, 0, 0, 1, 1…\n$ BornNZ                      <dbl> 1, 1, 1, 1, 1, 1, 1, 1…\n$ Employed                    <dbl> 1, 1, 1, 1, 1, 0, 0, 1…\n$ BigDoms                     <fct> Not_Rel, Not_Rel, Not_…\n$ TSCORE                      <dbl> 3685, 3677, 3687, 3399…\n$ GenCohort                   <fct> GenX: born >=1961 & b.…\n$ Hours.Exercise              <dbl> 2, 10, 1, 3, 10, 4, 0,…\n$ Hours.Work                  <dbl> 16.0, 65.0, 0.0, 35.0,…\n$ Hours.News                  <dbl> 2.0, 4.0, 0.5, 5.0, 1.…\n$ Hours.Internet              <dbl> 7, 15, 10, 20, 7, 0, 5…\n$ Hours.SocialMedia           <dbl> 4, 5, 10, 2, 2, 0, 2, …\n$ HoursCharity                <dbl> 6, 2, 3, 0, 0, 0, 0, 0…\n$ Hours.CompGames             <dbl> 0, 0, 5, 2, 0, 0, 2, 0…\n$ Hours.Family                <dbl> 7.00, 1.00, 10.00, 0.0…\n$ Hours.Friends               <dbl> 4, 3, 5, 2, 10, 0, 8, …\n$ Hours.Community             <dbl> 0, 0, 2, 0, 0, 0, 4, 0…\n$ Hours.Religious             <dbl> 0, 5, 0, 0, 0, 0, 0, 0…\n$ CharityDonate               <dbl> 200, 100, 200, 0, 50, …\n$ Family.Money                <dbl> 0, 0, 0, 0, 200, 0, 0,…\n$ Friends.Money               <dbl> 0, 0, 0, 0, 0, 0, 0, 0…\n$ Community.Money             <dbl> 0, 0, 0, 0, 0, 0, 0, 0…\n$ Family.Time                 <dbl> 0, 0, 12, 0, 0, 0, 0, …\n$ Friends.Time                <dbl> 0, 0, 1, 0, 0, 0, 0, 0…\n$ Community.Time              <dbl> 0, 0, 0, 0, 0, 0, 0, 0…\n$ Household.INC               <dbl> 200000, 40000, 140000,…\n$ Issue.IncomeRedistribution  <dbl> 6, 4, 4, 7, 7, 4, 3, 5…\n$ Religion.Church             <dbl> 0, 0, 0, 0, 0, 0, 0, 0…\n$ Religion.Believe.Cats       <dbl> 4, 1, 4, 1, 1, 1, 3, 3…\n$ Relid                       <dbl> 0, 0, 0, 5, 0, 4, 0, 0…\n$ HLTH.Fatigue                <dbl> 1, 2, 2, 1, 1, 1, 3, 1…\n$ HLTH.SleepHours             <dbl> 7.0, 4.0, 6.0, 7.0, 7.…\n$ HLTH.BMI                    <dbl> 19.83471, 13.14828, 26…\n$ HLTH.Weight                 <dbl> 54.0, 45.0, 70.0, 70.0…\n$ HLTH.Height                 <dbl> 1.65, 1.85, 1.63, 1.80…\n$ HomeOwner                   <dbl> 1, 0, 1, 0, 1, 0, 1, 1…\n$ Pol.Orient                  <dbl> 5, 3, 4, 1, 2, 4, 7, 2…\n$ PATRIOT                     <dbl> 6.0, 7.0, 7.0, 7.0, 6.…\n$ Env.SatNZEnvironment        <dbl> 7, 7, 9, 3, 3, 3, 6, 6…\n$ Env.MotorwaySpend           <dbl> 3, 5, 4, 3, 5, 7, 5, 6…\n$ Env.PubTransSubs            <dbl> 7, 5, 6, 7, 6, 4, 7, 5…\n$ Env.ClimateChgConcern       <dbl> 6, 7, 5, 7, 7, NA, 7, …\n$ LIFEMEANING                 <dbl> 6.0, 4.5, 5.0, 7.0, 7.…\n$ Issue.GovtSurveillance      <dbl> 5, 3, 4, 7, 6, 7, 6, 4…\n$ Issue.RegulateAI            <dbl> 6, 4, 5, 4, 2, 5, 4, 6…\n$ CONSCIENTIOUSNESS           <dbl> 5.250000, 5.500000, 2.…\n$ EXTRAVERSION                <dbl> 4.00, 4.00, 4.75, 2.25…\n$ AGREEABLENESS               <dbl> 6.75, 6.00, 6.00, 4.25…\n$ OPENNESS                    <dbl> 2.00, 4.25, 4.25, 6.25…\n$ Religious                   <fct> Not_Religious, Not_Rel…\n$ Spiritual.Identification    <dbl> 2, 5, 2, 7, 7, NA, 4, …\n$ Believe.God                 <fct> Not Believe God, Belie…\n$ Believe.Spirit              <fct> Not Believe Spirit, Be…\n$ Your.Personal.Relationships <dbl> 10, 2, 7, 10, 8, 10, 2…\n$ Your.Future.Security        <dbl> 10, 6, 8, 7, 8, 8, 2, …\n$ Standard.Living             <dbl> 10, 6, 9, 10, 8, 7, 10…\n$ NZ.Economic.Situation       <dbl> 8, 6, 4, 10, 3, 2, 3, …\n$ NZ.Social.Conditions        <dbl> 7, 6, 3, 4, 3, 3, 0, 7…\n$ NZ.Business.Conditions      <dbl> 8, 6, 5, 10, 3, 6, 2, …\n$ Emp.JobSecure               <dbl> 7, 6, 5, 7, 6, NA, NA,…\n$ Emp.JobSat                  <dbl> 6, 6, 5, 7, 6, NA, NA,…\n$ Emp.JobValued               <dbl> 7, 5, 5, 7, 6, NA, NA,…\n$ Issue.Food.GMO              <dbl> 6, 5, 6, 7, 7, 7, 4, 5…\n$ Env.SacMade                 <dbl> NA, NA, NA, NA, NA, NA…\n$ KESSLER6sum                 <dbl> 4, 7, 10, 2, 3, 2, 8, …\n$ POLICE.ENGAGE               <dbl> 7.0, 4.5, 7.0, 6.5, 6.…\n$ POLICE.TRUST                <dbl> 5.666667, 4.333333, 5.…\n$ FeelHopeless                <fct> A Little Of The Time, …\n$ FeelDepressed               <fct> None Of The Time, None…\n$ FeelRestless                <fct> A Little Of The Time, …\n$ EverythingIsEffort          <fct> A Little Of The Time, …\n$ FeelWorthless               <fct> None Of The Time, None…\n$ FeelNervous                 <fct> A Little Of The Time, …\n$ male_id                     <fct> Not_Male, Male, Not_Ma…\n$ date                        <date> 2019-08-02, 2019-07-2…\n\nAssessment\nInformation\nLink to the NZAVS data dictionary is here\nLink to questions only here\nTask\nWrite brief report that predicts belief in spirit or a life-force (Believe.Spirit) from no more than five covariates. Explain your model and results.\nWrite report that predicts charitable donations (CharityDonate) wrom no more than five covariates. Explain your model and results.\nMarking criteria\nClarity and organisation in the descriptive component of your work.\nClarity and organisation in description of your regression model.\nAccuracy and insight in the interpretation of your regression model.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-28T18:06:23+12:00",
    "input_file": {}
  },
  {
    "path": "workbooks/W_7_s/",
    "title": "Week 7 workbook and solutions",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-27",
    "categories": [],
    "contents": "\n\nContents\nAssessment\n\nImport the jittered NZAVS dataset:\n\n\n### Libraries\nlibrary(\"tidyverse\")\nlibrary(\"patchwork\")\nlibrary(\"lubridate\")\nlibrary(\"kableExtra\")\nlibrary(\"gtsummary\")\nlibrary(\"lubridate\")\ndevtools::install_github(\"data-edu/tidyLPA\")\nif (!require(tidyLPA)) {\n  install.packages(\"tidyLPA\")\n}\n\n\n\n\n\n\n# read data\n\nnz_0 <- as.data.frame(readr::read_csv2(\n  url(\n    \"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nzj.csv\"\n  )\n))\n\n\n# to relevel kessler 6 variables\nf <-\n  c(\n    \"None Of The Time\",\n    \"A Little Of The Time\",\n    \"Some Of The Time\",\n    \"Most Of The Time\",\n    \"All Of The Time\"\n  )\n\n# get data into shape\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(male_id = as.factor(Male)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE)%>%\n  dplyr::filter(Wave == 2019)\n\n\n\n\n\ndplyr::glimpse(nz)\n\n\nRows: 2,560\nColumns: 82\n$ Id                          <dbl> 2, 4, 6, 7, 9, 12, 14,…\n$ Wave                        <fct> 2019, 2019, 2019, 2019…\n$ years                       <dbl> 10.09446, 10.60917, 10…\n$ Age                         <dbl> 69.82957, 48.49350, 36…\n$ Male                        <fct> Male, Male, Male, Not_…\n$ Gender                      <dbl> 1, 1, 1, 0, 0, 0, 1, 0…\n$ w.GendAgeEthnic             <dbl> 1.0999955, 3.6685922, …\n$ Edu                         <dbl> 4, 7, 7, 7, 8, 4, 9, 1…\n$ Partner                     <dbl> 1, 1, 1, 0, 0, 1, 1, 1…\n$ BornNZ                      <dbl> 0, 1, 1, 1, 1, 1, 1, 1…\n$ Employed                    <dbl> 0, 1, 1, 1, 0, 1, 1, 1…\n$ BigDoms                     <fct> Not_Rel, Not_Rel, Not_…\n$ TSCORE                      <dbl> 3748, 3936, 3749, 3749…\n$ GenCohort                   <fct> Gen Boombers: born >= …\n$ Hours.Exercise              <dbl> 4, 7, 1, 10, 7, 4, 5, …\n$ Hours.Work                  <dbl> 0.0, 35.0, 35.0, 40.0,…\n$ Hours.News                  <dbl> 6.0, 5.0, 5.0, 1.0, 15…\n$ Hours.Internet              <dbl> 24.0, 14.0, 20.0, 25.0…\n$ Hours.SocialMedia           <dbl> 0.0, 0.0, 5.0, 5.0, 4.…\n$ HoursCharity                <dbl> 0, 0, 0, 0, 2, 0, 0, 0…\n$ Hours.CompGames             <dbl> 0, 0, 5, 0, 10, 5, 0, …\n$ Hours.Family                <dbl> 10.0, 0.0, 0.0, 3.0, 5…\n$ Hours.Friends               <dbl> 0.0, 0.0, 5.0, 3.0, 1.…\n$ Hours.Community             <dbl> 0, 0, 0, 3, 0, 0, 0, 0…\n$ Hours.Religious             <dbl> 0, 0, 0, 0, 0, 0, 0, 0…\n$ CharityDonate               <dbl> 0, 300, 5, 50, 500, 0,…\n$ Family.Money                <dbl> 0, 0, 0, 0, 0, 0, 0, 0…\n$ Friends.Money               <dbl> 0, 0, 0, 0, 0, 0, 0, 0…\n$ Community.Money             <dbl> 0, 0, 0, 0, 0, 0, 0, 0…\n$ Family.Time                 <dbl> 0, 4, 0, 0, 0, 0, 0, 0…\n$ Friends.Time                <dbl> 0, 10, 0, 0, 0, 0, 0, …\n$ Community.Time              <dbl> 0, 10, 0, 0, 0, 0, 0, …\n$ Household.INC               <dbl> 39000, 35000, 140000, …\n$ Issue.IncomeRedistribution  <dbl> 3, 4, 7, 7, 7, 5, 5, 7…\n$ Religion.Church             <dbl> 0, 0, 0, 0, 0, 0, 0, 0…\n$ Religion.Believe.Cats       <dbl> 4, 1, 4, 1, 4, 3, 3, 3…\n$ Relid                       <dbl> 0, 0, 0, 0, 0, 0, 0, 0…\n$ HLTH.Fatigue                <dbl> 0, 1, 1, 1, 0, 0, 0, 0…\n$ HLTH.SleepHours             <dbl> 8.0, 6.0, 7.5, 7.0, 8.…\n$ HLTH.BMI                    <dbl> 26.23457, 35.06209, 22…\n$ HLTH.Weight                 <dbl> 85, 120, 72, 77, 91, 6…\n$ HLTH.Height                 <dbl> 1.80, 1.85, 1.80, 1.76…\n$ HomeOwner                   <dbl> NA, NA, NA, NA, NA, NA…\n$ Pol.Orient                  <dbl> 4, 5, 1, 2, 2, 4, 3, 4…\n$ PATRIOT                     <dbl> 5.0, 6.5, 7.0, 6.0, 7.…\n$ Env.SatNZEnvironment        <dbl> 9, 7, 5, 2, 6, 4, 6, 5…\n$ Env.MotorwaySpend           <dbl> 4, 3, 3, 4, 4, 6, 6, 7…\n$ Env.PubTransSubs            <dbl> 4, 5, 7, 6, 7, 5, 5, 7…\n$ Env.ClimateChgConcern       <dbl> 4, 7, 7, 7, 6, 5, 7, 7…\n$ LIFEMEANING                 <dbl> 4.0, 5.0, 6.5, 7.0, 7.…\n$ Issue.GovtSurveillance      <dbl> 4, 3, 2, 2, 2, 7, 4, 7…\n$ Issue.RegulateAI            <dbl> NA, NA, NA, NA, NA, NA…\n$ CONSCIENTIOUSNESS           <dbl> 4.00, 5.50, 5.00, 5.00…\n$ EXTRAVERSION                <dbl> 4.25, 4.75, 2.25, 5.00…\n$ AGREEABLENESS               <dbl> 3.50, 5.00, 5.25, 6.50…\n$ OPENNESS                    <dbl> 3.50, 4.25, 6.00, 6.50…\n$ Religious                   <fct> Not_Religious, Not_Rel…\n$ Spiritual.Identification    <dbl> NA, NA, NA, NA, NA, NA…\n$ Believe.God                 <fct> Not Believe God, Belie…\n$ Believe.Spirit              <fct> Not Believe Spirit, Be…\n$ Your.Personal.Relationships <dbl> 10, 2, 10, 9, 7, 8, 8,…\n$ Your.Future.Security        <dbl> 9, 8, 9, 8, 9, 8, 8, 8…\n$ Standard.Living             <dbl> 10, 8, 9, 8, 10, 10, 8…\n$ NZ.Economic.Situation       <dbl> 9, 2, 9, 2, 7, 5, 6, 5…\n$ NZ.Social.Conditions        <dbl> 8, 2, 6, 2, 3, 4, 6, 5…\n$ NZ.Business.Conditions      <dbl> 8, 2, 9, 2, 5, 8, 5, 4…\n$ Emp.JobSecure               <dbl> NA, 6, 7, 4, NA, 6, 5,…\n$ Emp.JobSat                  <dbl> NA, 6, 7, 6, NA, 7, 7,…\n$ Emp.JobValued               <dbl> NA, 6, 7, 5, NA, 5, 6,…\n$ Issue.Food.GMO              <dbl> 4, 5, 7, 7, 1, 4, 4, 7…\n$ Env.SacMade                 <dbl> NA, NA, NA, NA, NA, NA…\n$ KESSLER6sum                 <dbl> 0, 7, 0, 3, 4, 6, 0, 3…\n$ POLICE.ENGAGE               <dbl> 4.0, 3.5, 6.0, 6.0, 7.…\n$ POLICE.TRUST                <dbl> 4.333333, 4.333333, 6.…\n$ FeelHopeless                <fct> None Of The Time, A Li…\n$ FeelDepressed               <fct> None Of The Time, None…\n$ FeelRestless                <fct> None Of The Time, Most…\n$ EverythingIsEffort          <fct> None Of The Time, A Li…\n$ FeelWorthless               <fct> None Of The Time, None…\n$ FeelNervous                 <fct> None Of The Time, Some…\n$ male_id                     <fct> Male, Male, Male, Not_…\n$ date                        <date> 2019-10-04, 2020-04-0…\n\nAssessment\nInformation\nLink to the NZAVS data dictionary is here\nLink to questions only here\nTask\nSelect up to five variables from Wave 11 (2019) NZAVS jittered dataset.\nBriefly describe your variables. Use graphs and tables as you deem helpful.\nWrite a single co-variate regression model with a continuous outcome. Interpret your results, using graphs and tables as you deem helpful.\nWrite a multiple co-variate regression model and interpret your results, using graphs and tables as you deem helpful.\nMarking criteria\nClarity and organisation in the descriptive component of your work.\nClarity and organisation in description of your regression model.\nAccuracy and insight in the interpretation of your regression model.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-27T21:21:18+12:00",
    "input_file": {}
  },
  {
    "path": "workbooks/W_1/",
    "title": "Week 1 Workbook",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\n\nContents\nTask 1 Apply for a github education account\nTask 2 Download R for free here:\nTask 3 Download Rstudio for free here:\nTask 4 Create your first repository\nTask 5 Commit your first Rmarkdown journal\nGrading points\nSubmit a link to your journal on Blackboard.\n\nTask 1 Apply for a github education account\napply here: https://education.github.com/pack/offers\nset up a github account.\nchoose a username that will be OK for professional purposes (e.g. your name)\nTask 2 Download R for free here:\nhttps://www.r-project.org\nTask 3 Download Rstudio for free here:\nhttps://rstudio.com/products/rstudio/download/#download\nUpdate R and Rstudio to the latest versions. Don’t worry if you have problems, we’ll be working through your problems in the first week, and throughout the course. We assume no prior experience with R or Git/GitHub\nTask 4 Create your first repository\nTask 5 Commit your first Rmarkdown journal\nCreate your first Rmarkdown document, and title it “My Journal”\nRecord your impressions of the process of getting set up with RStudio and GitHub\nRecord any help you have offered to other people, and any help you have sought from other people, along the way.\nSave the document and commit it to GitHub.\nNote: set your github repository to private if you wish the account to be private\nGrading points\nYour Rmarkdown document must include each of the following\n\nHeading\n\n\ncrossed out text\n\n\nlink to a website\n\n\nA footnote 1\n\n\nAn inspirational quote\n\n\nBONUS: figure out how to add a bibliographic citation, such as [@darwin1964origin]\n\n\nA record of help you sought and offered.\n\nSubmit a link to your journal on Blackboard.\nBlackboard isn’t our favourite tool, however, we need to respect the norms of our community. For this reason, presently all of your assessments need to be uploaded through Blackboard.\nOn Blackboard, pass us a link to your GitHub document on Blackboard for weekly assessments\nThis journal will count as both your weekly journal entry and your weekly workbook – double credits!\n\nHumanities scholars love footnotes.↩︎\n",
    "preview": {},
    "last_modified": "2021-04-23T15:50:24+12:00",
    "input_file": {}
  },
  {
    "path": "workbooks/W_2/",
    "title": "Week 2 Workbook",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\n\nContents\nCreate Repos (if you have not already)\nCreate R projects\nWorkbook 2\n\nCreate Repos (if you have not already)\nCreate a repository for 447-journals (if you don’t have one already)\nCreate a repository for your 447-workbooks.\nYou should initialise a Readme statement and a .gitignore document (make sure to click the R .gitignore)\nCreate R projects\nCreate R-projects in each of these repositories.\nIn your workbooks folder, using the + New Folder command, create a folder called data (or similar) This is where you’ll store your data.\nCreate a file called figs (where you’ll stor your figures) 5 Create a folder called workbooks, or similar this is where you’ll store your weekly workbooks.\nWorkbook 2\nOpen an R markdown document, and filling in your name\nThe first code chunk is for your preferences.\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\nFind the keyboard shortcuts menu.\nMemorise the keyboard shortcut for insert code chunk which is: ⌥⌘I\nUse the mac to insert a code chunk.\nMemorise the shortcut for evaluate which is ⌘-return^ use for all tasks in this workbook.\nUse R to do 10 mathematical calculations\n\n\n4+5\n\n\n[1] 9\n\n\n\n3^3\n\n\n[1] 27\n\nShow your work\nUsing R, find the square root of 324\nIn the iris dataset, find the flower with the longest Sepal.Length. (hint, use the max function.) Show the column details for this flower only.\nIn the iris dataset, find the flower with the shortest Sepal.Length. (hint, use the min function.)\nCalculate the difference bewteen the flower with the longest and shortest Sepal.Length\nMake a 7 column x 100 row dataframe using c, :, dataframe\nRename the columns of the dataframe using the names of snow white dwarfs, or any names.\nUsing the dataset woman, write a linear model for height ~ weight\nCreate a table and coefficient plot for this model\nUsing ggeffects, create a prediction plot for this model.\n\nNext week we’ll be creating plots like this\n\n\nlibrary(ggplot2)\nggplot2::ggplot(data = women) + \n  geom_point(mapping = aes(x = height, y = weight)) + geom_smooth(method = lm, aes(x = height, y = weight)) + theme_classic() \n\n\n\n\nExplain what is being calculated here\n\n\nsum(women$weight > 140) / length(women$weight)\n\n\n[1] 0.4\n\nCalculate the proportion of women who are over the mean weight of women\n\n\n\n\n\n\nWhat are the advantages and disadvantages of creating more breaks in the Petal.Length indicator? Clarify your answer in a paragraph.\n\n\nhist(iris$Petal.Length)\n\n\n\n\n\n\nhist(iris$Petal.Length, breaks = 100)\n\n\n\n\nSpot the error in this code\n\n\n# here is one method. \nmh <- mean(women$height)\nsum(women$weight > mh) / length(women$height)\n\n\n\nReorder the columns of the woman dataset so that weight comes before height. Then rename the columns “w” and “h”.\n\n\n\nRead data into R using the following method:\n\n\n\n\n\n# read data from file\nlibrary(\"readr\")\ntestdata<- readr::read_csv(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/testdata1.csv\"))\nstr(testdata)\n\n\n\nSave data into your data folder using the following method\n\n\nlibrary(\"here\")\n# save data using here\nsaveRDS(testdata, here::here(\"data\", \"td.RDS\"))\n\n\n\nRead data back into R\n\n\ntd <- readRDS(here::here(\"data\", \"td.RDS\"))\nstr(td)\n\n\n\nUsing the td dataset, write a linear model for height ~ weight as above\n\n\n\nCreate a coefficient plot\nCreate a prediction plot\nExtra credit, how would you interpret the intercept in this model?*\n\n\n\n",
    "preview": "workbooks/W_2/workbook_2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-04-23T15:50:24+12:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "workbooks/W_3_s/",
    "title": "Week 3 Workbook Solutions",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\n\nContents\nLibraries we will need\nQuestion 1: Why is this graph not printing any output?\nSolution, there are no layers.\nQuestion 2. Using the mpg dataset, graph the relationship between city milage and highway mileage by year manufacture\nSolution\nQuestion 3. Edit this graph so that the x axis and the y axis both start at 0\nSolution\nQuestion 4: what is one benefits and one limitation of this graph (in which the x and y values start at 0?)\nSolution\nQuestion 5. Which of these two graphs do you prefer and why?\nSolution\nQuestion 6. add a facets to this graph for the “class” variable\nSolution\nQuestion 7. which graph is more informative and why?\nSolution\nQuestion 8. remove the legend from the facet graph above (g4)\nSolution\nQuestion 9 Convert the y variable to “numeric” and graph the relationship betweeen religiousity (x-axis) and `thr_mus`` (y-axis) in the ISSP dataset. Create new axis labels\nQuestion 10. Note that I have the following graph should start from 1 and run to 4 but currently runs from 0-4. Fix the graph\nSoution\nExtra question 11. Find one way of improving the the following code and explain your Solution\nSolution.\n\nLibraries we will need\n\n\nlibrary(\"tidyverse\")\nlibrary(\"patchwork\")\nlibrary(\"readr\")\nlibrary(\"sjPlot\")\nlibrary(\"MASS\")\n\ntheme_set(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1: Why is this graph not printing any output?\n\n\nlibrary(\"tidyverse\")\nggplot(data = mtcars) + \n  aes(mpg, wt, colour=factor(cyl))\n\n\n\n\nSolution, there are no layers.\nQuestion 2. Using the mpg dataset, graph the relationship between city milage and highway mileage by year manufacture\nSolution\n\n\nggplot(mpg, aes(\n  x = hwy,\n  y = cty,\n  colour = as.factor(year)\n)) +\n  geom_point() +\n  geom_smooth(method = loess)\n\n\n\n\nQuestion 3. Edit this graph so that the x axis and the y axis both start at 0\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\")\n\n\n\n\nSolution\n\n\n  ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") + \n  expand_limits(x = 0, y = 0)\n\n\n\n\nQuestion 4: what is one benefits and one limitation of this graph (in which the x and y values start at 0?)\nSolution\nExample benefits:\nWe clearly see that engine displacement starts at just below\nWe clearly understand that no highway mileage below 10 mpg doesn’t exist.\nExample weakness\nEven if the thresholds are not evident, we can already see the thresholds without starting at graphs at zero (i.e. there is no information)\nZero engine displacement and zero highway miles per gallon are not physically meaningful concepts, so starting the graphs at these thresholds adds no new information.\nQuestion 5. Which of these two graphs do you prefer and why?\n\n\ng1 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, colour =  class )) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\n\ng2 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape =  class )) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\n\nlibrary(\"patchwork\")\n\ng1 / g2 + plot_annotation(title = \"Which plot do you prefer and why?\", tag_levels = 'a')\n\n\n\n\nSolution\nArguably, patterns are easier to detect using the colour aesthetic but there are no hard and fast rules\nQuestion 6. add a facets to this graph for the “class” variable\n\n\ng2 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape =  class )) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\nSolution\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\nQuestion 7. which graph is more informative and why?\n\n\n\nSolution\nArguably the facets make the role of class more evident by making the class indicator more salient. Where possible,i t is a good idea to declutter your graph.\nQuestion 8. remove the legend from the facet graph above (g4)\n\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_wrap( ~ class, nrow = 2)\n\n\n\n\nSolution\n\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_wrap( ~ class, nrow = 2) +\n  theme(legend.position = \"none\") \n\n\n\n\nQuestion 9 Convert the y variable to “numeric” and graph the relationship betweeen religiousity (x-axis) and `thr_mus`` (y-axis) in the ISSP dataset. Create new axis labels\nDownload the ISSP questionaire used in this study [here]: (https://github.com/go-bayes/psych-447/blob/main/data_raw/ISSP/ISSP_2018_Religion_Questionnaire_final_version1-2.pdf)\nNote1\n\n\n# read the issp dataset for questionaire see: ISSP_2018_Religion_Questionnaire_final_version1-2.pdf\n# \n# subset of data from the issp dataset\nissp <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/issp.csv\"))\n\n\n\n\n\n# note that we need to check our data\nhead(issp)\n\n# when we do we find that classes of the variables need to be adjusted\nstr(issp)\n\n\n\nLet’s get the data into shape using dplyr. Run this code below\n\n\nip <- issp %>%\n  mutate(\n    id = factor(id),\n    thr_ath = as.factor(thr_ath),\n    thr_bd = as.factor(thr_bd),\n    thr_ch = as.factor(thr_ch),\n    thr_hd = as.factor(thr_hd),\n    thr_jw = as.factor(thr_jw),\n    thr_ms = as.factor(thr_ms),\n    neg_ath = as.factor(neg_ath),\n    neg_bd = as.factor(neg_bd),\n    neg_ch = as.factor(neg_ch),\n    neg_hd  = as.factor(neg_hd),\n    neg_jw = as.factor(neg_jw),\n    neg_ms = as.factor(neg_ms),\n    wave  = as.factor(wave),\n    nzeuro = as.factor(nzeuro),\n    eduyears = as.numeric(eduyears),\n    male = as.factor(male),\n    age = as.numeric(age),\n    rightwing = as.numeric(rightwing),\n    rural = as.factor(rural),\n    religiosity = as.numeric(religiosity)\n  )\n\n\n\nSolution\n\n\nlibrary(ggplot2)\nggplot(data = ip, aes(y = as.numeric(thr_ms), x = religiosity, colour = wave)) + \n  geom_smooth(method = lm, fullrange = FALSE, alpha = 0.1) + \n  labs(title = \"Religiosity predict Muslim acceptance post-Christchurch shootings\") + \n  xlab(\"Level of Religiosity (scale 1-7) \") + ylab (\"acceptance of Muslims (1-4)\")\n\n\n\n\nQuestion 10. Note that I have the following graph should start from 1 and run to 4 but currently runs from 0-4. Fix the graph\n\n\nlibrary(ggplot2)\nggplot(data = ip, aes(y = as.numeric(thr_ms), x = religiosity, colour = wave))  +  geom_jitter(alpha = .1) + \n  geom_smooth(method = lm, fullrange = FALSE, alpha = 0.1) +\n   scale_y_continuous(limits = c(0,4))\n\n\n\n\nSoution\n\n\nlibrary(ggplot2)\nggplot(data = ip, aes(y = as.numeric(thr_ms), x = religiosity, colour = wave))  +  geom_jitter(alpha = .1) + \n  geom_smooth(method = lm, fullrange = FALSE, alpha = 0.1) +\n   scale_y_continuous(limits = c(1,4))\n\n\n\n\nExtra question 11. Find one way of improving the the following code and explain your Solution\n\n\nlibrary(sjPlot)\nplot_xtab(\n    ip$thr_ms,\n    ip$wave,\n    show.total = F,\n    show.n = F,\n    geom.colors = c(\"lightgreen\", \"darkred\")\n  ) +\n  xlab(\"Threatened by Muslims\") +  ylab(\"Frequency\") +\n  #scale_y_continuous(limits=c(0,7)) + #theme(plot.title = element_text(size=09))\n  theme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\nSolution.\nE.g. add a title\n\n\nlibrary(sjPlot)\nplot_xtab(\n    ip$thr_ms,\n    ip$wave,\n    show.total = F,\n    show.n = F,\n    geom.colors = c(\"lightgreen\", \"darkred\")\n  ) +\n  xlab(\"Threatened by Muslims\") +  ylab(\"Frequency\") +\n  #scale_y_continuous(limits=c(0,7)) + #theme(plot.title = element_text(size=09))\n  theme(axis.text.x = element_text(angle = 20, hjust = 1)) + \n  labs(title = \"Comparison of sample responses to Muslim threat in 2018 an 2019\")\n\n\n\n\n\nData were collected with Barry Milne and Martin Van dataset; the data are only authorised for the purposes of teaching.↩︎\n",
    "preview": "workbooks/W_3_s/lab_3_solutions_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-04-23T15:50:24+12:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "workbooks/W_4_s/",
    "title": "Week 4 solutions",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\n\nContents\nLibraries\nImport data\nQ1a. Warmup\nSolution\nQ1b. Change class for all instances of a class\nSolution\nQ2a. Scale, center, transform\nSolution\nQ2b Data wrangle\nSolution\nQ3. Working with dates\nQ4 Caculating dates and creating summaries\nSolution\nQ5. Working with date intervals\nSolution\nQ6 Create an ordered factor from numeric data\nSolution\nSolution\nQ7 Make a summary table\nSolution\nQ7. Make a summary graph\nSolution\nQ8. Correlation graph\nSolution\nQ9 Create a blank papaja report\nQ10 Patchwork\n\nLibraries\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(pmdplyr)\nlibrary(correlation)\nlibrary(ggraph)\nlibrary(patchwork)\n\n\n\n\n\n# you might need to run this code\n#easystats::install_easystats_latest()\n\n\n\nImport data\n\n\n# read data\nnz_0 <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"))\n\n# to relevel kessler 6 variables\nf<-c(\"None Of The Time\",\"A Little Of The Time\",\"Some Of The Time\",  \"Most Of The Time\", \"All Of The Time\")\n\n# get data into shape\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) \n\n\n\nQ1a. Warmup\nMake a new dataframe from the nz dataframe. Call it df\nSolution\n\n\ndf <- nz\n\n\n\nQ1b. Change class for all instances of a class\nMake all the hours variables into integers\nSolution\nThis would be the quick way:\n\n\ndf <-df %>%\n  dplyr::mutate(across(starts_with(\"Hours\"), as.integer))\n\n\n\nBut more likely they’ll do it by hand, which is fine:\n\n\ndf<- df%>%\n  mutate(Hours.Exercise = as.integer(Hours.Exercise),\n         Hours.Internet = as.integer(Hours.Internet))\n\n\n\nAnd so on…\nQ2a. Scale, center, transform\nCreate a new indicator that standardises the Pol.Orient variable, create a new indicator that centers the Pol.Orient variable, create a new indicator that centres the Age variable in decade-long units. Do this in a single piped workflow.\nPrint the head of the data frame so that we can see your work \nSolution\n\n\ndf1<-df %>%\n  dplyr::select(Pol.Orient,\n         Age) %>%\n  dplyr::mutate(\n    conservative_s = scale(Pol.Orient),\n    conservative_c = scale(Pol.Orient, center = TRUE, scale = FALSE),\n    age_decade_c = scale(Age, center = TRUE, scale = FALSE) / 10\n  )%>%\n  glimpse()\n\n\n\nQ2b Data wrangle\nSelect Hour.Exercise and filter Wave 2019.\nSolution\nNote that this won’t. We need to filter first because otherwise there would be no Wave column to select!\n\n\ndf$Wave\ndf%>%\n  dplyr::select(\"Hours.Exercise\")%>%\n  dplyr::filter(Wave == 2019)%>%\n  head()\n\n\n\nQ3. Working with dates\nWhat are the maximum number of responses for a single day in 2018 and the maximum number of responses for a single day in 2019?\nSolution\nThis is how I’d do it:\n\n\nnz %>%\n  group_by(date, Wave)%>%\n  count() %>%\n  group_by(Wave)%>%\n  dplyr::filter(n == max(n))\n\n\n# A tibble: 2 x 3\n# Groups:   Wave [2]\n  date       Wave      n\n  <date>     <fct> <int>\n1 2018-06-21 2018    112\n2 2019-12-03 2019     54\n\nHere’s a longer approach:\n\n\n# 2018 take the first\nnz %>%\n  group_by(date, Wave) %>%\n  count() %>%\n  filter(Wave == 2018) %>%\n  arrange(desc(n)) %>%\n  glimpse()\n\n\nRows: 312\nColumns: 3\nGroups: date, Wave [312]\n$ date <date> 2018-06-21, 2018-06-22, 2018-06-24, 2018-06-…\n$ Wave <fct> 2018, 2018, 2018, 2018, 2018, 2018, 2018, 201…\n$ n    <int> 112, 93, 80, 67, 59, 58, 52, 44, 44, 34, 27, …\n\n\n# 2019  take the first\nnz %>%\n  group_by(date, Wave) %>%\n  count() %>%\n  filter(Wave == 2019) %>%\n  arrange(desc(n))%>%\n  glimpse()\n\n\nRows: 298\nColumns: 3\nGroups: date, Wave [298]\n$ date <date> 2019-12-03, 2019-10-04, 2019-12-02, 2019-12-…\n$ Wave <fct> 2019, 2019, 2019, 2019, 2019, 2019, 2019, 201…\n$ n    <int> 54, 47, 46, 46, 45, 44, 43, 39, 39, 38, 38, 3…\n\nQ4 Caculating dates and creating summaries\nHow many days are there between the date with the highest number of responses and the date with the second highest number of responses?\nBonus: Calculate difference between the number of responses on the highest response date and second highest response date.\nSolution\nHere is an approach:\n\n\nts<-nz %>%\n  group_by(date)%>%\n  count() %>%\n  arrange(desc(n)) %>%\n  as_tibble() %>%\n  slice(c(1:2)) %>%\n  mutate(diff_days = date - lag(date),\n         diff_n = n - lag(n))\nts\n\n\n# A tibble: 2 x 4\n  date           n diff_days diff_n\n  <date>     <int> <drtn>     <int>\n1 2018-06-21   112 NA days       NA\n2 2018-06-22    93  1 days      -19\n\nQ5. Working with date intervals\nSuppose you were born on Dec 25, 1995 at 5.02:22 am Calculate your age in months on March 20,2021, at 1:22:04pm. (Hint use the lubridate package. Look up the interval function).\nSolution\n\n\nlibrary(lubridate)\nalive <- lubridate::interval(ymd_hms(\"1995-12-25 05:02:22\"),\n                             ymd_hms(\"2021-03-21 13:22:04\"))\n\ntime_length(alive, \"months\")\n\n\n[1] 302.8695\n\nQ6 Create an ordered factor from numeric data\nThe Religion.Church variable contains responses to the question: “How many times each month do you attend church or religious service?”\nCreate factor with the following three levels:\nPeople who attend church 0 times per month,\nPeople who attend church 1-3 times per month,\nPeople who attend church 4 or more times per month.\nMake sure to re-level the factor so that the ordinal ranking moves from lowest to highest.\nSolution\nSolution\nHere is one approach:\n\n\nnz %>%\n  dplyr::select(Religion.Church) %>%\n  dplyr::mutate(church_attendance_cats = as.factor(ifelse(\n    Religion.Church == 0,\n    \"zero\",\n    ifelse(Religion.Church < 4, \"less_4\", \"gr_4\")\n  ))) %>%\n  group_by(church_attendance_cats) %>%\n  count()\n\n\n# A tibble: 4 x 2\n# Groups:   church_attendance_cats [4]\n  church_attendance_cats     n\n  <fct>                  <int>\n1 gr_4                     363\n2 less_4                   288\n3 zero                    3366\n4 <NA>                     109\n\nQ7 Make a summary table\nUsing methods described in lecture 4, create a table for average hours of sleep by month in the nz dataset\nSolution\n\n\nlibrary(kableExtra)\ntabnz <- nz %>%\n  select(Id, date, HLTH.SleepHours) %>%\n  mutate(month = month(date, label = TRUE)) %>%\n  group_by(month) %>%\n  summarise(\n    average_sleep =  mean(HLTH.SleepHours, na.rm = TRUE),\n    sd_sleep  =  sd(HLTH.SleepHours, na.rm = TRUE),\n    n  = n()\n  )\ntabnz %>%\n  kbl(caption = \"Distress by month\") %>%\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\n\n\n\nTable 1: Distress by month\n\n\nmonth\n\n\naverage_sleep\n\n\nsd_sleep\n\n\nn\n\n\nJan\n\n\n6.810909\n\n\n1.1739898\n\n\n265\n\n\nFeb\n\n\n6.666667\n\n\n0.9537048\n\n\n174\n\n\nMar\n\n\n6.792891\n\n\n1.1216895\n\n\n220\n\n\nApr\n\n\n7.279487\n\n\n1.1232327\n\n\n81\n\n\nMay\n\n\n7.064935\n\n\n1.0922895\n\n\n81\n\n\nJun\n\n\n6.927352\n\n\n1.0601600\n\n\n725\n\n\nJul\n\n\n6.957453\n\n\n1.1072359\n\n\n507\n\n\nAug\n\n\n6.976087\n\n\n1.0035276\n\n\n240\n\n\nSep\n\n\n6.666667\n\n\n1.0817994\n\n\n100\n\n\nOct\n\n\n6.912844\n\n\n1.0240630\n\n\n772\n\n\nNov\n\n\n6.940741\n\n\n1.2160484\n\n\n307\n\n\nDec\n\n\n6.872973\n\n\n1.0420464\n\n\n654\n\n\nQ7. Make a summary graph\nGraph the average hours of sleep by month including 95% confidence intervals\nBriefly explain why some intervals are wider than others.\nSolution\n\n\nnz %>%\n  select(Id, date, HLTH.SleepHours) %>%\n  mutate(month = month(date, label = TRUE)) %>%\n  group_by(month) %>%\n  summarise(\n    mn_sh =  mean(HLTH.SleepHours, na.rm = TRUE),\n    sd_sh  =  sd(HLTH.SleepHours, na.rm = TRUE),\n    n_sh  = n()\n  ) %>%\n  mutate(\n    se_sh  = sd_sh  / sqrt(n_sh),\n    lw_ci = mn_sh  - qt(1 - (0.05 / 2), n_sh  - 1) * se_sh ,\n    up_ci = mn_sh  + qt(1 - (0.05 / 2), n_sh  - 1) * se_sh\n  ) %>%\n  ggplot(., aes(x = month, y = mn_sh , colour = mn_sh)) +\n  geom_errorbar(aes(ymin = lw_ci, ymax = up_ci), width = .1) +\n  geom_point(size = 3)  +\n  scale_y_continuous(limits = c(0, 8)) +\n  theme_classic() + scale_fill_viridis_d()\n\n\n\n\nSome intervals are wider than others because the number of responses by month varies, and a confidence interval divides by the number of cases.\n\n\n\nHere we see high standard errors of the mean with low n’s\n\n\n\nWe can graph the year-wise differences, focusing on April:\n\n\n\nQ8. Correlation graph\nCreated a correlation graph for the items in the Kessler 6 scale\nThese are FeelHopeless,FeelDepressed,FeelRestless,EverythingIsEffort,FeelWorthless,FeelNervous\nHint you must transform the factors into integers.\nWhat do you find most interesting about this plot? Explain.\nSolution\n\n\nk6 <- nz %>%\n  select(\n    FeelHopeless,\n    FeelDepressed,\n    FeelRestless,\n    EverythingIsEffort,\n    FeelWorthless,\n    FeelNervous,\n    Id\n  ) %>%\n  mutate_all(., as.numeric) %>%\n  mutate(Id = as.factor(Id))# make numeric for correlation plot\n\nk6 %>%\n  correlation::correlation(partial = FALSE, multilevel = TRUE) %>%\n  plot() +\n  theme_gray()\n\n\n\n\nI find it interesting that Nervousness and Restlessness are not correlated, or inversely correlated with Depression.\nSetting the correlations to partial does not destroy this effect.\n\n\nk6 %>%\n  correlation::correlation(partial = TRUE, multilevel = TRUE) %>%\n  plot() +\n  theme_gray()\n\n\n\n\nQ9 Create a blank papaja report\nInclude your your name, affiliation, contributors and r packages used in your analysis\nQ10 Patchwork\nUse the patchwork library to create a figure with two plots on top of each other.\n\n\nlibrary(patchwork)\nlibrary(ggplot2)\np1 <-\n  qplot(mtcars$cyl, geom = \"histogram\") + labs(title = \"this plot\") + xlab(\"mt cycle\")\np2 <-\n  qplot(mtcars$disp, geom = \"histogram\") + labs(title = \"that plot\")\n\np1 / p2 + plot_annotation(title = \"my title\", tag_levels = 'a') + xlab(\"mt cycle\") +  plot_layout(guides = 'collect')\n\n\n\n\nFigure 1: \n\n\n\n\n\n\n",
    "preview": "workbooks/W_4_s/lab_4_solutions_files/figure-html5/unnamed-chunk-15-1.png",
    "last_modified": "2021-04-23T15:50:24+12:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1920
  },
  {
    "path": "workbooks/W_5_s/",
    "title": "Week 5 workbook solutions",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\n\nContents\nIn 1903, Pearson and Lee collected 5,524 observations from mother/daughter height pairs. See lecture 5 for details\nNotes\nQ1. Create a descriptive table and a descriptive graph for the HLTH.Weight and HLTH.Height variables in the nz dataset\nSolution\n\nQ2. Write up a sample summary of the HLTH.Weight and HLTH.Height variables in the nz dataset in APA style.\nQ3. Regress height ~ weight and report results\nQ4. Regress height ~ male_id and report results\nQ5. Regression to predict\nSolutoin\n\nQ6. Bonus, not marked\nSolution\n\n\n\n\n\n### Libraries\nlibrary(\"tidyverse\")\nlibrary(\"patchwork\")\nlibrary(\"lubridate\")\nlibrary(\"kableExtra\")\nlibrary(\"gtsummary\")\n\n\n\n\n\n# read data\nnz_0 <- readr::read_csv2(\n  url(\n    \"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"\n  )\n)\n\n# to relevel kessler 6 variables\nf <-\n  c(\n    \"None Of The Time\",\n    \"A Little Of The Time\",\n    \"Some Of The Time\",\n    \"Most Of The Time\",\n    \"All Of The Time\"\n  )\n\n# get data into shape\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(male_id = as.factor(Male)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE)\n\n\n\n\n\n### Import `Pearson and Lee` mother's and daughters data\nmd_df <- data.frame(read.table(\n  url(\n    \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt\"\n  ),\n  header = TRUE\n))\n# Center mother's height for later example\nmd_df <- md_df %>%\n  dplyr::mutate(mother_height_c = as.numeric(scale(\n    mother_height, center = TRUE, scale = FALSE\n  )))\ndplyr::glimpse(md_df)\n\n\nRows: 5,524\nColumns: 3\n$ daughter_height <dbl> 52.5, 52.5, 53.5, 53.5, 55.5, 55.5…\n$ mother_height   <dbl> 59.5, 59.5, 59.5, 59.5, 59.5, 59.5…\n$ mother_height_c <dbl> -2.9987328, -2.9987328, -2.9987328…\n\nIn 1903, Pearson and Lee collected 5,524 observations from mother/daughter height pairs. See lecture 5 for details\nNotes\nFor all exercises below, use only the 2019 wave of the nz dataset.\nQ1. Create a descriptive table and a descriptive graph for the HLTH.Weight and HLTH.Height variables in the nz dataset\nSelect HLTH.Weight, HLTH.Height from the nz dataset.\nFilter only the 2019 wave.\nCreate a descriptive table and graph these two variables\nAnnotate your workflow (at each step, describe what you are doing and why).\nSolution\n\n\nlibrary(gtable)\n# select focal variables and rename them for clarity\nnzdat <- nz %>%\n  dplyr::filter(Wave == 2019) %>%\n  dplyr::select(HLTH.Weight, HLTH.Height, male_id) %>%\n  dplyr::rename(weight = HLTH.Weight,\n                height = HLTH.Height) %>%\n  dplyr::select(weight, height, male_id) %>%\n  dplyr::mutate(weight_c = as.numeric(scale(\n    weight, scale = F, center = TRUE\n  )))\n\n# create table\nnzdat %>%\n  dplyr::select(weight,\n                height) %>%\n  gtsummary::tbl_summary(\n    #by = Wave,\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} / {N} ({p}%)\"\n    ),\n    digits = all_continuous() ~ 2,\n    missing_text = \"(Missing)\"\n  ) %>%\n  bold_labels() \n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#qbrjosukyb .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#qbrjosukyb .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#qbrjosukyb .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#qbrjosukyb .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#qbrjosukyb .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#qbrjosukyb .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#qbrjosukyb .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#qbrjosukyb .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#qbrjosukyb .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#qbrjosukyb .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#qbrjosukyb .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#qbrjosukyb .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#qbrjosukyb .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#qbrjosukyb .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#qbrjosukyb .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#qbrjosukyb .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#qbrjosukyb .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#qbrjosukyb .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#qbrjosukyb .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#qbrjosukyb .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#qbrjosukyb .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#qbrjosukyb .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#qbrjosukyb .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#qbrjosukyb .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#qbrjosukyb .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#qbrjosukyb .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#qbrjosukyb .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#qbrjosukyb .gt_left {\n  text-align: left;\n}\n\n#qbrjosukyb .gt_center {\n  text-align: center;\n}\n\n#qbrjosukyb .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#qbrjosukyb .gt_font_normal {\n  font-weight: normal;\n}\n\n#qbrjosukyb .gt_font_bold {\n  font-weight: bold;\n}\n\n#qbrjosukyb .gt_font_italic {\n  font-style: italic;\n}\n\n#qbrjosukyb .gt_super {\n  font-size: 65%;\n}\n\n#qbrjosukyb .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nCharacteristic\n      N = 2,0631\n    weight\n      79.68 (18.79)\n    (Missing)\n      22\n    height\n      1.70 (0.10)\n    (Missing)\n      23\n    \n        \n          1\n          \n           \n          Mean (SD)\n          \n      \n    \n\nHere’s another approach:\n\n\nlibrary(table1)\n\n# filter 2019 wave\nnz1 <- nz %>%\n  dplyr::filter(Wave == 2019)\n\n# nicer labels\ntable1::label(nz1$HLTH.Weight)     <- \"Weight\"\ntable1::label(nz1$HLTH.Height)     <- \"Height\"\n\n# table\ntable1::table1(~ HLTH.Weight + HLTH.Height, data = nz1)     \n\n\n\nOverall(N=2063)\nWeight\n\nMean (SD)\n79.7 (18.8)\nMedian [Min, Max]\n78.0 [41.0, 200]\nMissing\n22 (1.1%)\nHeight\n\nMean (SD)\n1.70 (0.0984)\nMedian [Min, Max]\n1.69 [1.20, 2.06]\nMissing\n23 (1.1%)\n\n\nCreate graph\n\n\nnzdat1 <- nzdat%>%\n    dplyr::filter(!is.na(weight),\n                  !is.na(height),\n                  !is.na(male_id)) # filter na's for density plots\n  \nweight_density <-ggplot2::ggplot(data = nzdat1, aes(x = weight)) + geom_density(fill = \"chocolate2\") + \n  labs(title = \"Density plot of weight of NZ sample years 2019/2020\") + theme_classic()\n\nheight_density <-ggplot2::ggplot(data = nzdat1, aes(x = height)) + geom_density(fill =\"blue2\") + \n  labs(title = \"Density plot of height of NZ sample years 2019/2020\") + theme_classic()\n\nweight_density / height_density + plot_annotation(tag_levels = 'a')\n\n\n\n\nQ2. Write up a sample summary of the HLTH.Weight and HLTH.Height variables in the nz dataset in APA style.\nUsing the analysis in Q1, describe Height and Weight in the nz dataset\nWrite brief APA methods summary for these two variables.\nNote: *if useful, use the ‘male_id’ variable to clarify interesting or puzzling features of the HLTH.Weight and HLTH.Height responses.\nNZAVS data dictionary\nSolution\nWhy aren’t the distributions of height and weight normal? Likely there is a hidden co-variate.\nLet’s assess whether there might be differences in these parameters that might owe to sex.\n\n\n# plot of weight ~ is_male\nweight_density2 <-\n  ggplot2::ggplot(data = nzdat1, aes(x = weight, fill = male_id)) +\n  geom_density() +\n  labs(title = \"Density plot of weight of NZ sample years 2019/2020\") +\n  theme_classic() +\n  facet_grid(. ~ male_id)\nweight_density2 <-\n  weight_density2 + \n  aes(fill = male_id) + \n  scale_fill_viridis_d() # nicer colour\n\n# plot of height ~ is_male\nheight_density2 <-\n  height_density2 <-\n  ggplot2::ggplot(data = nzdat1, aes(x = height, fill = male_id)) +\n  geom_density() +\n  labs(title = \"Density plot of height of NZ sample years 2019/2020\") +\n  theme_classic() +\n  facet_grid(. ~ male_id)  + \n  aes(fill = male_id) +  \n  scale_fill_viridis_d() # nicer colour\n\nheight_density2 <- height_density2 + aes(fill = male_id)\nweight_density2 / height_density2 + plot_annotation(tag_levels = 'a')\n\n\n\n\nQ3. Regress height ~ weight and report results\nUsing the nz dataset, write a regression model for height as predicted by weight.\nCreate a table for your results.\nCreate a graphs/graphs to clarify the results of your regression model.\nBriefly report your results.\nSolution\nModel:\n\n\n# regression of height ~ weight\nmod1 <- lm(height ~ weight_c, data = nzdat)\n\n\n\nTable:\n\n\nsjPlot::tab_model(mod1)\n\n\n\n \n\n\nheight\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n1.70\n\n\n1.69 – 1.70\n\n\n<0.001\n\n\nweight_c\n\n\n0.00\n\n\n0.00 – 0.00\n\n\n<0.001\n\n\nObservations\n\n\n2031\n\n\nR2 / R2 adjusted\n\n\n0.169 / 0.169\n\n\nPrediction\n\n\nsjPlot::tab_model(mod1)\n\n\n\n \n\n\nheight\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n1.70\n\n\n1.69 – 1.70\n\n\n<0.001\n\n\nweight_c\n\n\n0.00\n\n\n0.00 – 0.00\n\n\n<0.001\n\n\nObservations\n\n\n2031\n\n\nR2 / R2 adjusted\n\n\n0.169 / 0.169\n\n\nplot(ggeffects::ggpredict(mod1,terms = \"weight_c\"))\n\n\n\n\nBriefly report your results. (note: please replace “significant” with \"statistically significant.)\n\n\nreport::report(mod1)\n\n\nWe fitted a linear model (estimated using OLS) to predict height with weight_c (formula: height ~ weight_c). The model explains a statistically significant and moderate proportion of variance (R2 = 0.17, F(1, 2029) = 412.87, p < .001, adj. R2 = 0.17). The model's intercept, corresponding to weight_c = 0, is at 1.70 (95% CI [1.69, 1.70], t(2029) = 852.41, p < .001). Within this model:\n\n  - The effect of weight_c is statistically significant and positive (beta = 2.17e-03, 95% CI [1.96e-03, 2.37e-03], t(2029) = 20.32, p < .001; Std. beta = 0.41, 95% CI [0.37, 0.45])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\nQ4. Regress height ~ male_id and report results\nUsing the nz dataset, write a regression model for height as predicted by male_id\nCreate a table for your results.\nCreate a graph/graphs to clarify the results of your regression model.\nBriefly report your results.\nSolution\nModel and table:\n\n\n# regression of height ~ weight\nmod2 <- lm(height ~ male_id, data = nzdat)\nsjPlot::tab_model(mod2)\n\n\n\n \n\n\nheight\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n1.78\n\n\n1.77 – 1.78\n\n\n<0.001\n\n\nmale_id [Not_Male]\n\n\n-0.13\n\n\n-0.13 – -0.12\n\n\n<0.001\n\n\nObservations\n\n\n2034\n\n\nR2 / R2 adjusted\n\n\n0.384 / 0.383\n\n\nGraph:\n\n\n# plot over the range of the data\npl1<- plot(\n  ggeffects::ggpredict(mod2, terms = \"male_id [all]\"),\n  add.data = TRUE,\n  dot.alpha = .2\n) \npl1 + \n  scale_y_continuous(limits = c(1.2, 2.1)) +  # range of data\n  geom_point(colour = c(\"brown2\", \"cadetblue2\")) \n\n\n\n\nReport\n\n\nreport::report(mod1)\n\n\nWe fitted a linear model (estimated using OLS) to predict height with weight_c (formula: height ~ weight_c). The model explains a statistically significant and moderate proportion of variance (R2 = 0.17, F(1, 2029) = 412.87, p < .001, adj. R2 = 0.17). The model's intercept, corresponding to weight_c = 0, is at 1.70 (95% CI [1.69, 1.70], t(2029) = 852.41, p < .001). Within this model:\n\n  - The effect of weight_c is statistically significant and positive (beta = 2.17e-03, 95% CI [1.96e-03, 2.37e-03], t(2029) = 20.32, p < .001; Std. beta = 0.41, 95% CI [0.37, 0.45])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\nQ5. Regression to predict\nUsing the regression coefficients from the Pearson and Lee 1903 dataset, predict the heights of daughters of women in the nz dataset.\nSolutoin\n\n\n# model for daughter height from mother height\nmpf<- lm(daughter_height ~ mother_height, data = md_df)\n\n# create data frame of not_male's in 2019\n# Notice problem. not_male != woman\n# additionally, woman != mother! \n\nnz2 <-nzdat %>%\n  filter(male_id == \"Not_Male\")%>% # not_males\n  dplyr::select(height) %>% # variable of interest\n  dplyr::mutate(mother_height = height * 39.36)%>% # Convert meters to inches\n  dplyr::select(mother_height)%>%\n  dplyr::arrange((mother_height)) \n\n# find min and max heights, store as objects \nmnh<-min(nz2$mother_height, na.rm = TRUE)\nmxh<-max(nz2$mother_height, na.rm = TRUE)\n\n# expand grid, use stored objects to define boundaries.\nndat2<-expand.grid(mother_height = seq(from = mnh, to = mxh, length.out=200))\n\n# use the `predict` function to create a new response using the pearson and fox regression model\n\npr2<- predict(mpf, type = \"response\", interval = \"confidence\", newdata =ndat2)\n\n# create a new dataframe for the response variables, following the method in lecture 5\n\n# combine variables into a data frame\nnewdata2 <- data.frame(ndat2,pr2)\n\n# graph the expected average hypothetical heights of \"daughters\"  \npredplot2 <-\n  ggplot(data = newdata2, aes(x = mother_height, y = fit))  +\n  geom_line(colour = \"cadetblue\")  +  geom_errorbar(aes(ymin = lwr, ymax = upr), width = .1) + scale_x_continuous(limits = c(50, 75)) + scale_y_continuous(limits = c(50, 75)) + theme_classic()  +\n  xlab(\"NZ 2019 female population\") +\n  ylab(\"predicted daughter heights in inches\") +\n  labs(title = \"Regression prediction for hypothetical daughter heights of NZ population in 2019 \")\n\n# plot\npredplot2\n\n\n\n\nQ6. Bonus, not marked\nOn average, how much taller or shorter are women in New Zealand as sampled in 2019 nz dataset compared with women in 1903 as sampled in the Pearson and Lee dataset.\nSolution\n\n\n#create var for 1903 dataset\nmd_df2 <- md_df %>%\n  dplyr::select(mother_height, daughter_height) %>%\n  tidyr::pivot_longer(everything(),\n                      names_to = c(\"height\")) %>%\n  dplyr::select(value) %>%\n  dplyr::rename(f_height = value) %>%\n  dplyr::mutate(is_2019 = factor(rep(\"1903\", nrow(.)))) # create identifier\n\n# create var for 2019 dataset\nnz_df2 <-\n  nz2 %>%  # dataset with only women and height transformed to inches\n  dplyr::rename(f_height = mother_height) %>%\n  dplyr::mutate(is_2019 = factor(rep(\"2019\", nrow(.))))\n\n# combine data frames row-wise\nrdf <- rbind(md_df2, nz_df2)\n\n# look at data structure\ndplyr::glimpse(rdf)\n\n\nRows: 12,353\nColumns: 2\n$ f_height <dbl> 59.5, 52.5, 59.5, 52.5, 59.5, 53.5, 59.5,…\n$ is_2019  <fct> 1903, 1903, 1903, 1903, 1903, 1903, 1903,…\n\nLook at heights in sample\n\n\nggplot2::ggplot(data = rdf, aes(x= is_2019, y= f_height, fill = is_2019))+ geom_boxplot(notch = TRUE) + \n  labs(title = \"Differences in female height 1903/2019\") + theme_classic() + scale_fill_viridis_d()\n\n\n\n\nPredict heights out of sample\n\n\nmod3<-lm(f_height ~   is_2019, data = rdf)\nsjPlot::tab_model(mod3)\n\n\n\n \n\n\nf_height\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n63.18\n\n\n63.13 – 63.23\n\n\n<0.001\n\n\nis_2019 [2019]\n\n\n1.82\n\n\n1.66 – 1.97\n\n\n<0.001\n\n\nObservations\n\n\n12336\n\n\nR2 / R2 adjusted\n\n\n0.042 / 0.042\n\n\nWomen in 2019 are taller\n\n\nreport::report(mod3)\n\n\nWe fitted a linear model (estimated using OLS) to predict f_height with is_2019 (formula: f_height ~ is_2019). The model explains a statistically significant and weak proportion of variance (R2 = 0.04, F(1, 12334) = 546.73, p < .001, adj. R2 = 0.04). The model's intercept, corresponding to is_2019 = 1903, is at 63.18 (95% CI [63.13, 63.23], t(12334) = 2516.62, p < .001). Within this model:\n\n  - The effect of is_2019 [2019] is statistically significant and positive (beta = 1.82, 95% CI [1.66, 1.97], t(12334) = 23.38, p < .001; Std. beta = 0.67, 95% CI [0.62, 0.73])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset.\n\nGraph:\n\n\nplot(\n  ggeffects::ggpredict(mod3, terms = \"is_2019\"),\n  add.data = F,\n  dot.alpha = .2\n) + labs(title = \"Predicted difference in female heights 1903/2019\") + \n  xlab(\"Study year\") + \n  ylab (\"Predicted femal height (inches)\")\n\n\n\n\nGraph with predicted points:\n\n\nplot(\n  ggeffects::ggpredict(mod3, terms = \"is_2019\"),\n  add.data = F,\n  dot.alpha = .2\n) + labs(title = \"Predicted difference in female heights 1903/2019\") + \n  xlab(\"Study year\") + \n  ylab (\"Predicted femal height (inches)\")\n\n\n\n\n\n\n\n",
    "preview": "workbooks/W_5_s/workbook_5_solutions_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-04-23T15:50:24+12:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1920
  },
  {
    "path": "workbooks/W_6_s/",
    "title": "Week 6 workbook and solutions",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\n\nContents\nWarm-up\nAssignment\n\nWarm-up\nCopy the R markdown file from here\nCopy the text and drop it into a blank R markdown file.\nRender the R markdown file on your computer.\nDo the exercises.\nThe warm-up exercises will not be assessed, however they will teach you skills that you need for simulating data. So do them 😺\nAssignment\nYour final exercises is a 2500 word report for a substantive research question in psychological science.\nThis week,\nWrite a two-three paragraph introduction to your report. The introduction should do the following\nState your research question;\nClarify, in plain terms and without jargon, why your research question is important;\nClarify your research hypothesis or hypotheses;\nIdentify the data you will use to address your research question.1\nQuantitatively summarise your sample information and indicator variables using methods that you have learned in this class. Your summary should do the following:\nInclude at least one summary table;\nInclude at least one summary graph;\nReport your data features.\nYou will be marked on how effectively your summary introduces your research question and clarifies properties of your sample.\nTo keep the summary streamlined, you may include an appendix.\nSummaries should contain a bibliography where relevant.\nPlease submit your R markdown file as well as a rendered pdf or word document to Blackboard by midnight April 20th.\nPlease annotate your R markdown file to clarify your approach and any decisions.\nPlease keep a record of your summary on Github (i.e. push to Github).\n\nNote: you might need to simulate data because you have yet to collect data.↩︎\n",
    "preview": {},
    "last_modified": "2021-04-23T15:50:24+12:00",
    "input_file": {}
  },
  {
    "path": "workbooks/W_5/",
    "title": "Week 5 workbook",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-04-08",
    "categories": [],
    "contents": "\n\nContents\nSetup\nLibraries\nImport nz data\nImport Pearson and Lee mother’s and daughters data\nNote\n\nQ1. Create a descriptive table and a descriptive graph for the HLTH.Weight and HLTH.Height variables in the nz dataset\nQ2. Write up a sample summary of the HLTH.Weight and HLTH.Height variables in the nz dataset in APA style.\nQ3. Regression height ~ weight and report results\nQ4. Regress height ~ male_id and report results\nQ5. Regression to predict\nQ6. Bonus, not marked\n\nSetup\nLibraries\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(lubridate)\nlibrary(kableExtra)\nlibrary(gtsummary)\n\n\n\nImport nz data\n\n\n# read data\nnz_0 <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"))\n\n# to relevel kessler 6 variables\nf<-c(\"None Of The Time\",\"A Little Of The Time\",\"Some Of The Time\",  \"Most Of The Time\", \"All Of The Time\")\n\n# get data into shape\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(male_id = as.factor(Male)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE)\n\n\n\nImport Pearson and Lee mother’s and daughters data\n\n\nmd_df <- data.frame(read.table(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt\"), header=TRUE))\n# Center mother's height for later example\nmd_df <- md_df %>%\n  dplyr::mutate(mother_height_c = as.numeric(scale(mother_height, center = TRUE, scale = FALSE)))\ndplyr::glimpse(md_df)\n\n\nRows: 5,524\nColumns: 3\n$ daughter_height <dbl> 52.5, 52.5, 53.5, 53.5, 55.5, 55.5, 55.5, 55…\n$ mother_height   <dbl> 59.5, 59.5, 59.5, 59.5, 59.5, 59.5, 59.5, 59…\n$ mother_height_c <dbl> -2.9987328, -2.9987328, -2.9987328, -2.99873…\n\n# In 1903, Pearson and Lee collected 5,524 observations from mother/daughter height pairs. See lecture 5 for details\n\n\n\nNote\nFor all exercises below, use only the 2019 wave of the nz dataset.\nQ1. Create a descriptive table and a descriptive graph for the HLTH.Weight and HLTH.Height variables in the nz dataset\nSelect HLTH.Weight, HLTH.Height from the nz dataset.\nFilter only the 2019 wave.\nCreate a descriptive table and graph these two variables\nAnnotate your workflow (at each step, describe what you are doing and why).\nQ2. Write up a sample summary of the HLTH.Weight and HLTH.Height variables in the nz dataset in APA style.\nUsing the analysis in Q1, describe Height and Weight in the nz dataset\nWrite brief APA methods summary for these two variables.\nNote: *if useful, use the ‘male_id’ variable to clarify interesting or puzzling features of the HLTH.Weight and HLTH.Height responses.\nNZAVS data dictionary\nQ3. Regression height ~ weight and report results\nUsing the nz dataset, write a regression model for height as predicted by weight.\nCreate a table for your results.\nCreate a graphs/graphs to clarify the results of your regression model.\nBriefly report your results.\nQ4. Regress height ~ male_id and report results\nUsing the nz dataset, write a regression model for height as predicted by male_id\nCreate a table for your results.\nCreate a graphs/graphs to clarify the results of your regression model.\nBriefly report your results.\nQ5. Regression to predict\nUsing the regression coefficients from the Pearson and Lee 1903 dataset\nPredict the heights of daughters of women in the nz dataset.\nQ6. Bonus, not marked\nOn average, how much taller or shorter are women in New Zealand as sampled in 2019 nz dataset compared with women in 1903 as sampled in the Pearson and Lee dataset.\nClarify your inference.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-08T20:44:48+12:00",
    "input_file": {}
  },
  {
    "path": "workbooks/W_4/",
    "title": "Workbook 4",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-01",
    "categories": [],
    "contents": "\n\nContents\nSetup\nLibraries\nImport data\nQ1. Warmup\nQ2a. Scale, center, transform\nQ2b Data wrangle\nQ3. Working with dates\nQ4 Caculating dates and creating summaries\nQ5. Working with date intervals\nQ6 Create an ordered factor from numeric data\nQ7 Make a summary table\nQ7. Make a summary graph\nQ8. Correlation graph\nQ9 Create a blank papaja report\nQ10 Patchwork\n\n\n\n\n\nSetup\nLibraries\n\n\nif (!require(skimr)) install.packages('skimr')\nif (!require(lubridate)) install.packages('lubridate')\nif (!require(tidyverse)) install.packages('tidyverse')\nif (!requireNamespace(\"devtools\")) {\n  install.packages(\"devtools\")\n}\nif (!require(easystats)) devtools::install_github(\"easystats/easystats\")\n\n\n# Attaching packages (red = needs update)\n✔ insight     0.13.1.1   ✔ bayestestR  0.8.3.1 \n✔ performance 0.7.0.1    ✔ parameters  0.12.0.1\n✔ see         0.6.2.1    ⚠ effectsize  0.4.3.1 \n✔ correlation 0.6.0.1    ✔ modelbased  0.5.9   \n✔ report      0.2.0      \nWarnings or errors in CRAN checks for package(s) 'bayestestR', 'parameters', 'effectsize', 'correlation'.\nRestart the R-Session and update packages in red with 'easystats::easystats_update()'.\n\nif (!require(ggthemes)) install.packages('ggthemes')\nif (!require(pmdplyr)) install.packages(\"pmdplyr\")\nif (!require(kableExtra)) install.packages(\"kableExtra\")\n# this should be part of easystats but in case not:\nif (!require(report)) install.packages('report')\nif (!require(brms)) install.packages('brms')\nif (!require(lme4)) install.packages('lme4')\nif (!require(table1)) install.packages('table1')\nif (!require(modelsummary)) install.packages(\"modelsummary\")\nif (!require(naniar)) install.packages(\"naniar\")\nif (!require(ggraph)) install.packages(\"ggraph\")\nif (!require(gtsummary)) install.packages(\"gtsummary\")\n\n\n\n\n\n# You might need to run this\n# easystats::install_easystats_latest()\n\n\n\nImport data\n\n\n# read data\nnz_0 <- \n  readr::read_csv2(\n    url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\")\n    )\n\n# to relevel kessler 6 variables\nf<-c(\"None Of The Time\",\n     \"A Little Of The Time\",\n     \"Some Of The Time\",\n     \"Most Of The Time\",\n     \"All Of The Time\")\n\n# get data into shape\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) \n\n\n\nQ1. Warmup\nUsing the nz dataset, make all the hours variables into integers\nQ2a. Scale, center, transform\nCreate a new indicator that standardises the Pol.Orient variable, create a new indicator that centers the Pol.Orient variable, create a new indicator that centres the Age variable in decade-long units. Do this in a single piped workflow.\nPrint the head of the data frame so that we can see your work\nQ2b Data wrangle\nSelect Hour.Exercise and filter Wave 2019.\nQ3. Working with dates\nWhat are the maximum number of responses for a single day in 2018 and the maximum number of responses for a single day in 2019?\nQ4 Caculating dates and creating summaries\nHow many days are there between the date with the highest number of responses and the date with the second highest number of responses?\nBonus: Calculate difference between the number of responses on the highest response date and second highest response date.\nQ5. Working with date intervals\nSuppose you were born on Dec 25, 1995 at 5.02:22 am Calculate your age in months on March 20,2021, at 1:22:04pm. (Hint use the lubridate package. Look up the interval function).\nQ6 Create an ordered factor from numeric data\nThe Religion.Church variable contains responses to the question: “How many times each month do you attend church or religious service?”\nCreate factor with the following three levels:\nPeople who attend church 0 times per month,\nPeople who attend church 1-3 times per month,\nPeople who attend church 4 or more times per month.\nMake sure to re-level the factor so that the ordinal ranking moves from lowest to highest.\nQ7 Make a summary table\nUsing methods described in Lecture 4, create a table for average hours of sleep by month in the nz dataset\nQ7. Make a summary graph\nGraph the average hours of sleep by month including 95% confidence intervals\nBriefly explain why some intervals are wider than others.\nQ8. Correlation graph\nCreated a correlation graph for the items in the Kessler 6 scale\nThese are:\n-FeelHopeless, -FeelDepressed, -FeelRestless, -EverythingIsEffort, -FeelWorthless, -FeelNervous\nHint you must transform the factors into integers.\nWhat do you find most interesting about this plot? Explain.\nQ9 Create a blank papaja report\nInclude your your name, affiliation, contributors and r packages used in your analysis\nQ10 Patchwork\nUse the patchwork library to create a figure with two plots on top of each other. Use the tag_levels function to index each of the two plots. The graphs should describe some dimension of the truncated nz dataset.\n\n\n\n",
    "preview": "workbooks/W_4/k6.jpeg",
    "last_modified": "2021-04-01T19:50:52+13:00",
    "input_file": {}
  },
  {
    "path": "workbooks/W_2_s/",
    "title": "Week 2 Workbook Solutions",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-17",
    "categories": [],
    "contents": "\n\nContents\nCreate Repos (if you have not already)\nSolution\nCreate R projects\nSolution\nWorkbook 2\nQ1 Open an R markdown document, and filling in your name\nQ2. The first code chunk is for your preferences.\nQ3. Find the keyboard shortcuts menu.\nQ4. Memorise the keyboard shortcut for insert code chunk which is: ⌥⌘I\nQ5. Use the mac to insert a code chunk.\nQ6. Memorise the shortcut for evaluate which is ⌘-return^ use for all tasks in this workbook.\nQ7. Use R to do 10 mathematical calculations\nSolution\nQ8. Using R, find the square root of 324\nSolution\n\nCreate Repos (if you have not already)\nCreate a repository for 447-journals (if you don’t have one already)\nCreate a repository for your 447-workbooks.\nYou should initialise a Readme statement and a .gitignore document (make sure to click the R .gitignore)\nSolution\nStudents should confirm that they’ve done this\nCreate R projects\nCreate R-projects in each of these repositories.\nIn your workbooks folder, using the + New Folder command, create a folder called data (or similar) This is where you’ll store your data.\nCreate a file called figs (where you’ll stor your figures) 5 Create a folder called workbooks, or similar this is where you’ll store your weekly workbooks.\nSolution\nIf students did not create a data folder, then the here package task won’t work.\nWorkbook 2\nQ1 Open an R markdown document, and filling in your name\nQ2. The first code chunk is for your preferences.\n\nknitr::opts_chunk$set(echo = TRUE)\n\nQ3. Find the keyboard shortcuts menu.\nQ4. Memorise the keyboard shortcut for insert code chunk which is: ⌥⌘I\nQ5. Use the mac to insert a code chunk.\nQ6. Memorise the shortcut for evaluate which is ⌘-return^ use for all tasks in this workbook.\nQ7. Use R to do 10 mathematical calculations\nSolution\nStudents should confirm that they’ve done this, e.g.:\n\n4 + 5\n[1] 9\n\n\n3 ^ 3\n[1] 27\n\nShow your work\nQ8. Using R, find the square root of 324\nSolution\nStudents should show their works*\nJB: Students should confirm that they’ve done this\nIn the iris dataset, find the flower with the longest Sepal.Length. (hint, use the max function.) Show the column details for this flower only.\n\n# answer:\nmax(iris$Sepal.Length)\n[1] 7.9\n\nIn the iris dataset, find the flower with the shortest Sepal.Length. (hint, use the min function.)\n\n# answer:\nmin(iris$Sepal.Length)\n[1] 4.3\n\nCalculate the difference bewteen the flower with the longest and shortest Sepal.Length\n\n# answers\nmax(iris$Sepal.Length) - min(iris$Sepal.Length)\n[1] 3.6\n# or\na <- max(iris$Sepal.Length)\nb <- min(iris$Sepal.Length)\n\nMake a 7 column x 100 row dataframe using c, :, dataframe\n\n# some quick fixes\ndf <- data.frame(\n  col1 = sample(1000, 100, replace = TRUE),\n  col2 = rnorm(n = 100, mean = 0, sd = 1),\n  col3 = 1:100,\n  col4 = rbinom(n = 100, size = 1, prob = .5),\n  col5 = rpois(n = 100, lambda = 10),\n  col6 = sample(c(\"a\", \"b\", \"c\", \"d\"), 100, replace = TRUE),\n  col7 =  rep(1:10, 10)\n)\n\nRename the columns of the dataframe using the names of snow white dwarfs, or any names.\n\n# using base R\nnames(df)[] <-\n  c(\n    \"sleepy\",\n    \"grumpy\",\n    \"happy\",\n    \"smelly\",\n    \"lovely\",\n    \"daggy\",\n    \"crazy\"\n    )\n\nUsing the dataset woman, write a linear model for height ~ weight\n\n# basic linear model\n\nm1 <-  lm ( height ~ weight, data = women)\n\nCreate a table and coefficient plot for this model\n\n# coefficient plot\nsjPlot::plot_model(m1)\n\n# table\nsjPlot::tab_model(m1)\n\n \n\n\nheight\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n25.72\n\n\n23.47 – 27.98\n\n\n<0.001\n\n\nweight\n\n\n0.29\n\n\n0.27 – 0.30\n\n\n<0.001\n\n\nObservations\n\n\n15\n\n\nR2 / R2 adjusted\n\n\n0.991 / 0.990\n\n\nUsing ggeffects, create a prediction plot for this model.\n\nplot(\n  ggeffects::ggpredict( m1 , effects = \"height\"),\n  add.data = TRUE\n)\n$weight\n\n\n\nNext week we’ll be creating plots like this\n\nlibrary(ggplot2)\nggplot2::ggplot(data = women) +\n  geom_point(mapping = aes(x = height, y = weight)) +\n  geom_smooth(method = loess, aes(x = height, y = weight)) +\n  theme_classic() \n\n\nExplain what is being calculated here\n\n# this is a proportion (i.e. the number of instances / the total number of cases)\nsum(women$weight > 140) / length(women$weight)\n[1] 0.4\n\nCalculate the proportion of women who are over the mean weight of women\n\n# This is onle approach\n# Find the mean woman weight\nmw <- mean( women$weight )\n\n# calculae the proportion of women who are over that weight \nsum(women$weight > mw) / length(women$weight)\n[1] 0.4666667\n\nWhat are the advantages and disadvantages of creating more breaks in the Petal.Length indicator? Clarify your answer in a paragraph.\n\n# advantates: major pattern of data evident; clearly two clusters (at least); no tempatation to over interpret patters in the graph\n\n# disadvantages, looks as if there is a cluster at zero; only two modes in the dataset are discernable\nhist( iris$Petal.Length )\n\n\n\n# advantages: we can find more than one mode;  distribution not clustered at zero\n# disadvantages: with such a small sample, it is tempting to read too much into all the modes. \n\nhist( iris$Petal.Length, \n      breaks = 100 )\n\n\nSpot the error in this code\n\n# here is one method. \nmh <- mean (women$height)\nsum(women$weight > mh) / length(women$height)\n\n\n# should be `sum(women$height > mh)\n\nReorder the columns of the woman dataset so that weight comes before height. Then rename the columns “w” and “h”.\n\n# here is one method\n# Bind columns as data frame\ndfa <- cbind.data.frame(women$weight, women$height)\n# change names\nnames(dfa)[] <- c(\"w\", \"h\")\n\nRead data into R using the following method:\n\n\n\n\n# read data from file\nlibrary(\"readr\")\ntestdata<- readr::read_csv(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/testdata1.csv\"))\nstr(testdata)\nspec_tbl_df [100 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ id    : num [1:100] 1 2 3 4 5 6 7 8 9 10 ...\n $ weight: num [1:100] 80.5 87.8 95.7 74.6 68.3 ...\n $ height: num [1:100] 153 182 188 142 137 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   id = col_double(),\n  ..   weight = col_double(),\n  ..   height = col_double()\n  .. )\n\nSave data into your data folder using the following method\n\nlibrary(\"here\")\n# save data using here\nsaveRDS(testdata, here::here(\"data\", \"td.RDS\"))\n\nRead data back into R\n\ntd <- readRDS(here::here(\"data\", \"td.RDS\"))\n\nUsing the td dataset, write a linear model for height ~ weight as above\n\n\n\n\ntd <- readRDS(here::here(\"data\", \"td.RDS\"))\n# note a clever student will change the object to something other than `m1` which we used above\nm2<-lm(height ~ weight, data = td)\n\nsjPlot::tab_model(m2)\n\n \n\n\nheight\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n12.74\n\n\n5.51 – 19.98\n\n\n0.001\n\n\nweight\n\n\n1.87\n\n\n1.78 – 1.96\n\n\n<0.001\n\n\nObservations\n\n\n100\n\n\nR2 / R2 adjusted\n\n\n0.948 / 0.947\n\n\nCreate a coefficient plot\n\nsjPlot::plot_model(m2)\n\n\nCreate a prediction plot\n\nplot(\n  ggeffects::ggpredict( m2 , effects = \"height\" ),\n  add.data = TRUE\n)\n$weight\n\n\nExtra credit, how would you interpret the intercept in this model?*\n\n# a zero intercept makes no sense because biologically speaking weight can't be zero\nlibrary( ggplot2 )\nsummary( m2 )\n\nCall:\nlm(formula = height ~ weight, data = td)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.4761  -6.2215  -0.1467   4.8392  23.9751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.74337    3.64557   3.496 0.000712 ***\nweight       1.87029    0.04432  42.201  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.683 on 98 degrees of freedom\nMultiple R-squared:  0.9478,    Adjusted R-squared:  0.9473 \nF-statistic:  1781 on 1 and 98 DF,  p-value: < 2.2e-16\n\n```\n\n\n",
    "preview": "workbooks/W_2_s/workbook_2_answer_files/figure-html5/plotwomen-1.png",
    "last_modified": "2021-03-17T00:02:20+13:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "workbooks/W_3/",
    "title": "Workbook 3",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-03-10",
    "categories": [],
    "contents": "\n\nContents\nLibraries we will need\nQuestion 1: Why is this graph not printing any output?\nQuestion 2. Using the mpg dataset, graph the relationship between city milage and highway mileage by year manufacture\nQuestion 3. Edit this graph so that the x axis and the y axis both start at 0\nQuestion 4: what is one benefit and one limitation for this graph above (in which the x and y values start at 0?)\nQuetion 5. Which of these two graphs do you prefer and why?\nQuetion 6. add a facet to this graph for the “class” variable\nQuestion 7. which graph is more informative and why?\nQuestion 8. remove the legend from the facet graph above (g4)\nQuestion 9 Convert the y variable to “numeric” and graph the relationship betweeen religiousity (x-axis) and `thr_mus`` (y-axis) in the ISSP dataset. Create new axis labels\nQuestion 10. Note that I have the following graph should start from 1 and run to 4 but currently runs from 0-4. Fix the graph\nExtra Question 11. Find one way of improving the the following code and explain your answer\n\nLibraries we will need\n\n\nlibrary(\"tidyverse\")\nlibrary(\"patchwork\")\nlibrary(\"readr\")\nlibrary(\"sjPlot\")\n\n\n\n\n\n# set theme\ntheme_set(theme_classic())\n\n\n\nQuestion 1: Why is this graph not printing any output?\n\n\nlibrary(\"tidyverse\")\nggplot(data = mtcars) + \n  aes(mpg, wt, colour=factor(cyl))\n\n\n\n\nQuestion 2. Using the mpg dataset, graph the relationship between city milage and highway mileage by year manufacture\nQuestion 3. Edit this graph so that the x axis and the y axis both start at 0\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Relationship bewtween engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\")\n\n\n\n\nQuestion 4: what is one benefit and one limitation for this graph above (in which the x and y values start at 0?)\nQuetion 5. Which of these two graphs do you prefer and why?\n\n\ng1 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, colour =  class )) + \n  labs(title = \"Relationship bewtween engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\n\ng2 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape =  class )) + \n  labs(title = \"Relationship bewtween engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\n\nlibrary(\"patchwork\")\n\ng1 / g2 + plot_annotation(title = \"Which plot do you prefer and why?\", tag_levels = 'a')\n\n\n\n\nQuetion 6. add a facet to this graph for the “class” variable\n\n\ng2 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape =  class )) + \n  labs(title = \"Relationship bewtween engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\nQuestion 7. which graph is more informative and why?\n\n\n\nQuestion 8. remove the legend from the facet graph above (g4)\n\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_wrap( ~ class, nrow = 2)\n\n\n\n\nQuestion 9 Convert the y variable to “numeric” and graph the relationship betweeen religiousity (x-axis) and `thr_mus`` (y-axis) in the ISSP dataset. Create new axis labels\nDownload the ISSP questionaire used in this study [here]: (https://github.com/go-bayes/psych-447/blob/main/data_raw/ISSP/ISSP_2018_Religion_Questionnaire_final_version1-2.pdf)\nNote1\n\n\n# read the issp dataset for questionaire see: ISSP_2018_Religion_Questionnaire_final_version1-2.pdf\n# \n# subset of data from the issp dataset\nissp <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/issp.csv\"))\n\n\n\n\n\n# note that we need to check our data\nhead(issp)\n\n# when we do we find that classes of the variables need to be adjusted\nstr(issp)\n\n\n\nLet’s get the data into shape using dplyr. Run this code below\n\n\nip <- issp %>%\n  mutate(\n    id = factor(id),\n    thr_ath = as.factor(thr_ath),\n    thr_bd = as.factor(thr_bd),\n    thr_ch = as.factor(thr_ch),\n    thr_hd = as.factor(thr_hd),\n    thr_jw = as.factor(thr_jw),\n    thr_ms = as.factor(thr_ms),\n    neg_ath = as.factor(neg_ath),\n    neg_bd = as.factor(neg_bd),\n    neg_ch = as.factor(neg_ch),\n    neg_hd  = as.factor(neg_hd),\n    neg_jw = as.factor(neg_jw),\n    neg_ms = as.factor(neg_ms),\n    wave  = as.factor(wave),\n    nzeuro = as.factor(nzeuro),\n    eduyears = as.numeric(eduyears),\n    male = as.factor(male),\n    age = as.numeric(age),\n    rightwing = as.numeric(rightwing),\n    rural = as.factor(rural),\n    religiosity = as.numeric(religiosity)\n  )\n\n\n\nQuestion 10. Note that I have the following graph should start from 1 and run to 4 but currently runs from 0-4. Fix the graph\n\n\nlibrary(ggplot2)\nggplot(data = ip, aes(y = as.numeric(thr_ms), x = religiosity, colour = wave))  +  geom_jitter(alpha = .1) + \n  geom_smooth(method = lm, fullrange = FALSE, alpha = 0.1) +\n   scale_y_continuous(limits = c(0,4))\n\n\n\n\nExtra Question 11. Find one way of improving the the following code and explain your answer\n\n\nlibrary(sjPlot)\nplot_xtab(\n    ip$thr_ms,\n    ip$wave,\n    show.total = F,\n    show.n = F,\n    geom.colors = c(\"lightgreen\", \"darkred\")\n  ) +\n  xlab(\"Threatened by Muslims\") +  ylab(\"Frequency\") +\n  #scale_y_continuous(limits=c(0,7)) + #theme(plot.title = element_text(size=09))\n  theme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\nData were collected with Barry Milne and Martin Van dataset; the data are only authorised for the purposes of teaching.↩︎\n",
    "preview": "workbooks/W_3/workbook_3_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-10T14:58:42+13:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
