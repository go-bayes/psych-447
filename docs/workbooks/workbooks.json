[
  {
    "path": "workbooks/W_4_s/",
    "title": "Week 4 solutions",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-03-24",
    "categories": [],
    "contents": "\n\nContents\nSetup\nLibraries\nImport data\nQ1a. Warmup\nSolution\nQ1b. Change class for all instances of a class\nSolution\nQ2a. Scale, center, transform\nSolution\nQ2b Data wrangle\nSolution\nQ3. Working with dates\nQ4 Caculating dates and creating summaries\nSolution\nQ5. Working with date intervals\nSolution\nQ6 Create an ordered factor from numeric data\nSolution\nAnother solution\nQ7 Make a summary table\nSolution\nQ7. Make a summary graph\nSolution\nQ8. Correlation graph\nSolution\nQ9 Create a blank papaja report\nQ10 Patchwork\n\n\nSetup\nLibraries\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(pmdplyr)\nlibrary(correlation)\nlibrary(ggraph)\nlibrary(patchwork)\n\n\n\nRun this code\n\n\n#easystats::install_easystats_latest()\n\n\n\nImport data\n\n\n# read data\nnz_0 <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"))\n\n# to relevel kessler 6 variables\nf<-c(\"None Of The Time\",\"A Little Of The Time\",\"Some Of The Time\",  \"Most Of The Time\", \"All Of The Time\")\n\n# get data into shape\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) \n\n\n\nQ1a. Warmup\nMake a new dataframe from the nz dataframe. Call it df\nSolution\n\n\ndf <- nz\n\n\n\nQ1b. Change class for all instances of a class\nMake all the hours variables into integers\nSolution\nThis would be the quick way:\n\n\nhead(df)\n\n\n# A tibble: 6 x 63\n     Id Wave  years   Age Male  Gender   Edu Partner BornNZ Employed\n  <dbl> <fct> <dbl> <dbl> <fct>  <dbl> <dbl>   <dbl>  <dbl>    <dbl>\n1     1 2019  10.4     47 Male       1     3       1      1        1\n2     1 2018   9.47    46 Male       1     3       1      1        0\n3     2 2019  10.6     47 Male       1     7       1      1        1\n4     2 2018   9.90    46 Male       1     7       1      1        1\n5     3 2019  10.2     53 Male       1     4       0      1        1\n6     3 2018   9.20    52 Male       1     4       0      1        1\n# … with 53 more variables: BigDoms <fct>, TSCORE <dbl>,\n#   GenCohort <fct>, Religion.Church <dbl>,\n#   Religion.Believe.Cats <dbl>, Relid <dbl>, HLTH.Fatigue <dbl>,\n#   HLTH.SleepHours <dbl>, HLTH.BMI <dbl>, HLTH.Weight <dbl>,\n#   HLTH.Height <dbl>, HomeOwner <dbl>, Pol.Orient <dbl>,\n#   PATRIOT <dbl>, Env.SatNZEnvironment <dbl>,\n#   Env.MotorwaySpend <dbl>, Env.PubTransSubs <dbl>,\n#   Env.ClimateChgConcern <dbl>, LIFEMEANING <dbl>,\n#   Hours.Internet <dbl>, Issue.GovtSurveillance <dbl>,\n#   Issue.RegulateAI <dbl>, Issue.IncomeRedistribution <dbl>,\n#   Hours.Exercise <dbl>, Hours.Work <dbl>, Hours.News <dbl>,\n#   CONSCIENTIOUSNESS <dbl>, EXTRAVERSION <dbl>, AGREEABLENESS <dbl>,\n#   OPENNESS <dbl>, Religious <fct>, Spiritual.Identification <dbl>,\n#   Believe.God <fct>, Believe.Spirit <fct>, HoursCharity <dbl>,\n#   CharityDonate <dbl>, Your.Personal.Relationships <dbl>,\n#   Your.Future.Security <dbl>, Standard.Living <dbl>,\n#   NZ.Economic.Situation <dbl>, NZ.Social.Conditions <dbl>,\n#   NZ.Business.Conditions <dbl>, Emp.JobSecure <dbl>,\n#   Issue.Food.GMO <dbl>, Env.SacMade <lgl>, KESSLER6sum <dbl>,\n#   FeelHopeless <fct>, FeelDepressed <fct>, FeelRestless <fct>,\n#   EverythingIsEffort <fct>, FeelWorthless <fct>, FeelNervous <fct>,\n#   date <date>\n\ndf <-df %>%\n  dplyr::mutate(across(starts_with(\"Hours\"), as.integer))\n\n\n\nBut more likely they’ll do it by hand, which is fine:\n\n\ndf<- df%>%\n  mutate(Hours.Exercise = as.integer(Hours.Exercise),\n         Hours.Internet = as.integer(Hours.Internet))\n\n\n\nAnd so on…\nQ2a. Scale, center, transform\nCreate a new indicator that standardises the Pol.Orient variable, create a new indicator that centers the Pol.Orient variable, create a new indicator that centres the Age variable in decade-long units. Do this in a single piped workflow.\nPrint the head of the data frame so that we can see your work \nSolution\n\n\ndf1<-df %>%\n  dplyr::select(Pol.Orient,\n         Age) %>%\n  dplyr::mutate(\n    conservative_s = scale(Pol.Orient),\n    conservative_c = scale(Pol.Orient, center = TRUE, scale = FALSE),\n    age_decade_c = scale(Age, center = TRUE, scale = FALSE) / 10\n  )%>%\n  glimpse()\n\n\n\nQ2b Data wrangle\nSelect Hour.Exercise and filter Wave 2019.\nSolution\nNote that this won’t. We need to filter first because otherwise there would be no Wave column to select!\n\n\ndf$Wave\ndf%>%\n  dplyr::select(\"Hours.Exercise\")%>%\n  dplyr::filter(Wave == 2019)%>%\n  head()\n\n\n\nQ3. Working with dates\nWhat are the maximum number of responses for a single day in 2018 and the maximum number of responses for a single day in 2019?\nSolution\nThis is how I’d do it:\n\n\nnz %>%\n  group_by(date, Wave)%>%\n  count() %>%\n  group_by(Wave)%>%\n  dplyr::filter(n == max(n))\n\n\n# A tibble: 2 x 3\n# Groups:   Wave [2]\n  date       Wave      n\n  <date>     <fct> <int>\n1 2018-06-21 2018    112\n2 2019-12-03 2019     54\n\nHere’s a longer approach:\n\n\n# 2018 take the first\nnz %>%\n  group_by(date, Wave) %>%\n  count() %>%\n  filter(Wave == 2018) %>%\n  arrange(desc(n)) %>%\n  glimpse()\n\n\nRows: 312\nColumns: 3\nGroups: date, Wave [312]\n$ date <date> 2018-06-21, 2018-06-22, 2018-06-24, 2018-06-20, 2018-0…\n$ Wave <fct> 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2…\n$ n    <int> 112, 93, 80, 67, 59, 58, 52, 44, 44, 34, 27, 27, 27, 27…\n\n# 2019  take the first\nnz %>%\n  group_by(date, Wave) %>%\n  count() %>%\n  filter(Wave == 2019) %>%\n  arrange(desc(n))%>%\n  glimpse()\n\n\nRows: 298\nColumns: 3\nGroups: date, Wave [298]\n$ date <date> 2019-12-03, 2019-10-04, 2019-12-02, 2019-12-09, 2019-1…\n$ Wave <fct> 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2…\n$ n    <int> 54, 47, 46, 46, 45, 44, 43, 39, 39, 38, 38, 37, 35, 34,…\n\nQ4 Caculating dates and creating summaries\nHow many days are there between the date with the highest number of responses and the date with the second highest number of responses?\nBonus: Calculate difference between the number of responses on the highest response date and second highest response date.\nSolution\nHere is an approach:\n\n\nts<-nz %>%\n  group_by(date)%>%\n  count() %>%\n  arrange(desc(n)) %>%\n  as_tibble() %>%\n  slice(c(1:2)) %>%\n  mutate(diff_days = date - lag(date),\n         diff_n = n - lag(n))\nts\n\n\n# A tibble: 2 x 4\n  date           n diff_days diff_n\n  <date>     <int> <drtn>     <int>\n1 2018-06-21   112 NA days       NA\n2 2018-06-22    93  1 days      -19\n\nQ5. Working with date intervals\nSuppose you were born on Dec 25, 1995 at 5.02:22 am Calculate your age in months on March 20,2021, at 1:22:04pm. (Hint use the lubridate package. Look up the interval function).\nSolution\n\n\nlibrary(lubridate)\nalive <-lubridate::interval(ymd_hms(\"1995-12-25 05:02:22\"), ymd_hms(\"2021-03-21 13:22:04\"))\n\ntime_length(alive, \"months\")\n\n\n[1] 302.8695\n\nQ6 Create an ordered factor from numeric data\nThe Religion.Church variable contains responses to the question: “How many times each month do you attend church or religious service?”\nCreate factor with the following three levels:\nPeople who attend church 0 times per month,\nPeople who attend church 1-3 times per month,\nPeople who attend church 4 or more times per month.\nMake sure to re-level the factor so that the ordinal ranking moves from lowest to highest.\nSolution\n\n\nnz %>%\n  dplyr::mutate(church_attendance_cats = cut(\n    Religion.Church,\n    breaks = c(-Inf, 0, 3.99, Inf),\n    labels = c(\"zero\", \"less_4\", \"gr_4\"),\n    right = TRUE,\n  )) %>% \n  group_by(church_attendance_cats) %>%\n  count()\n\n\n# A tibble: 4 x 2\n# Groups:   church_attendance_cats [4]\n  church_attendance_cats     n\n  <fct>                  <int>\n1 zero                    3366\n2 less_4                   288\n3 gr_4                     363\n4 <NA>                     109\n\nAnother solution\n\n\nnz %>%\n  dplyr::select(Religion.Church) %>%\n  dplyr::mutate(church_attendance_cats = as.factor(ifelse(\n    Religion.Church == 0,\n    \"zero\",\n    ifelse(Religion.Church < 4, \"less_4\", \"gr_4\")\n  ))) %>%\n  group_by(church_attendance_cats) %>%\n  count()\n\n\n# A tibble: 4 x 2\n# Groups:   church_attendance_cats [4]\n  church_attendance_cats     n\n  <fct>                  <int>\n1 gr_4                     363\n2 less_4                   288\n3 zero                    3366\n4 <NA>                     109\n\nQ7 Make a summary table\nUsing methods described in lecture 4, create a table for average hours of sleep by month in the nz dataset\nSolution\n\n\nlibrary(kableExtra)\ntabnz<-nz %>%\n  select(Id, date, HLTH.SleepHours) %>%\n  mutate(month = month(date, label = TRUE))%>%\n  group_by(month) %>%\n  summarise(\n    average_sleep =  mean(HLTH.SleepHours, na.rm = TRUE),\n    sd_sleep  =  sd(HLTH.SleepHours, na.rm = TRUE),\n    n  = n()  ) \ntabnz%>%\n  kbl(caption = \"Distress by month\") %>%\n   kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\n\n\n\nTable 1: Distress by month\n\n\nmonth\n\n\naverage_sleep\n\n\nsd_sleep\n\n\nn\n\n\nJan\n\n\n6.810909\n\n\n1.1739898\n\n\n265\n\n\nFeb\n\n\n6.666667\n\n\n0.9537048\n\n\n174\n\n\nMar\n\n\n6.792891\n\n\n1.1216895\n\n\n220\n\n\nApr\n\n\n7.279487\n\n\n1.1232327\n\n\n81\n\n\nMay\n\n\n7.064935\n\n\n1.0922895\n\n\n81\n\n\nJun\n\n\n6.927352\n\n\n1.0601600\n\n\n725\n\n\nJul\n\n\n6.957453\n\n\n1.1072359\n\n\n507\n\n\nAug\n\n\n6.976087\n\n\n1.0035276\n\n\n240\n\n\nSep\n\n\n6.666667\n\n\n1.0817994\n\n\n100\n\n\nOct\n\n\n6.912844\n\n\n1.0240630\n\n\n772\n\n\nNov\n\n\n6.940741\n\n\n1.2160484\n\n\n307\n\n\nDec\n\n\n6.872973\n\n\n1.0420464\n\n\n654\n\n\nQ7. Make a summary graph\nGraph the average hours of sleep by month including 95% confidence intervals\nBriefly explain why some intervals are wider than others.\nSolution\n\n\nnz %>%\n  select(Id, date, HLTH.SleepHours) %>%\n  mutate(month = month(date, label = TRUE))%>%\n  group_by(month) %>%\n  summarise(\n    mn_sh =  mean(HLTH.SleepHours, na.rm = TRUE),\n    sd_sh  =  sd(HLTH.SleepHours, na.rm = TRUE),\n    n_sh  = n()  ) %>%\n  mutate(\n    se_sh  = sd_sh  / sqrt(n_sh ),\n    lw_ci = mn_sh  - qt(1 - (0.05 / 2), n_sh  - 1) * se_sh ,\n    up_ci = mn_sh  + qt(1 - (0.05 / 2), n_sh  - 1) * se_sh \n  ) %>%\n  ggplot(., aes(x = month, y = mn_sh , colour = mn_sh )) +\n  geom_errorbar(aes(ymin = lw_ci, ymax = up_ci), width = .1) +\n  geom_point(size = 3)  +\n  scale_y_continuous(limits = c(0,8)) + \n  theme_classic() + scale_fill_viridis_d()\n\n\n\n\nSome intervals are wider than others because the number of responses by month varies, and a confidence interval divides by the number of cases.\n\n\n\nHere we see high standard errors of the mean with low n’s\n\n\n\nWe can graph the year-wise differences, focusing on April:\n\n\n\nQ8. Correlation graph\nCreated a correlation graph for the items in the Kessler 6 scale\nThese are FeelHopeless,FeelDepressed,FeelRestless,EverythingIsEffort,FeelWorthless,FeelNervous\nHint you must transform the factors into integers.\nWhat do you find most interesting about this plot? Explain.\nSolution\n\n\nk6<-nz%>%\n  select(FeelHopeless,FeelDepressed,FeelRestless,EverythingIsEffort,\n         FeelWorthless,FeelNervous, Id)%>%\n  mutate_all(.,as.numeric) %>%\n  mutate(Id= as.factor(Id))# make numeric for correlation plot\n\nk6 %>%\n  correlation::correlation( partial = FALSE, multilevel = TRUE ) %>%\n  plot() + theme_gray()\n\n\n\n\nI find it interesting that Nervousness and Restlessness are not correlated, or inversely correlated with Depression.\nSetting the correlations to partial does not destroy this effect.\n\n\nk6 %>%\n  correlation::correlation( partial = TRUE, multilevel = TRUE ) %>%\n  plot() + theme_gray()\n\n\n\n\nQ9 Create a blank papaja report\nInclude your your name, affiliation, contributors and r packages used in your analysis\nQ10 Patchwork\nUse the patchwork library to create a figure with two plots on top of each other.\n\n\nlibrary(patchwork)\nlibrary(ggplot2)\np1 <- qplot(mtcars$cyl, geom = \"histogram\") + labs(title = \"this plot\") + xlab(\"mt cycle\")\np2 <- qplot(mtcars$disp, geom = \"histogram\")+ labs(title = \"that plot\")\n\np1/p2 + plot_annotation(title = \"my title\", tag_levels = 'a') + xlab(\"mt cycle\") +  plot_layout(guides = 'collect')\n\n\n\n\nFigure 1: \n\n\n\n\n\n\n",
    "preview": "workbooks/W_4_s/lab_4_solutions_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2021-03-24T12:00:07+13:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "workbooks/W_5/",
    "title": "Week 5 workbook",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-03-24",
    "categories": [],
    "contents": "\n\nContents\nSetup\nLibraries\nImport nz data\nImport Pearson and Lee mother’s and daughters data\nNote\n\nQ1. Create a descriptive table and a descriptive graph for the HLTH.Weight and HLTH.Height variables in the nz dataset\nQ2. Write up a sample summary of the HLTH.Weight and HLTH.Height variables in the nz dataset in APA style.\nQ3. Regression height ~ weight and report results\nQ4. Regress height ~ male_id and report results\nQ5. Regression to predict\nQ6. Bonus, not marked\n\nSetup\nLibraries\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(lubridate)\nlibrary(kableExtra)\nlibrary(gtsummary)\n\n\n\nImport nz data\n\n\n# read data\nnz_0 <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"))\n\n# to relevel kessler 6 variables\nf<-c(\"None Of The Time\",\"A Little Of The Time\",\"Some Of The Time\",  \"Most Of The Time\", \"All Of The Time\")\n\n# get data into shape\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(male_id = as.factor(Male)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE)\n\n\n\nImport Pearson and Lee mother’s and daughters data\n\n\nmd_df <- data.frame(read.table(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt\"), header=TRUE))\n# Center mother's height for later example\nmd_df <- md_df %>%\n  dplyr::mutate(mother_height_c = as.numeric(scale(mother_height, center = TRUE, scale = FALSE)))\ndplyr::glimpse(md_df)\n\n\nRows: 5,524\nColumns: 3\n$ daughter_height <dbl> 52.5, 52.5, 53.5, 53.5, 55.5, 55.5, 55.5, 55…\n$ mother_height   <dbl> 59.5, 59.5, 59.5, 59.5, 59.5, 59.5, 59.5, 59…\n$ mother_height_c <dbl> -2.9987328, -2.9987328, -2.9987328, -2.99873…\n\n# In 1903, Pearson and Lee collected 5,524 observations from mother/daughter height pairs. See lecture 5 for details\n\n\n\nNote\nFor all exercises below, use only the 2019 wave of the nz dataset.\nQ1. Create a descriptive table and a descriptive graph for the HLTH.Weight and HLTH.Height variables in the nz dataset\nSelect HLTH.Weight, HLTH.Height from the nz dataset.\nFilter only the 2019 wave.\nCreate a descriptive table and graph these two variables\nAnnotate your workflow (at each step, describe what you are doing and why).\nQ2. Write up a sample summary of the HLTH.Weight and HLTH.Height variables in the nz dataset in APA style.\nUsing the analysis in Q1, describe Height and Weight in the nz dataset\nWrite brief APA methods summary for these two variables.\nNote: *if useful, use the ‘male_id’ variable to clarify interesting or puzzling features of the HLTH.Weight and HLTH.Height responses.\nNZAVS data dictionary\nQ3. Regression height ~ weight and report results\nUsing the nz dataset, write a regression model for height as predicted by weight.\nCreate a table for your results.\nCreate a graphs/graphs to clarify the results of your regression model.\nBriefly report your results.\nQ4. Regress height ~ male_id and report results\nUsing the nz dataset, write a regression model for height as predicted by male_id\nCreate a table for your results.\nCreate a graphs/graphs to clarify the results of your regression model.\nBriefly report your results.\nQ5. Regression to predict\nUsing the regression coefficients from the Pearson and Lee 1903 dataset\nPredict the heights of daughters of women in the nz dataset.\nQ6. Bonus, not marked\nOn average, how much taller or shorter are women in New Zealand as sampled in 2019 nz dataset compared with women in 1903 as sampled in the Pearson and Lee dataset.\nClarify your inference.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-24T11:54:53+13:00",
    "input_file": {}
  },
  {
    "path": "workbooks/W_2/",
    "title": "Week 2 Workbook",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-23",
    "categories": [],
    "contents": "\n\nContents\nCreate Repos (if you have not already)\nCreate R projects\nWorkbook 2\n\nCreate Repos (if you have not already)\nCreate a repository for 447-journals (if you don’t have one already)\nCreate a repository for your 447-workbooks.\nYou should initialise a Readme statement and a .gitignore document (make sure to click the R .gitignore)\nCreate R projects\nCreate R-projects in each of these repositories.\nIn your workbooks folder, using the + New Folder command, create a folder called data (or similar) This is where you’ll store your data.\nCreate a file called figs (where you’ll stor your figures) 5 Create a folder called workbooks, or similar this is where you’ll store your weekly workbooks.\nWorkbook 2\nOpen an R markdown document, and filling in your name\nThe first code chunk is for your preferences.\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\nFind the keyboard shortcuts menu.\nMemorise the keyboard shortcut for insert code chunk which is: ⌥⌘I\nUse the mac to insert a code chunk.\nMemorise the shortcut for evaluate which is ⌘-return^ use for all tasks in this workbook.\nUse R to do 10 mathematical calculations\n\n\n4+5\n\n\n[1] 9\n\n\n\n3^3\n\n\n[1] 27\n\nShow your work\nUsing R, find the square root of 324\nIn the iris dataset, find the flower with the longest Sepal.Length. (hint, use the max function.) Show the column details for this flower only.\nIn the iris dataset, find the flower with the shortest Sepal.Length. (hint, use the min function.)\nCalculate the difference bewteen the flower with the longest and shortest Sepal.Length\nMake a 7 column x 100 row dataframe using c, :, dataframe\nRename the columns of the dataframe using the names of snow white dwarfs, or any names.\nUsing the dataset woman, write a linear model for height ~ weight\nCreate a table and coefficient plot for this model\nUsing ggeffects, create a prediction plot for this model.\n\nNext week we’ll be creating plots like this\n\n\nlibrary(ggplot2)\nggplot2::ggplot(data = women) + \n  geom_point(mapping = aes(x = height, y = weight)) + geom_smooth(method = lm, aes(x = height, y = weight)) + theme_classic() \n\n\n\n\nExplain what is being calculated here\n\n\nsum(women$weight > 140) / length(women$weight)\n\n\n[1] 0.4\n\nCalculate the proportion of women who are over the mean weight of women\n\n\n\n\n\n\nWhat are the advantages and disadvantages of creating more breaks in the Petal.Length indicator? Clarify your answer in a paragraph.\n\n\nhist(iris$Petal.Length)\n\n\n\n\n\n\nhist(iris$Petal.Length, breaks = 100)\n\n\n\n\nSpot the error in this code\n\n\n# here is one method. \nmh <- mean(women$height)\nsum(women$weight > mh) / length(women$height)\n\n\n\nReorder the columns of the woman dataset so that weight comes before height. Then rename the columns “w” and “h”.\n\n\n\nRead data into R using the following method:\n\n\n\n\n\n# read data from file\nlibrary(\"readr\")\ntestdata<- readr::read_csv(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/testdata1.csv\"))\nstr(testdata)\n\n\n\nSave data into your data folder using the following method\n\n\nlibrary(\"here\")\n# save data using here\nsaveRDS(testdata, here::here(\"data\", \"td.RDS\"))\n\n\n\nRead data back into R\n\n\ntd <- readRDS(here::here(\"data\", \"td.RDS\"))\nstr(td)\n\n\n\nUsing the td dataset, write a linear model for height ~ weight as above\n\n\n\nCreate a coefficient plot\nCreate a prediction plot\nExtra credit, how would you interpret the intercept in this model?*\n\n\n\n",
    "preview": "workbooks/W_2/workbook_2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-23T11:06:19+13:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "workbooks/W_3_s/",
    "title": "Week 3 Workbook Solutions",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-03-18",
    "categories": [],
    "contents": "\n\nContents\nLibraries we will need\nQuestion 1: Why is this graph not printing any output?\nSolution, there are no layers.\nQuestion 2. Using the mpg dataset, graph the relationship between city milage and highway mileage by year manufacture\nSolution\nQuestion 3. Edit this graph so that the x axis and the y axis both start at 0\nSolution\nQuestion 4: what is one benefits and one limitation of this graph (in which the x and y values start at 0?)\nSolution\nQuestion 5. Which of these two graphs do you prefer and why?\nSolution\nQuestion 6. add a facets to this graph for the “class” variable\nSolution\nQuestion 7. which graph is more informative and why?\nSolution\nQuestion 8. remove the legend from the facet graph above (g4)\nSolution\nQuestion 9 Convert the y variable to “numeric” and graph the relationship betweeen religiousity (x-axis) and `thr_mus`` (y-axis) in the ISSP dataset. Create new axis labels\nQuestion 10. Note that I have the following graph should start from 1 and run to 4 but currently runs from 0-4. Fix the graph\nSoution\nExtra question 11. Find one way of improving the the following code and explain your Solution\nSolution.\n\nLibraries we will need\n\n\nlibrary(\"tidyverse\")\nlibrary(\"patchwork\")\nlibrary(\"readr\")\nlibrary(\"sjPlot\")\nlibrary(\"MASS\")\n\ntheme_set(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1: Why is this graph not printing any output?\n\n\nlibrary(\"tidyverse\")\nggplot(data = mtcars) + \n  aes(mpg, wt, colour=factor(cyl))\n\n\n\n\nSolution, there are no layers.\nQuestion 2. Using the mpg dataset, graph the relationship between city milage and highway mileage by year manufacture\nSolution\n\n\nggplot(mpg, aes(\n  x = hwy,\n  y = cty,\n  colour = as.factor(year)\n)) +\n  geom_point() +\n  geom_smooth(method = loess)\n\n\n\n\nQuestion 3. Edit this graph so that the x axis and the y axis both start at 0\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\")\n\n\n\n\nSolution\n\n\n  ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") + \n  expand_limits(x = 0, y = 0)\n\n\n\n\nQuestion 4: what is one benefits and one limitation of this graph (in which the x and y values start at 0?)\nSolution\nExample benefits:\nWe clearly see that engine displacement starts at just below\nWe clearly understand that no highway mileage below 10 mpg doesn’t exist.\nExample weakness\nEven if the thresholds are not evident, we can already see the thresholds without starting at graphs at zero (i.e. there is no information)\nZero engine displacement and zero highway miles per gallon are not physically meaningful concepts, so starting the graphs at these thresholds adds no new information.\nQuestion 5. Which of these two graphs do you prefer and why?\n\n\ng1 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, colour =  class )) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\n\ng2 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape =  class )) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\n\nlibrary(\"patchwork\")\n\ng1 / g2 + plot_annotation(title = \"Which plot do you prefer and why?\", tag_levels = 'a')\n\n\n\n\nSolution\nArguably, patterns are easier to detect using the colour aesthetic but there are no hard and fast rules\nQuestion 6. add a facets to this graph for the “class” variable\n\n\ng2 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape =  class )) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\nSolution\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class) + \n  labs(title = \"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\nQuestion 7. which graph is more informative and why?\n\n\n\nSolution\nArguably the facets make the role of class more evident by making the class indicator more salient. Where possible,i t is a good idea to declutter your graph.\nQuestion 8. remove the legend from the facet graph above (g4)\n\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_wrap( ~ class, nrow = 2)\n\n\n\n\nSolution\n\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_wrap( ~ class, nrow = 2) +\n  theme(legend.position = \"none\") \n\n\n\n\nQuestion 9 Convert the y variable to “numeric” and graph the relationship betweeen religiousity (x-axis) and `thr_mus`` (y-axis) in the ISSP dataset. Create new axis labels\nDownload the ISSP questionaire used in this study [here]: (https://github.com/go-bayes/psych-447/blob/main/data_raw/ISSP/ISSP_2018_Religion_Questionnaire_final_version1-2.pdf)\nNote1\n\n\n# read the issp dataset for questionaire see: ISSP_2018_Religion_Questionnaire_final_version1-2.pdf\n# \n# subset of data from the issp dataset\nissp <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/issp.csv\"))\n\n\n\n\n\n# note that we need to check our data\nhead(issp)\n\n# when we do we find that classes of the variables need to be adjusted\nstr(issp)\n\n\n\nLet’s get the data into shape using dplyr. Run this code below\n\n\nip <- issp %>%\n  mutate(\n    id = factor(id),\n    thr_ath = as.factor(thr_ath),\n    thr_bd = as.factor(thr_bd),\n    thr_ch = as.factor(thr_ch),\n    thr_hd = as.factor(thr_hd),\n    thr_jw = as.factor(thr_jw),\n    thr_ms = as.factor(thr_ms),\n    neg_ath = as.factor(neg_ath),\n    neg_bd = as.factor(neg_bd),\n    neg_ch = as.factor(neg_ch),\n    neg_hd  = as.factor(neg_hd),\n    neg_jw = as.factor(neg_jw),\n    neg_ms = as.factor(neg_ms),\n    wave  = as.factor(wave),\n    nzeuro = as.factor(nzeuro),\n    eduyears = as.numeric(eduyears),\n    male = as.factor(male),\n    age = as.numeric(age),\n    rightwing = as.numeric(rightwing),\n    rural = as.factor(rural),\n    religiosity = as.numeric(religiosity)\n  )\n\n\n\nSolution\n\n\nlibrary(ggplot2)\nggplot(data = ip, aes(y = as.numeric(thr_ms), x = religiosity, colour = wave)) + \n  geom_smooth(method = lm, fullrange = FALSE, alpha = 0.1) + \n  labs(title = \"Religiosity predict Muslim acceptance post-Christchurch shootings\") + \n  xlab(\"Level of Religiosity (scale 1-7) \") + ylab (\"acceptance of Muslims (1-4)\")\n\n\n\n\nQuestion 10. Note that I have the following graph should start from 1 and run to 4 but currently runs from 0-4. Fix the graph\n\n\nlibrary(ggplot2)\nggplot(data = ip, aes(y = as.numeric(thr_ms), x = religiosity, colour = wave))  +  geom_jitter(alpha = .1) + \n  geom_smooth(method = lm, fullrange = FALSE, alpha = 0.1) +\n   scale_y_continuous(limits = c(0,4))\n\n\n\n\nSoution\n\n\nlibrary(ggplot2)\nggplot(data = ip, aes(y = as.numeric(thr_ms), x = religiosity, colour = wave))  +  geom_jitter(alpha = .1) + \n  geom_smooth(method = lm, fullrange = FALSE, alpha = 0.1) +\n   scale_y_continuous(limits = c(1,4))\n\n\n\n\nExtra question 11. Find one way of improving the the following code and explain your Solution\n\n\nlibrary(sjPlot)\nplot_xtab(\n    ip$thr_ms,\n    ip$wave,\n    show.total = F,\n    show.n = F,\n    geom.colors = c(\"lightgreen\", \"darkred\")\n  ) +\n  xlab(\"Threatened by Muslims\") +  ylab(\"Frequency\") +\n  #scale_y_continuous(limits=c(0,7)) + #theme(plot.title = element_text(size=09))\n  theme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\nSolution.\nE.g. add a title\n\n\nlibrary(sjPlot)\nplot_xtab(\n    ip$thr_ms,\n    ip$wave,\n    show.total = F,\n    show.n = F,\n    geom.colors = c(\"lightgreen\", \"darkred\")\n  ) +\n  xlab(\"Threatened by Muslims\") +  ylab(\"Frequency\") +\n  #scale_y_continuous(limits=c(0,7)) + #theme(plot.title = element_text(size=09))\n  theme(axis.text.x = element_text(angle = 20, hjust = 1)) + \n  labs(title = \"Comparison of sample responses to Muslim threat in 2018 an 2019\")\n\n\n\n\n\nData were collected with Barry Milne and Martin Van dataset; the data are only authorised for the purposes of teaching.↩︎\n",
    "preview": "workbooks/W_3_s/lab_3_solutions_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-18T17:35:24+13:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "workbooks/W_2_s/",
    "title": "Week 2 Workbook Solutions",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-17",
    "categories": [],
    "contents": "\n\nContents\nCreate Repos (if you have not already)\nSolution\nCreate R projects\nSolution\nWorkbook 2\nQ1 Open an R markdown document, and filling in your name\nQ2. The first code chunk is for your preferences.\nQ3. Find the keyboard shortcuts menu.\nQ4. Memorise the keyboard shortcut for insert code chunk which is: ⌥⌘I\nQ5. Use the mac to insert a code chunk.\nQ6. Memorise the shortcut for evaluate which is ⌘-return^ use for all tasks in this workbook.\nQ7. Use R to do 10 mathematical calculations\nSolution\nQ8. Using R, find the square root of 324\nSolution\n\nCreate Repos (if you have not already)\nCreate a repository for 447-journals (if you don’t have one already)\nCreate a repository for your 447-workbooks.\nYou should initialise a Readme statement and a .gitignore document (make sure to click the R .gitignore)\nSolution\nStudents should confirm that they’ve done this\nCreate R projects\nCreate R-projects in each of these repositories.\nIn your workbooks folder, using the + New Folder command, create a folder called data (or similar) This is where you’ll store your data.\nCreate a file called figs (where you’ll stor your figures) 5 Create a folder called workbooks, or similar this is where you’ll store your weekly workbooks.\nSolution\nIf students did not create a data folder, then the here package task won’t work.\nWorkbook 2\nQ1 Open an R markdown document, and filling in your name\nQ2. The first code chunk is for your preferences.\n\nknitr::opts_chunk$set(echo = TRUE)\n\nQ3. Find the keyboard shortcuts menu.\nQ4. Memorise the keyboard shortcut for insert code chunk which is: ⌥⌘I\nQ5. Use the mac to insert a code chunk.\nQ6. Memorise the shortcut for evaluate which is ⌘-return^ use for all tasks in this workbook.\nQ7. Use R to do 10 mathematical calculations\nSolution\nStudents should confirm that they’ve done this, e.g.:\n\n4 + 5\n[1] 9\n\n\n3 ^ 3\n[1] 27\n\nShow your work\nQ8. Using R, find the square root of 324\nSolution\nStudents should show their works*\nJB: Students should confirm that they’ve done this\nIn the iris dataset, find the flower with the longest Sepal.Length. (hint, use the max function.) Show the column details for this flower only.\n\n# answer:\nmax(iris$Sepal.Length)\n[1] 7.9\n\nIn the iris dataset, find the flower with the shortest Sepal.Length. (hint, use the min function.)\n\n# answer:\nmin(iris$Sepal.Length)\n[1] 4.3\n\nCalculate the difference bewteen the flower with the longest and shortest Sepal.Length\n\n# answers\nmax(iris$Sepal.Length) - min(iris$Sepal.Length)\n[1] 3.6\n# or\na <- max(iris$Sepal.Length)\nb <- min(iris$Sepal.Length)\n\nMake a 7 column x 100 row dataframe using c, :, dataframe\n\n# some quick fixes\ndf <- data.frame(\n  col1 = sample(1000, 100, replace = TRUE),\n  col2 = rnorm(n = 100, mean = 0, sd = 1),\n  col3 = 1:100,\n  col4 = rbinom(n = 100, size = 1, prob = .5),\n  col5 = rpois(n = 100, lambda = 10),\n  col6 = sample(c(\"a\", \"b\", \"c\", \"d\"), 100, replace = TRUE),\n  col7 =  rep(1:10, 10)\n)\n\nRename the columns of the dataframe using the names of snow white dwarfs, or any names.\n\n# using base R\nnames(df)[] <-\n  c(\n    \"sleepy\",\n    \"grumpy\",\n    \"happy\",\n    \"smelly\",\n    \"lovely\",\n    \"daggy\",\n    \"crazy\"\n    )\n\nUsing the dataset woman, write a linear model for height ~ weight\n\n# basic linear model\n\nm1 <-  lm ( height ~ weight, data = women)\n\nCreate a table and coefficient plot for this model\n\n# coefficient plot\nsjPlot::plot_model(m1)\n\n# table\nsjPlot::tab_model(m1)\n\n \n\n\nheight\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n25.72\n\n\n23.47 – 27.98\n\n\n<0.001\n\n\nweight\n\n\n0.29\n\n\n0.27 – 0.30\n\n\n<0.001\n\n\nObservations\n\n\n15\n\n\nR2 / R2 adjusted\n\n\n0.991 / 0.990\n\n\nUsing ggeffects, create a prediction plot for this model.\n\nplot(\n  ggeffects::ggpredict( m1 , effects = \"height\"),\n  add.data = TRUE\n)\n$weight\n\n\n\nNext week we’ll be creating plots like this\n\nlibrary(ggplot2)\nggplot2::ggplot(data = women) +\n  geom_point(mapping = aes(x = height, y = weight)) +\n  geom_smooth(method = loess, aes(x = height, y = weight)) +\n  theme_classic() \n\n\nExplain what is being calculated here\n\n# this is a proportion (i.e. the number of instances / the total number of cases)\nsum(women$weight > 140) / length(women$weight)\n[1] 0.4\n\nCalculate the proportion of women who are over the mean weight of women\n\n# This is onle approach\n# Find the mean woman weight\nmw <- mean( women$weight )\n\n# calculae the proportion of women who are over that weight \nsum(women$weight > mw) / length(women$weight)\n[1] 0.4666667\n\nWhat are the advantages and disadvantages of creating more breaks in the Petal.Length indicator? Clarify your answer in a paragraph.\n\n# advantates: major pattern of data evident; clearly two clusters (at least); no tempatation to over interpret patters in the graph\n\n# disadvantages, looks as if there is a cluster at zero; only two modes in the dataset are discernable\nhist( iris$Petal.Length )\n\n\n\n# advantages: we can find more than one mode;  distribution not clustered at zero\n# disadvantages: with such a small sample, it is tempting to read too much into all the modes. \n\nhist( iris$Petal.Length, \n      breaks = 100 )\n\n\nSpot the error in this code\n\n# here is one method. \nmh <- mean (women$height)\nsum(women$weight > mh) / length(women$height)\n\n\n# should be `sum(women$height > mh)\n\nReorder the columns of the woman dataset so that weight comes before height. Then rename the columns “w” and “h”.\n\n# here is one method\n# Bind columns as data frame\ndfa <- cbind.data.frame(women$weight, women$height)\n# change names\nnames(dfa)[] <- c(\"w\", \"h\")\n\nRead data into R using the following method:\n\n\n\n\n# read data from file\nlibrary(\"readr\")\ntestdata<- readr::read_csv(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/testdata1.csv\"))\nstr(testdata)\nspec_tbl_df [100 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ id    : num [1:100] 1 2 3 4 5 6 7 8 9 10 ...\n $ weight: num [1:100] 80.5 87.8 95.7 74.6 68.3 ...\n $ height: num [1:100] 153 182 188 142 137 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   id = col_double(),\n  ..   weight = col_double(),\n  ..   height = col_double()\n  .. )\n\nSave data into your data folder using the following method\n\nlibrary(\"here\")\n# save data using here\nsaveRDS(testdata, here::here(\"data\", \"td.RDS\"))\n\nRead data back into R\n\ntd <- readRDS(here::here(\"data\", \"td.RDS\"))\n\nUsing the td dataset, write a linear model for height ~ weight as above\n\n\n\n\ntd <- readRDS(here::here(\"data\", \"td.RDS\"))\n# note a clever student will change the object to something other than `m1` which we used above\nm2<-lm(height ~ weight, data = td)\n\nsjPlot::tab_model(m2)\n\n \n\n\nheight\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n12.74\n\n\n5.51 – 19.98\n\n\n0.001\n\n\nweight\n\n\n1.87\n\n\n1.78 – 1.96\n\n\n<0.001\n\n\nObservations\n\n\n100\n\n\nR2 / R2 adjusted\n\n\n0.948 / 0.947\n\n\nCreate a coefficient plot\n\nsjPlot::plot_model(m2)\n\n\nCreate a prediction plot\n\nplot(\n  ggeffects::ggpredict( m2 , effects = \"height\" ),\n  add.data = TRUE\n)\n$weight\n\n\nExtra credit, how would you interpret the intercept in this model?*\n\n# a zero intercept makes no sense because biologically speaking weight can't be zero\nlibrary( ggplot2 )\nsummary( m2 )\n\nCall:\nlm(formula = height ~ weight, data = td)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.4761  -6.2215  -0.1467   4.8392  23.9751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.74337    3.64557   3.496 0.000712 ***\nweight       1.87029    0.04432  42.201  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.683 on 98 degrees of freedom\nMultiple R-squared:  0.9478,    Adjusted R-squared:  0.9473 \nF-statistic:  1781 on 1 and 98 DF,  p-value: < 2.2e-16\n\n```\n\n\n",
    "preview": "workbooks/W_2_s/workbook_2_answer_files/figure-html5/plotwomen-1.png",
    "last_modified": "2021-03-17T00:02:20+13:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "workbooks/W_4/",
    "title": "Workbook 4",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-17",
    "categories": [],
    "contents": "\n\nContents\nSetup\nLibraries\nImport data\nQ1. Warmup\nQ2a. Scale, center, transform\nQ2b Data wrangle\nQ3. Working with dates\nQ4 Caculating dates and creating summaries\nQ5. Working with date intervals\nQ6 Create an ordered factor from numeric data\nQ7 Make a summary table\nQ7. Make a summary graph\nQ8. Correlation graph\nQ9 Create a blank papaja report\nQ10 Patchwork\n\n\n\n\n\nSetup\nLibraries\n\n\nif (!require(skimr)) install.packages('skimr')\nif (!require(lubridate)) install.packages('lubridate')\nif (!require(tidyverse)) install.packages('tidyverse')\nif (!requireNamespace(\"devtools\")) {\n  install.packages(\"devtools\")\n}\nif (!require(easystats)) devtools::install_github(\"easystats/easystats\")\n\n\n# Attaching packages (red = needs update)\n✔ insight     0.13.1.1   ✔ bayestestR  0.8.3.1 \n✔ performance 0.7.0.1    ✔ parameters  0.12.0.1\n✔ see         0.6.2.1    ⚠ effectsize  0.4.3.1 \n✔ correlation 0.6.0.1    ✔ modelbased  0.5.9   \n✔ report      0.2.0      \nWarnings or errors in CRAN checks for package(s) 'bayestestR', 'parameters', 'effectsize', 'correlation'.\nRestart the R-Session and update packages in red with 'easystats::easystats_update()'.\n\nif (!require(ggthemes)) install.packages('ggthemes')\nif (!require(pmdplyr)) install.packages(\"pmdplyr\")\nif (!require(kableExtra)) install.packages(\"kableExtra\")\n# this should be part of easystats but in case not:\nif (!require(report)) install.packages('report')\nif (!require(brms)) install.packages('brms')\nif (!require(lme4)) install.packages('lme4')\nif (!require(table1)) install.packages('table1')\nif (!require(modelsummary)) install.packages(\"modelsummary\")\nif (!require(naniar)) install.packages(\"naniar\")\nif (!require(ggraph)) install.packages(\"ggraph\")\nif (!require(gtsummary)) install.packages(\"gtsummary\")\n\n\n\n\n\n# You might need to run this\n# easystats::install_easystats_latest()\n\n\n\nImport data\n\n\n# read data\nnz_0 <- \n  readr::read_csv2(\n    url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\")\n    )\n\n# to relevel kessler 6 variables\nf<-c(\"None Of The Time\",\n     \"A Little Of The Time\",\n     \"Some Of The Time\",\n     \"Most Of The Time\",\n     \"All Of The Time\")\n\n# get data into shape\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) \n\n\n\nQ1. Warmup\nUsing the nz dataset, make all the hours variables into integers\nQ2a. Scale, center, transform\nCreate a new indicator that standardises the Pol.Orient variable, create a new indicator that centers the Pol.Orient variable, create a new indicator that centres the Age variable in decade-long units. Do this in a single piped workflow.\nPrint the head of the data frame so that we can see your work\nQ2b Data wrangle\nSelect Hour.Exercise and filter Wave 2019.\nQ3. Working with dates\nWhat are the maximum number of responses for a single day in 2018 and the maximum number of responses for a single day in 2019?\nQ4 Caculating dates and creating summaries\nHow many days are there between the date with the highest number of responses and the date with the second highest number of responses?\nBonus: Calculate difference between the number of responses on the highest response date and second highest response date.\nQ5. Working with date intervals\nSuppose you were born on Dec 25, 1995 at 5.02:22 am Calculate your age in months on March 20,2021, at 1:22:04pm. (Hint use the lubridate package. Look up the interval function).\nQ6 Create an ordered factor from numeric data\nThe Religion.Church variable contains responses to the question: “How many times each month do you attend church or religious service?”\nCreate factor with the following three levels:\nPeople who attend church 0 times per month,\nPeople who attend church 1-3 times per month,\nPeople who attend church 4 or more times per month.\nMake sure to re-level the factor so that the ordinal ranking moves from lowest to highest.\nQ7 Make a summary table\nUsing methods described in Lecture 4, create a table for average hours of sleep by month in the nz dataset\nQ7. Make a summary graph\nGraph the average hours of sleep by month including 95% confidence intervals\nBriefly explain why some intervals are wider than others.\nQ8. Correlation graph\nCreated a correlation graph for the items in the Kessler 6 scale\nThese are:\n-FeelHopeless, -FeelDepressed, -FeelRestless, -EverythingIsEffort, -FeelWorthless, -FeelNervous\nHint you must transform the factors into integers.\nWhat do you find most interesting about this plot? Explain.\nQ9 Create a blank papaja report\nInclude your your name, affiliation, contributors and r packages used in your analysis\nQ10 Patchwork\nUse the patchwork library to create a figure with two plots on top of each other. Use the tag_levels function to index each of the two plots. The graphs should describe some dimension of the truncated nz dataset.\n\n\n\n",
    "preview": "workbooks/W_4/k6.jpeg",
    "last_modified": "2021-03-17T01:32:36+13:00",
    "input_file": {}
  },
  {
    "path": "workbooks/W_3/",
    "title": "Workbook 3",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-03-10",
    "categories": [],
    "contents": "\n\nContents\nLibraries we will need\nQuestion 1: Why is this graph not printing any output?\nQuestion 2. Using the mpg dataset, graph the relationship between city milage and highway mileage by year manufacture\nQuestion 3. Edit this graph so that the x axis and the y axis both start at 0\nQuestion 4: what is one benefit and one limitation for this graph above (in which the x and y values start at 0?)\nQuetion 5. Which of these two graphs do you prefer and why?\nQuetion 6. add a facet to this graph for the “class” variable\nQuestion 7. which graph is more informative and why?\nQuestion 8. remove the legend from the facet graph above (g4)\nQuestion 9 Convert the y variable to “numeric” and graph the relationship betweeen religiousity (x-axis) and `thr_mus`` (y-axis) in the ISSP dataset. Create new axis labels\nQuestion 10. Note that I have the following graph should start from 1 and run to 4 but currently runs from 0-4. Fix the graph\nExtra Question 11. Find one way of improving the the following code and explain your answer\n\nLibraries we will need\n\n\nlibrary(\"tidyverse\")\nlibrary(\"patchwork\")\nlibrary(\"readr\")\nlibrary(\"sjPlot\")\n\n\n\n\n\n# set theme\ntheme_set(theme_classic())\n\n\n\nQuestion 1: Why is this graph not printing any output?\n\n\nlibrary(\"tidyverse\")\nggplot(data = mtcars) + \n  aes(mpg, wt, colour=factor(cyl))\n\n\n\n\nQuestion 2. Using the mpg dataset, graph the relationship between city milage and highway mileage by year manufacture\nQuestion 3. Edit this graph so that the x axis and the y axis both start at 0\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Relationship bewtween engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\")\n\n\n\n\nQuestion 4: what is one benefit and one limitation for this graph above (in which the x and y values start at 0?)\nQuetion 5. Which of these two graphs do you prefer and why?\n\n\ng1 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, colour =  class )) + \n  labs(title = \"Relationship bewtween engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\n\ng2 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape =  class )) + \n  labs(title = \"Relationship bewtween engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\n\nlibrary(\"patchwork\")\n\ng1 / g2 + plot_annotation(title = \"Which plot do you prefer and why?\", tag_levels = 'a')\n\n\n\n\nQuetion 6. add a facet to this graph for the “class” variable\n\n\ng2 <-ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape =  class )) + \n  labs(title = \"Relationship bewtween engine displacement and fuel efficiency in the mpg automobile dataset\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\nQuestion 7. which graph is more informative and why?\n\n\n\nQuestion 8. remove the legend from the facet graph above (g4)\n\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_wrap( ~ class, nrow = 2)\n\n\n\n\nQuestion 9 Convert the y variable to “numeric” and graph the relationship betweeen religiousity (x-axis) and `thr_mus`` (y-axis) in the ISSP dataset. Create new axis labels\nDownload the ISSP questionaire used in this study [here]: (https://github.com/go-bayes/psych-447/blob/main/data_raw/ISSP/ISSP_2018_Religion_Questionnaire_final_version1-2.pdf)\nNote1\n\n\n# read the issp dataset for questionaire see: ISSP_2018_Religion_Questionnaire_final_version1-2.pdf\n# \n# subset of data from the issp dataset\nissp <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/issp.csv\"))\n\n\n\n\n\n# note that we need to check our data\nhead(issp)\n\n# when we do we find that classes of the variables need to be adjusted\nstr(issp)\n\n\n\nLet’s get the data into shape using dplyr. Run this code below\n\n\nip <- issp %>%\n  mutate(\n    id = factor(id),\n    thr_ath = as.factor(thr_ath),\n    thr_bd = as.factor(thr_bd),\n    thr_ch = as.factor(thr_ch),\n    thr_hd = as.factor(thr_hd),\n    thr_jw = as.factor(thr_jw),\n    thr_ms = as.factor(thr_ms),\n    neg_ath = as.factor(neg_ath),\n    neg_bd = as.factor(neg_bd),\n    neg_ch = as.factor(neg_ch),\n    neg_hd  = as.factor(neg_hd),\n    neg_jw = as.factor(neg_jw),\n    neg_ms = as.factor(neg_ms),\n    wave  = as.factor(wave),\n    nzeuro = as.factor(nzeuro),\n    eduyears = as.numeric(eduyears),\n    male = as.factor(male),\n    age = as.numeric(age),\n    rightwing = as.numeric(rightwing),\n    rural = as.factor(rural),\n    religiosity = as.numeric(religiosity)\n  )\n\n\n\nQuestion 10. Note that I have the following graph should start from 1 and run to 4 but currently runs from 0-4. Fix the graph\n\n\nlibrary(ggplot2)\nggplot(data = ip, aes(y = as.numeric(thr_ms), x = religiosity, colour = wave))  +  geom_jitter(alpha = .1) + \n  geom_smooth(method = lm, fullrange = FALSE, alpha = 0.1) +\n   scale_y_continuous(limits = c(0,4))\n\n\n\n\nExtra Question 11. Find one way of improving the the following code and explain your answer\n\n\nlibrary(sjPlot)\nplot_xtab(\n    ip$thr_ms,\n    ip$wave,\n    show.total = F,\n    show.n = F,\n    geom.colors = c(\"lightgreen\", \"darkred\")\n  ) +\n  xlab(\"Threatened by Muslims\") +  ylab(\"Frequency\") +\n  #scale_y_continuous(limits=c(0,7)) + #theme(plot.title = element_text(size=09))\n  theme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\nData were collected with Barry Milne and Martin Van dataset; the data are only authorised for the purposes of teaching.↩︎\n",
    "preview": "workbooks/W_3/workbook_3_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-10T14:58:42+13:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "workbooks/W_1/",
    "title": "Week 1 Workbook",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-02-24",
    "categories": [],
    "contents": "\n\nContents\nTask 1 Apply for a github education account\nTask 2 Download R for free here:\nTask 3 Download Rstudio for free here:\nTask 4 Create your first repository\nTask 5 Commit your first Rmarkdown journal\nGrading points\nSubmit a link to your journal on Blackboard.\n\nTask 1 Apply for a github education account\napply here: https://education.github.com/pack/offers\nset up a github account.\nchoose a username that will be OK for professional purposes (e.g. your name)\nTask 2 Download R for free here:\nhttps://www.r-project.org\nTask 3 Download Rstudio for free here:\nhttps://rstudio.com/products/rstudio/download/#download\nUpdate R and Rstudio to the latest versions. Don’t worry if you have problems, we’ll be working through your problems in the first week, and throughout the course. We assume no prior experience with R or Git/GitHub\nTask 4 Create your first repository\nTask 5 Commit your first Rmarkdown journal\nCreate your first Rmarkdown document, and title it “My Journal”\nRecord your impressions of the process of getting set up with RStudio and GitHub\nRecord any help you have offered to other people, and any help you have sought from other people, along the way.\nSave the document and commit it to GitHub.\nNote: set your github repository to private if you wish the account to be private\nGrading points\nYour Rmarkdown document must include each of the following\n\nHeading\n\n\ncrossed out text\n\n\nlink to a website\n\n\nA footnote 1\n\n\nAn inspirational quote\n\n\nBONUS: figure out how to add a bibliographic citation, such as [@darwin1964origin]\n\n\nA record of help you sought and offered.\n\nSubmit a link to your journal on Blackboard.\nBlackboard isn’t our favourite tool, however, we need to respect the norms of our community. For this reason, presently all of your assessments need to be uploaded through Blackboard.\nOn Blackboard, pass us a link to your GitHub document on Blackboard for weekly assessments\nThis journal will count as both your weekly journal entry and your weekly workbook – double credits!\n\nHumanities scholars love footnotes.↩︎\n",
    "preview": {},
    "last_modified": "2021-02-24T18:56:41+13:00",
    "input_file": {}
  }
]
