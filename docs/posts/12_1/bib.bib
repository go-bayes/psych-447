
@article{rohrer2018, title={Thinking clearly about correlations and causation: Graphical causal models for observational data}, volume={1}, number={1}, journal={Advances in Methods and Practices in Psychological Science}, publisher={Sage}, author={Rohrer, Julia M}, year={2018}, pages={27–42} }

@book{barrett2021, title={ggdag: Analyze and Create Elegant Directed Acyclic Graphs}, url={https://CRAN.R-project.org/package=ggdag}, author={Barrett, Malcolm}, year={2021} }

@book{mcelreath2020, title={Statistical rethinking: A Bayesian course with examples in R and Stan}, publisher={CRC press}, author={McElreath, Richard}, year={2020} }

@article{honaker2011, title={Amelia II: A Program for Missing Data}, volume={45}, url={http://www.jstatsoft.org/v45/i07/}, number={7}, journal={Journal of Statistical Software}, author={Honaker, James and King, Gary and Blackwell, Matthew}, year={2011}, pages={1–47} }

@article{bhaskaran2014, title={What is the difference between missing completely at random and missing at random?}, volume={43}, ISSN={0300-5771, 1464-3685}, url={https://academic.oup.com/ije/article-lookup/doi/10.1093/ije/dyu080}, DOI={10.1093/ije/dyu080}, number={4}, journal={International Journal of Epidemiology}, author={Bhaskaran, Krishnan and Smeeth, Liam}, year={2014}, month={Aug}, pages={1336–1339} }

@article{blackwell2017, title={A Unified Approach to Measurement Error and Missing Data: Overview and Applications}, volume={46}, url={http://journals.sagepub.com/doi/full/10.1177/0049124115585360}, number={3}, journal={Sociological Methods and Research}, author={Blackwell, Matthew and Honaker, James and King, Gary}, year={2017}, pages={303–341} }

@book{gelman2006,
	title = {Data analysis using regression and multilevel/hierarchical models},
	author = {{Gelman}, {Andrew} and {Hill}, {Jennifer}},
	year = {2006},
	date = {2006},
	publisher = {Cambridge university press}
}

@book{mcelreath2020a,
	title = {Statistical rethinking: A Bayesian course with examples in R and Stan},
	author = {{McElreath}, {Richard}},
	year = {2020},
	date = {2020},
	publisher = {CRC press}
}
@article{McNeish2018,
abstract = {Empirical studies in psychology commonly report Cronbach's alpha as a measure of internal consistency reliability despite the fact that many methodological studies have shown that Cronbach's alpha is riddled with problems stemming from unrealistic assumptions. In many circumstances, violating these assumptions yields estimates of reliability that are too small, making measures look less reliable than they actually are. Although methodological critiques of Cronbach's alpha are being cited with increasing frequency in empirical studies, in this tutorial we discuss how the trend is not necessarily improving methodology used in the literature. That is, many studies continue to use Cronbach's alpha without regard for its assumptions or merely cite methodological articles advising against its use to rationalize unfavorable Cronbach's alpha estimates. This tutorial first provides evidence that recommendations against Cronbach's alpha have not appreciably changed how empirical studies report reliability. Then, we summarize the drawbacks of Cronbach's alpha conceptually without relying on mathematical or simulation-based arguments so that these arguments are accessible to a broad audience. We continue by discussing several alternative measures that make less rigid assumptions which provide justifiably higher estimates of reliability compared to Cronbach's alpha. We conclude with empirical examples to illustrate advantages of alternative measures of reliability including omega total, Revelle's omega total, the greatest lower bound, and Coefficient H. A detailed software appendix is also provided to help researchers implement alternative methods. (PsycINFO Database Record},
author = {McNeish, Daniel},
doi = {10.1037/met0000144},
issn = {1939-1463},
journal = {Psychological Methods},
month = {sep},
number = {3},
pages = {412--433},
pmid = {28557467},
title = {{Thanks coefficient alpha, we'll take it from here.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28557467 http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000144},
volume = {23},
year = {2018}
}
@article{Raykov2017,
abstract = {This note discusses the merits of coefficient alpha and their conditions in light of recent critical publications that miss out on significant research findings over the past several decades. That earlier research has demonstrated the empirical relevance and utility of coefficient alpha under certain empirical circumstances. The article highlights the fact that as an index aimed at informing about multiple-component measuring instrument reliability, coefficient alpha is dependable then as a reliability estimator. Therefore, alpha should remain in service when these conditions are fulfilled and not be abandoned.},
author = {Raykov, Tenko and Marcoulides, George A.},
doi = {10.1177/0013164417725127},
issn = {0013-1644},
journal = {Educational and Psychological Measurement},
keywords = {coefficient alpha,measuring instrument,population discrepancy,reliability,single-factor model},
month = {aug},
pages = {001316441772512},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Thanks Coefficient Alpha, We Still Need You!}},
url = {http://journals.sagepub.com/doi/10.1177/0013164417725127},
year = {2017}
}

@incollection{Nunnally1994,
author = {Nunnally, J.C. and Bernstein, I.H.},
booktitle = {Psychometric Theory},
edition = {3},
pages = {248--292},
title = {{The Assessment of Reliability.}},
year = {1994}
}

@article{Lance2006,
author = {Lance, C. E. and Butts, Marcus M. and Michels, Lawrence C.},
doi = {10.1177/1094428105284919},
file = {:C$\backslash$:/Users/Johannes.Karl/Dropbox/Papers/Lance, Butts, Michels/2006/The Sources of Four Commonly Reported Cutoff Criteria What Did They Really Say/Organizational Research Methods.pdf:pdf},
issn = {1094-4281},
journal = {Organizational Research Methods},
month = {apr},
number = {2},
pages = {202--220},
publisher = {SAGE Publications},
title = {{The Sources of Four Commonly Reported Cutoff Criteria: What Did They Really Say?}},
url = {http://orm.sagepub.com/cgi/doi/10.1177/1094428105284919},
volume = {9},
year = {2006}
}

@article{Norman2010,
abstract = {Reviewers of research reports frequently criticize the choice of statistical methods. While some of these criticisms are well-founded, frequently the use of various parametric methods such as analysis of variance, regression, correlation are faulted because: (a) the sample size is too small, (b) the data may not be normally distributed, or (c) The data are from Likert scales, which are ordinal, so parametric statistics cannot be used. In this paper, I dissect these arguments, and show that many studies, dating back to the 1930s consistently show that parametric statistics are robust with respect to violations of these assumptions. Hence, challenges like those above are unfounded, and parametric methods can be utilized without concern for "getting the wrong answer".},
author = {Norman, Geoff},
doi = {10.1007/s10459-010-9222-y},
issn = {1382-4996},
journal = {Advances in Health Sciences Education},
month = {dec},
number = {5},
pages = {625--632},
pmid = {20146096},
title = {{Likert scales, levels of measurement and the "laws" of statistics}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20146096 http://link.springer.com/10.1007/s10459-010-9222-y},
volume = {15},
year = {2010}
}

@article{Carifio2007,
abstract = {A recent article by Jamieson in Medical Education outlined some of the (alleged) abuses of "Likert scales" with suggestions about how researchers can overcome some of the (alleged) methodological pitfalls and limitations [1]. However, many of the ideas advanced in the Jamison article, as well as a great many of articles it cited, and similar recent articles in medical, health, psychology, and educational journals and books, are themselves common misunderstandings, misconceptions, conceptual errors, persistent myths and "urban legends" about "Likert scales" and their characteristics and qualities that have been propagated and perpetuated across six decades, for a variety of different reasons. This article identifies, analyses and traces many of these aforementioned problems and presents the arguments, counter arguments and empirical evidence that show these many persistent claims and myths about "Likert scales" to be factually incorrect and untrue. Many studies have shown that Likert Scales (as opposed to single Likert response format items) produce interval data and that the F-test is very robust to violations of the interval data assumption and moderate skewing and may be used to analyze "Likert data" (even if it is ordinal), but not on an item-by-item "shotgun" basis, which is simply a current research and analysis practice that must stop. After sixty years, it is more than time to dispel these particular research myths and urban legends as well as the various damage and problems they cause, and put them to bed and out of their misery once and for all.},
author = {Carifio, James and Perla, Rocco J},
file = {:C$\backslash$:/Users/Johannes.Karl/Dropbox/Papers/Carifio, Perla/2007/Ten Common Misunderstandings, Misconceptions, Persistent Myths and Urban Legends about Likert Scales and Likert Response Formats and their A./Journal of Social Sciences.pdf:pdf},
issn = {1549-3652},
journal = {Journal of Social Sciences},
keywords = {Likert,formats,measurement,psychological,scales},
number = {3},
pages = {106--116},
title = {{Ten Common Misunderstandings, Misconceptions, Persistent Myths and Urban Legends about Likert Scales and Likert Response Formats and their Antidotes}},
url = {https://thescipub.com/PDF/jssp.2007.106.116.pdf},
volume = {3},
year = {2007}
}
