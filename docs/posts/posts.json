[
  {
    "path": "posts/12_1/",
    "title": "Bayesian inference and consolidation of statistics",
    "description": "\"With an excursion into the whens and when nots of mediation\"",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-25",
    "categories": [],
    "contents": "\n\nContents\n\n\n\n\n\nProbability\nWe have been working with probability throughout this course.\nSuppose there is a test that is 99% accurate at detecting COVID if you have it.\nVery rarely it throws up a false positive,say one in a thousand.\nYou just tested positve. What is the probability that you have COVID? Our intuition is that we probably have COVID. However, let’s assume COVID is rare. Currently in NZ, there are about 50 cases, so 1 in 100,000. The background rate matters.\nBayes rule says\n\\[ Pr(COVID|Positive) = \\frac{Pr(Positive|COVID)\\times Pr (COVID}{Pr(Positive)}\n\\]\nWe plug in the numbers:\nPr_Positive_COVID <- 0.99 Pr_Positive_Healthy <- 0.01 Pr_COVID <- 0.00001\nCalculate the background probability of testing positive\nPr_Positive <- Pr_Positive_COVID * Pr_COVID + Pr_Positive_Healthy * ( 1 - Pr_COVID )\nNow calculated your probability of testing positive\nPr_COVID_Positive <- Pr_Positive_COVID * Pr_COVID / Pr_Positive Pr_COVID_Positive\nUncertainty\nFuture Horizons\n\n\n\n",
    "preview": "posts/12_1/distill-preview.png",
    "last_modified": "2021-03-23T22:23:22+13:00",
    "input_file": {},
    "preview_width": 1023,
    "preview_height": 876
  },
  {
    "path": "posts/11_1/",
    "title": "Missing data and measurement error",
    "description": "\"With an excursion into the whens and when nots of mediation\"",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-18",
    "categories": [],
    "contents": "\n\nContents\n\n\n\n\n\nRemember: parameters are not observed\nMeasurement error\nMissing eata\n\n\n\n",
    "preview": "posts/11_1/distill-preview.png",
    "last_modified": "2021-02-23T15:06:35+13:00",
    "input_file": {},
    "preview_width": 1106,
    "preview_height": 553
  },
  {
    "path": "posts/10_1/",
    "title": "Longitudinal data: within and between-individual effects",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-11",
    "categories": [],
    "contents": "\n\nContents\nLongitudional data and the multi-level model\nMediation from the vantage point of causal inference (when and when not)\n\nLongitudional data and the multi-level model\n\n\n\nMediation from the vantage point of causal inference (when and when not)\n\n\n\n",
    "preview": "posts/10_1/op2.png",
    "last_modified": "2021-03-23T21:38:34+13:00",
    "input_file": {},
    "preview_width": 1800,
    "preview_height": 600
  },
  {
    "path": "posts/9_1/",
    "title": "Introduction to multilevel models",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-04",
    "categories": [],
    "contents": "\n\nContents\nPartial pooling\nRandom intercepts\nCovariance\n\nPartial pooling\nRandom intercepts\nCovariance\n\n\n\n\n\n\n",
    "preview": "posts/9_1/op.png",
    "last_modified": "2021-03-23T22:24:03+13:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 1500
  },
  {
    "path": "posts/8_1/",
    "title": "Modelling binary, count data, and ordinal data",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-27",
    "categories": [],
    "contents": "\n\nContents\nBinary Data\nCount data\nZero infalted data\nOrdinal data\n\n\n\n\nBinary Data\nCount data\nZero infalted data\nOrdinal data\n\n\n\n",
    "preview": "posts/8_1/op.png",
    "last_modified": "2021-03-23T22:23:38+13:00",
    "input_file": {},
    "preview_width": 1000,
    "preview_height": 1000
  },
  {
    "path": "posts/7_1/",
    "title": "An introduction to directed acylical graphs (DAGS) ",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\n\nContents\nRequired readings\nWhat is causal confounding?\nThe difference between prediction and explanation?\nExamples of confounding\n\n\n\n\nRequired readings\nRequired readings are as follows:\nRohrer (2018)\nBarrett (2021)\nWhat is causal confounding?\n\n\n\nThe difference between prediction and explanation?\nExamples of confounding\n\n\n\nBarrett, Malcolm. 2021. Ggdag: Analyze and Create Elegant Directed Acyclic Graphs. https://CRAN.R-project.org/package=ggdag.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42.\n\n\n\n\n",
    "preview": "posts/7_1/op.png",
    "last_modified": "2021-04-06T12:15:03+12:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1920
  },
  {
    "path": "posts/6__1/",
    "title": "An introduction to simulation",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-30",
    "categories": [],
    "contents": "\n\nContents\nRequired readings\nOverview\nLearning outcomes\nFunctions for simulation\nrnorm\nrunif\nrep\nseq\nCustom sequences\n\nWhy simulate?\nCreate noise to understand inference\nWe can use simulation to explore potential problems with a sample\nSimulate a relationship between two variables\nSimulate a non-linear relationship\nCreate fake factors\nSimulate a relationship between two factors\nIs imbalance in my study causing a problem?\n\nAcknowledgments\n\n\n\n\n\n# Attaching packages (red = needs update)\n⚠ insight     0.13.1.1   ⚠ bayestestR  0.8.3.1 \n⚠ performance 0.7.0.1    ⚠ parameters  0.12.0.1\n⚠ see         0.6.2.1    ⚠ effectsize  0.4.3.1 \n⚠ correlation 0.6.0.1    ✔ modelbased  0.5.9   \n✔ report      0.2.0      \nWarnings or errors in CRAN checks for package(s) 'bayestestR', 'parameters', 'effectsize', 'correlation'.\nRestart the R-Session and update packages in red with 'easystats::easystats_update()'.\n\nRequired readings\nOverview\nThis week we will:\nLearn how to use R to generate random numbers\nUse random numbers to simulate data\nRelate lessons from simulation to lessons from regression.\nLearning outcomes\nBy learning how to simulate data you will better understand what a regression model is doing.\nAdditionally, you will need to understand simulation to follow the seminars on causal inference and multilevel modelling.\nFunctions for simulation\nrnorm\nrnorm is a R’s random number generator. Within this function:\nn: specifies the number of observations that you will generate\nsd: specifies the value of the standard deviation\nmean: specifies the value of the mean\n\n\n# seed\nset.seed(123)\n# generate random numbers\nds <- rnorm(n = 1000,\n            mean = 0,\n            sd = 1)\ndplyr::glimpse(ds)\n\n\n num [1:1000] -0.5605 -0.2302 1.5587 0.0705 0.1293 ...\n\nWe can create a histogram:\n\n\np1 <- ggplot2::qplot(ds) + labs(title = \"1st random number list\")\nplot(p1)\n\n\n\n\nNote that R users frequently shorten the above code, which can be written:\n\n\nset.seed(123)\n# generate random numbers\nds2 <- rnorm(1000)\n\n# check whether the abbreviated simulated vector is the same as the long form vector\nall.equal( ds, ds2 )\n\n\n[1] TRUE\n\nWe used set.seed to ensure that the same random vector will be generated for our audience. We can also use set.seed to ensure that a different vector will be generated.\nHere we ise a different seed to produce a new random sample; we then check whether the new simulated sample differs from the previous random sample:\n\n\nset.seed(54321)\nds3 <- rnorm(1000)\n# dplyr::glimpse( ds3 ) # uncomment to glimpse at the data\n# better method\nidentical(ds, ds3)\n\n\n[1] FALSE\n\nWe can assess the average by-row difference:\n\n\n#check equality\nall.equal(ds , ds3)\n\n\n[1] \"Mean relative difference: 1.44105\"\n\nIndeed, these differences are large enough to detect visually\":\n\n\np2 <- ggplot2::qplot(ds3) + labs(title = \"2d random number list\")\n\n# plot two graphs, each with different random samples\np1 + p2 + \n  plot_annotation(\"Random samples\",  tag_levels = 'i')\n\n\n\n\nrunif\nWe use r uniform to generate continuous data within a point range\n\n\nset.seed(123)\n# 100 numbers between zero and 50\nds4 <- runif(n = 100, min = 0, max = 50)\ndplyr::glimpse(ds4)\n\n\n num [1:100] 14.4 39.4 20.4 44.2 47 ...\n\n# how long is the vector?\nlength(ds4)\n\n\n[1] 100\n\n# visualise how are the data distributed?\nhist(ds4, breaks = 100)\n\n\n\n\nrep\nHow can we generate random factors.\nFor this, R’s rep function, letters function, and LETTERS function make happy friends. Here’s how these functions may be combined:\nCreate lower case letters:\n\n\nletters[1:3]\n\n\n[1] \"a\" \"b\" \"c\"\n\nCreate upper case letters:\n\n\nLETTERS[4:10]\n\n\n[1] \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n\nCreate sequences using each\n\n\nrep(letters[1:3], each = 3)\n\n\n[1] \"a\" \"a\" \"a\" \"b\" \"b\" \"b\" \"c\" \"c\" \"c\"\n\nCreate sequences using times\n\n\nrep( letters[1:3], times = 3 )\n\n\n[1] \"a\" \"b\" \"c\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\"\n\nCreate uneven sequences:\n\n\nrep( letters[1:3], times = c(3, 1, 4) )\n\n\n[1] \"a\" \"a\" \"a\" \"b\" \"c\" \"c\" \"c\" \"c\"\n\nCombine each + times:\n\n\nrep(letters[1:3], each = 2, times = 3)\n\n\n [1] \"a\" \"a\" \"b\" \"b\" \"c\" \"c\" \"a\" \"a\" \"b\" \"b\" \"c\" \"c\" \"a\" \"a\" \"b\" \"b\"\n[17] \"c\" \"c\"\n\nlength.out\n\n\nrep(letters[1:3], each = 2, length.out = 17)\n\n\n [1] \"a\" \"a\" \"b\" \"b\" \"c\" \"c\" \"a\" \"a\" \"b\" \"b\" \"c\" \"c\" \"a\" \"a\" \"b\" \"b\"\n[17] \"c\"\n\nNote length.out take priority over times – use length.out if you have a fixed vector length.\nseq\nCreate a vector of numbers of a specific length:\n\n\nseq(from = 1, to = 45, by = 1)\n\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22\n[23] 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[45] 45\n\n17 unit steps:\n\n\nseq(from = 1, to = 45, by = 17)\n\n\n[1]  1 18 35\n\n17 steps:\n\n\nseq(from = 1, to = 45, length.out = 17)\n\n\n [1]  1.00  3.75  6.50  9.25 12.00 14.75 17.50 20.25 23.00 25.75 28.50\n[12] 31.25 34.00 36.75 39.50 42.25 45.00\n\nCustom sequences\nWe can use vectors within random number generation\n\n\nset.seed(123)\nvdf<-rnorm(n = 20, \n           mean = c(0, 500, 1000, 10000), \n           sd = c(5,50,100,1000))\n\n# we created a vector \n# dplyr::glimpse(vdf) # reveal to show structure\n\n# quick graph\nqplot(vdf, binwidth=4) # note the combined means + sds around them.\n\n\n\n\nWhy simulate?\nCreate noise to understand inference\nRecall the relationship between mother’s heights and daughters heights in the Pearson/Fox dataset:\n\n\n# recall this model\nexplore_md <- ggplot2::ggplot(data = md_df,\n                              aes(y = daughter_height,\n                                  x = mother_height)) +\n  geom_jitter(alpha = .2) +\n  labs(title = \"The relationship between mothers height and daughter's height\") +\n  ylab(\"Daughter's height\") +\n  xlab(\"Mother's height\") +\n  theme_classic()\n\n# plot\nexplore_md\n\n\n\n\nWhat would a random relationship look like?\nSimulation can help us to address this question.\n\n\n# average daughter height\nav_dh <- mean(md_df$daughter_height, na.rm = TRUE)\n\n# sd of daughter height\nsd_dh <- sd(md_df$daughter_height, na.rm = TRUE)\n\n# average mother height\nav_mh <- mean(md_df$mother_height, na.rm = TRUE)\n\n# sd of mother height\nsd_mh <- sd(md_df$mother_height, na.rm = TRUE)\n\n# number of obs\nN <- nrow(md_df) # 5524\n\n# fake data\n# simulate values for these parameters but do not relate the two parameters\nsim_dh = rnorm(N, av_dh, sd_dh)\nsim_mh = rnorm(N, av_mh, sd_mh)\n\n# create a datframe of the simulations\nsim_df_md <- data.frame(sim_dh, sim_mh)\n\n# graph the data\nfake_md <- ggplot2::ggplot(data = sim_df_md,\n                           aes(y = sim_dh, x = sim_mh)) +\n  geom_jitter(alpha = .2) +\n  geom_smooth(method = lm) +\n  theme_classic() +\n  labs(title = \"Fake data relationship between mothers height and daughter's height\") +\n  ylab(\"Daughter's height\") +\n  xlab(\"Mother's height\")\n\n\n# real data\nexplore_md <-ggplot2::ggplot(data = md_df, aes(y = daughter_height, x = mother_height)) + \n  geom_jitter(alpha = .2) + \n  labs(title = \"The relationship between mothers height and daughter's height\") +\n       ylab(\"Daughter's height\") +\n       xlab(\"Mother's height\") + theme_classic() + \n  geom_smooth(method = lm)\n\n# plot uncorrelated data against data plot\nlibrary(patchwork)\nfake_md + explore_md  + plot_annotation(tag_levels = \"a\")\n\n\n\n\nWhat would a postive linear relationship look like?\nSimulation can help us to address this question too.\n\n\nN <- nrow(md_df)\n# average mother height\nset.seed(123)\nmd_df$daughter_height_c = scale(md_df$daughter_height, scale = FALSE) # center but not scale\nmd_df$mother_height_c = scale(md_df$mother_height, scale = FALSE) # center but not scale\nmh_fake_c <- runif(N,\n  min = min(as.numeric(md_df$mother_height_c), na.rm=TRUE),\n  max = max(as.numeric(md_df$mother_height_c), na.rm = TRUE))\n\n# let's take the intercept as the average height of daughters\na <- rnorm(N, mean = , mean(md_df$daughter_height), sd = 5)\n# recall the beta for the model was .55,\nb <- rnorm(N,mean = .55, sd = .1)\n\n# now the outcome: this is just the linear model:\ndh_fake <-  a + b * mh_fake_c\n\nmd_fake <- data.frame(mh_fake_c, dh_fake)\n\nmod_fake <-lm(dh_fake ~ mh_fake_c, data = md_fake)\n\n# Simulated relationship \nsim_plot <- plot(\n  ggeffects::ggpredict(mod_fake,\n                       terms = c(\"mh_fake_c [all]\")),\n  add.data = TRUE,\n  dot.alpha = .2\n)  + labs(title = \"Simulated relationship\") +\n  xlab(\"simulated mother height\") +  ylab(\"simulated daughter height\")\n\n# measured relationship\nmod_real <-lm(daughter_height ~ mother_height, data = md_df)\n\n# Simulated relationship \nreal_plot <- plot(\n  ggeffects::ggpredict(mod_real,\n                       terms = c(\"mother_height [all]\")),\n  add.data = TRUE,\n  dot.alpha = .2\n)  + labs(title = \"Actual relationship\") +  ylab(\"simulated daughter height\")\n\n# graph both\nlibrary(patchwork)\nsim_plot + real_plot +\n  plot_annotation(\"Simulated Model and Data-based Model\",\n                  subtitle = \"Daughters heights predicted by mother's heights\")\n\n\n\n\nWhat do we learn from the real data that we cannot obtain from the fake data?\nHere’s a shortcut we will occasionally use to simulate a dependency\n\n\nsim_dh2 = rnorm(N, sim_mh)\n# quick plot\nplot(sim_dh2 ~ sim_mh) + title(main = \"Qucik simulation of a data dependency\")\n\n\n\ninteger(0)\n\nWe can quickly generate a negative relationship\n\n\nsim_dh3 = rnorm(N, -sim_mh)\n# quick plot\nplot(sim_dh3 ~ sim_mh) + title(main = \"Qucik simulation of a negative data dependency\")\n\n\n\ninteger(0)\n\nIncrease the standard deviation from 1 to 5\n\n\nsim_dh4 = rnorm(N, -sim_mh, sd = 5)\n# quick plot\nplot(sim_dh4 ~ sim_mh) + title(main = \"Qucik simulation of a data dependency with larger standard deviation\")\n\n\n\ninteger(0)\n\nWe can use simulation to explore potential problems with a sample\nWe might think there is a relationship when we know (owing to simulation) that there is no relationship.\nDo you see a linear relationship?\n\n\nset.seed(123)\n# no relationship between x and y\nx = rnorm(n = 10, mean = 0, sd = 1)\ny = rnorm(n = 10, mean = 0, sd = 1)\n\nbar_df <- data.frame(x,y)\n# Barely significant linear model? \nsummary(bad <-lm(y ~ x , data = bar_df))\n\n\n\nCall:\nlm(formula = y ~ x, data = bar_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.33303 -0.64421 -0.02448  0.49596  1.41472 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   0.1617     0.2852   0.567   0.5863  \nx             0.6287     0.3141   2.001   0.0803 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8988 on 8 degrees of freedom\nMultiple R-squared:  0.3336,    Adjusted R-squared:  0.2503 \nF-statistic: 4.006 on 1 and 8 DF,  p-value: 0.08034\n\nggplot2::ggplot(bar_df,aes(y,x)) + geom_point() + geom_smooth(method=lm)\n\n\n\n\nIs this a one off?\n\n\n\n\n\n\n\n\nGo Bayesian? Note that the default implies a reliable association\n\n\nbad_bayes <- brms::brm(y ~ x, data = bar_df, file = here::here(\"models\", \"bad_bayes\"))\nbayestestR::describe_posterior( bad_bayes )\n\n\n# Fixed effects\n\nParameter   | Median |        89% CI |     pd |          ROPE | % in ROPE |  Rhat |     ESS\n-------------------------------------------------------------------------------------------\n(Intercept) |   0.17 | [-0.37, 0.71] | 69.67% | [-0.10, 0.10] |    24.66% | 1.000 | 3014.46\nx           |   0.62 | [ 0.08, 1.21] | 95.70% | [-0.10, 0.10] |     0.93% | 1.001 | 2638.88\n\n# find the coefficient does not cross zero!\nplot( parameters::model_parameters( bad_bayes ) )\n\n\n\n\nWe can replicate a result many times, without relying on one seeded draw.\nThe are two steps.\nFirst we make our one off simulation into a function. We do this so that the simulation can be repeated many times.\nHere is a function:\n\n\n# function for simulating a relationship\nsimple_sim = function(mn, sd) {\n  # we will plug numbers in for 'mn' and 'sd'\n  x_out = rnorm(mn, sd) # random x\n  y_out = rnorm(mn, sd) # random y (uncorrelated)\n  dat = data.frame(x_out, y_out) # bind into a dataframe\n  lm(y_out ~ x_out, data = dat) # linear model\n}\n\n# try it out\nset.seed(123)\nm1<-simple_sim(10,1) # n = 10 \nsjPlot::tab_model(m1) # almost significant! \n\n\n\n \n\n\ny_out\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n0.53\n\n\n-0.48 – 1.55\n\n\n0.262\n\n\nx_out\n\n\n0.63\n\n\n-0.10 – 1.35\n\n\n0.080\n\n\nObservations\n\n\n10\n\n\nR2 / R2 adjusted\n\n\n0.334 / 0.250\n\n\nSecond, we use replicate to generate many outcomes from this function:\n\n\nsms = replicate(100, simple_sim(10,1), simplify = FALSE ) # make 100 examples\n# we set simplify to \"FALSE\" to recover a list\nsms[[100]] # here is the 100th outcome\n\n\n\nCall:\nlm(formula = y_out ~ x_out, data = dat)\n\nCoefficients:\n(Intercept)        x_out  \n    0.98366      0.08593  \n\nCombine purrr and broom to get the simulation\n\n\nlibrary(purrr)\nlibrary(broom)\n## all regressions\n# map(sms, coef)\n\nmap_dfr(sms, broom::tidy)\n\n\n# A tibble: 200 x 5\n   term        estimate std.error statistic   p.value\n   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)   1.46       0.190    7.71   0.0000567\n 2 x_out        -0.248      0.180   -1.38   0.206    \n 3 (Intercept)   1.10       0.394    2.78   0.0238   \n 4 x_out         0.126      0.276    0.455  0.661    \n 5 (Intercept)   0.275      0.513    0.536  0.607    \n 6 x_out         0.323      0.358    0.901  0.394    \n 7 (Intercept)   1.52       0.974    1.57   0.156    \n 8 x_out        -0.0667     0.690   -0.0967 0.925    \n 9 (Intercept)   0.736      0.330    2.23   0.0561   \n10 x_out        -0.0971     0.366   -0.266  0.797    \n# … with 190 more rows\n\nHow many p.values are less than or equalt to p = .05?\n\n\nmapped<-map_dfr(sms, broom::tidy)%>%\n  filter(term == \"x_out\") # note we only want the coefficients not the intercept\n\n# In 5 cases we find p <=.05\nsum(mapped$p.value <=.05) / length(mapped$p.value) # we find 5% of the simulations yield \"significant values\"\n\n\n[1] 0.05\n\nIs this surprising?\nWhich proportion is negative and statistically significant?\n\n\nsum((mapped$estimate < 0) &  mapped$p.value <=.05 )/ length(mapped$estimate) # we find 5% of the simulations yield \"significant values\"\n\n\n[1] 0.02\n\nAnd which proportion is positive?\n\n\nsum((mapped$estimate > 0) &  mapped$p.value <=.05 )/ length(mapped$estimate) # we find 5% of the simulations yield \"significant values\"\n\n\n[1] 0.03\n\nWhat does this simulation suggest to you about science?\nSimulate a relationship between two variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulate a non-linear relationship\n\n\n\n\n\n\n\n\n\n\n#Polynomial\nN <- 1000\n\n# simulate weights\nweight <- runif(N, min = 60, max = 120)\nweight_c <- scale(weight, scale = FALSE)\n\n# simulate coefficients\na = rnorm(N, mean = 180 , 10)\nb1 = rnorm(N, mean = 2.2, .01)\nb2 = -rnorm(N, mean = .02, .001)\n\nheight <- a + b1 * weight_c  +  b2 * weight_c ^ 2\n\n# simulated height/ weight data\n\ndf1 <- data.frame(height, weight_c, weight)\n\nplot(height ~ weight)\n\n\n\n\nLet’s fit a linear model to the data and graph the results\n\n\nm1 <- lm(height ~ weight_c, data = df1)\n\n# graph model\nplot(ggeffects::ggpredict(m1, terms = c(\"weight_c [all]\")),\n     add.data = TRUE,\n     dot.alpha = .2)  + labs(title = \"simulated linear relationship\") +\n  xlab(\"simulated weight\") +  ylab(\"simulated height\")\n\n\n\nsjPlot::tab_model(m1)\n\n\n\n \n\n\nheight\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n174.20\n\n\n173.51 – 174.89\n\n\n<0.001\n\n\nweight_c\n\n\n2.20\n\n\n2.16 – 2.25\n\n\n<0.001\n\n\nObservations\n\n\n1000\n\n\nR2 / R2 adjusted\n\n\n0.918 / 0.918\n\n\nLet’s fit a non-linear model and graph the results\n\n\nm2 <-lm(height ~ weight_c + I(weight_c^2), data = df1)\n\nplot(ggeffects::ggpredict(m2, terms = c(\"weight_c [all]\")),\n     add.data = TRUE,\n     dot.alpha = .2) + labs( title = \"simulated linear relationship\") + \n  xlab(\"simulated weight\") +  ylab(\"simulated height\")\n\n\n\n\nLet’s fit a non-linear model using splines and graph the results\n\n\nm3 <-lm(height ~ bs(weight_c), data = df1)\n\nplot(ggeffects::ggpredict(m3, terms = c(\"weight_c\")),\n     add.data = TRUE,\n     dot.alpha = .2)  + labs(title = \"simulated linear relationship\") +\n  xlab(\"simulated weight\") +  ylab(\"simulated height\")\n\n\n\n\nCheck the linear model and perform model checks:\n\n\nperformance::check_model(m1)\n\n\n\n\nThe model looks OK.\nCheck quadratic model (polynomial = 2)\n\n\nperformance::check_model(m2)\n\n\n\n\nThis model looks better\nCheck splines model.\n\n\nperformance::check_model(m3)\n\n\n\n\nThe splines model also looks better.\nWe can compare model performance:\n\n\nperformance::compare_performance(m1, m2, m3)\n\n\n# Comparison of Model Performance Indices\n\nName | Model |      AIC |      BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n------------------------------------------------------------------------\nm1   |    lm | 7668.482 | 7683.205 | 0.918 |     0.918 | 11.160 | 11.171\nm2   |    lm | 7458.284 | 7477.915 | 0.934 |     0.934 | 10.036 | 10.051\nm3   |    lm | 7458.556 | 7483.095 | 0.934 |     0.934 | 10.028 | 10.048\n\nWhich do we prefer?\nCreate fake factors\nThe key thing to remember about regression with factors is that the intercept is intercept as the lowest category of the factor.\n\n\nN <- 200 # number of observations\n#group <- rep((0:1), length.out = 200) # 2 groups\ngroup <- rep(c(\"m\",\"n_m\"), each = N/2) #equivalent:\na <- rnorm(N, 150, 3) # intercept\nb1 <- rnorm(N, 20, 1) # coefficient of \"b\nsigma = rexp(N,1)# error term\noutcome <- rnorm(N, mean = a + b1 * (group == \"m\"), sigma)\n\ndf <-data.frame(outcome,group)\n#dplyr::glimpse(df)\n\n\n#model removing the intercept to show the difference\nms<-lm(outcome ~ group, data = df)\n\n# graph\nsjPlot::plot_model(ms)\n\n\n\n#table\nsjPlot::tab_model(ms)\n\n\n\n \n\n\noutcome\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n169.98\n\n\n169.38 – 170.58\n\n\n<0.001\n\n\ngroup [n_m]\n\n\n-19.79\n\n\n-20.63 – -18.94\n\n\n<0.001\n\n\nObservations\n\n\n200\n\n\nR2 / R2 adjusted\n\n\n0.915 / 0.915\n\n\nThe key thing to remember about regression with factors is that the intercept is intercept as the lowest category of the factor.\nTo see this we can remove the intercept by including -1 in the model. This recovers contrasts:\n\n\n#model removing the intercept to show the difference\nms2<-lm(outcome ~ -1 + group, data = df)\n\n# graph\nsjPlot::plot_model(ms2)\n\n\n\n# table\nsjPlot::tab_model(ms2)\n\n\n\n \n\n\noutcome\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\ngroup [m]\n\n\n169.98\n\n\n169.38 – 170.58\n\n\n<0.001\n\n\ngroup [n_m]\n\n\n150.19\n\n\n149.60 – 150.79\n\n\n<0.001\n\n\nObservations\n\n\n200\n\n\nR2 / R2 adjusted\n\n\n1.000 / 1.000\n\n\nSimulate a relationship between two factors\nSimulate two groups with mean = 6 and mean = 12\n\n\ncdf <- data.frame(group = rep(letters[1:2], length.out = 100), # 2 groups\n                  response = rnorm(n = 100, mean = c(3, 12), sd = 1)) # mean of 3 mean of\n\nhead(cdf)\n\n\n  group  response\n1     a  1.898622\n2     b 10.131812\n3     a  2.736358\n4     b 12.508020\n5     a  3.962040\n6     b 10.471951\n\nModel:\n\n\nm_gr<- lm(response ~ -1 +  group, data = cdf)\n\n\n\nResults\n\n\nsjPlot::tab_model(m_gr)\n\n\n\n \n\n\nresponse\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\ngroup [a]\n\n\n3.06\n\n\n2.78 – 3.34\n\n\n<0.001\n\n\ngroup [b]\n\n\n11.93\n\n\n11.66 – 12.21\n\n\n<0.001\n\n\nObservations\n\n\n100\n\n\nR2 / R2 adjusted\n\n\n0.988 / 0.987\n\n\nIs imbalance in my study causing a problem?\n\n\n### Is imbalance wrecking my inference? \nset.seed(123)\nN <- 120\ncells <-rep( letters[1:2], times = c(15, 105))\n\na <- rnorm(N, 2, 1)\nb1 <- rnorm(N, .2, .1)\nsigma <- rexp(N,1)\n\nout <- rnorm(N, mean = a + b1 * (cells == \"b\"), sigma)\ndfc<-data.frame(out,cells)\nsim_cells<-lm(out ~ cells, data = dfc)\n\nsjPlot::tab_model(sim_cells)\n\n\n\n \n\n\nout\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n2.21\n\n\n1.41 – 3.02\n\n\n<0.001\n\n\ncells [b]\n\n\n-0.12\n\n\n-0.98 – 0.74\n\n\n0.779\n\n\nObservations\n\n\n120\n\n\nR2 / R2 adjusted\n\n\n0.001 / -0.008\n\n\nThis isn’t too convincing. We need to replicate the model many times\n\n\n# Make a function for the simulation\nset.seed(12)\ntest_fun = function() {\n  N <- 120\n  cells <-rep( letters[1:2], times = c(110, 10))\n  a <- rnorm(N, 2, 1)\n  b1 <- rnorm(N, 1, .1)\n  sigma <- rexp(N, 1)\n  out <- rnorm(N, mean = a + b1 * (cells == \"b\"), sigma)\n  dfc <- data.frame(out, cells)\n  sim_cells <- lm(out ~ cells, data = dfc)\n  sim_cells\n}\n\nr_lm = replicate(20, test_fun(), simplify = FALSE )\nlength(r_lm)\n\n\n[1] 20\n\nWe can use the purrrand broom packages to generate many replicates of a model and summarise them, and ask: What percentage of simulations yield statistically significant results?\n\n\ntab_sim<-purrr::map_dfr(r_lm, broom::tidy)\nmout<-tab_sim %>%\n  dplyr::filter(term ==\"cellsb\")%>%\n  dplyr::mutate_if(is.numeric, round, 5)\n\n# calculate proportion\nsum(mout$p.value <= .05) / length(mout$p.value)\n\n\n[1] 0.45\n\nDoes increasing the cells the issue?\n\n\n# Make a function for the simulation\nset.seed(12)\ntest_fun = function(cell1, cell2) {\n  N <- 120\n  cells <-rep( letters[1:2], times = c(cell1, cell2))\n  a <- rnorm(N, 2, 1)\n  b1 <- rnorm(N, 1, .1)\n  sigma <- rexp(N, 1)\n  out <- rnorm(N, mean = a + b1 * (cells == \"b\"), sigma)\n  dfc <- data.frame(out, cells)\n  sim_cells <- lm(out ~ cells, data = dfc)\n  sim_cells\n}\n\n# balanced cells of 60 each\nsim_lm = replicate(20, test_fun(60,60), simplify = FALSE )\nlength(sim_lm)\n\n\n[1] 20\n\nWe can use the purrr package to generate many replicates of a model.\nThe simulation suggests that balances of 60/60 is good here.\n\n\n# summarise coefficients \ntab_sim2<-purrr::map_dfr(sim_lm, broom::tidy)\n\n# obtain only the treatment coefficient\nmout2<-tab_sim2 %>%\n  dplyr::filter(term ==\"cellsb\")%>%\n  dplyr::mutate_if(is.numeric, round, 5)\n\nsum(mout2$p.value <= .05) / length(mout2$p.value)\n\n\n[1] 0.95\n\nAcknowledgments\nMuch in my approach to teaching simulation owes to Ariel Muldoon. PLease check out Ariel’s wonderful R webpage here: https://aosmith.rbind.io\nTry out Ariel’s tutorial for simulation functions here\n\n\n\n",
    "preview": "posts/6__1/Lecture-6_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-04-10T13:10:32+12:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/5_1/",
    "title": "Elements of a linear model",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-23",
    "categories": [],
    "contents": "\n\nContents\n\n\n\n\nShow code\nknitr::include_graphics(\"op.png\")\n\n\n\n\nShow code\n# libraries\nlibrary(\"tidyverse\")\nlibrary(\"patchwork\")\nlibrary(\"brms\")\nlibrary(\"lubridate\")\nlibrary(\"splines\")\nif (!require(equatiomatic)) {\n  remotes::install_github(\"datalorax/equatiomatic\")\n  }\n# set theme\n# theme set\ntheme_set(theme_classic())\n\n\n\nShow code\n\n# Import data\n# read data\nnz_0 <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"))\n\n# to relevel kessler 6 variables\nf<-c(\"None Of The Time\",\"A Little Of The Time\",\"Some Of The Time\",  \"Most Of The Time\", \"All Of The Time\")\n\n# get data into shape\nlibrary(\"tidyverse\")\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  dplyr::mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  dplyr::mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  dplyr::mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  dplyr::mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  dplyr::mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) %>%\n  dplyr::mutate(height_m = HLTH.Height * 100,\n         weight_kg =  HLTH.Weight) # better height vars\n\n\n\nOverview\nThis week we introduce regression.\nLearning outcomes\nBy learning regression, you will be better equipped to do psychological science and to evaluate psychological research.\nWhat is regression?\nBroadly speaking, a regression model is method for inferring the expected average features of a population, and the variance of a population, conditional on other features of the population as measured in a sample.\nWe’ll see that regression encompasses more than this definition, however, this definition makes a start.\nTo understand regression, then, we need to understand the following jargon words: population, sample, measurement, and inference.\nWhat is a population?\nIn science, a population is a hypothetical construct. It is the set of all potential members of a set of things. In psychological science that set is typically a collection of individuals. We want to understand “The population of all human beings?” or “The New Zealand adult population”; or “The population of undergraduates who may be recruited for IPRP in New Zealand.”\nWhat is a sample?\nA sample is a randomly realised sub-population from the larger abstract population that a scientific community hopes to generalise about.\nThink of selecting balls randomly from an urn. If pulled at random, the balls may inform us about the contents of the urn. For example, if we select one white ball and one black ball, we may infer that the balls in the urn are not all white or all black.\nWhat is “measurement?”\nA measure is tool or method for obtaining numerical descriptions of a sample. We often call measures “scales.” We can think of a bathroom weight scale as a tool and method for tracking body weight.\nA measurement is the numerical description we obtain from sensors such as statistical surveys, census data, twitter feeds, & etc.\nIn the course, we have encountered numerical scales, ordinal scales, and factors. The topic of measurement in psychological is, to say the least, very broad.\nFor now, it is important to keep in mind that, similar to bathroom scales, measures can be prone to error.\nAlso similar to bathroom scales, error prone scales may nevertheless be useful. We need to investigate the utility of error prone scales against the backdrop of specific interests and purposes.\nWhat is a parameter?\nIn regression, we combine measurements on samples with probability theory to guess about the properties of a population we will never observe. We call these properties “parameters.”\nWhat is statistical inference?\nThe bulk of statistical inference consists of educated guessing about population parameters.\nProbability distributions and statistical guessing\nInference is possible because the parameters of naturally occurring populations are structured by data generating processes that are approximated by probability distributions. A probability distribution is a mathematical function that describes the probability of a random event. Today we will be focusing on height.1\nToday we will be talking about the “normal” or “Gaussian distribution.” A very large number of data-generating processes in nature conform the normal distribution.\nLet’s consider some examples of randomly generated samples, which we will obtain using R’s rnorm function.\n100-person sample of heights\n\n\nShow code\nset.seed(123)\nsm<-rnorm(100, mean = 170, sd = 20)\nggplot2::qplot(sm, binwidth = 10)\n\n\n10-person sample of heights\n\n\nShow code\nset.seed(123)\nsubsm <-rnorm(10, mean = 170, sd = 20)\n\nggplot2::qplot(\n  subsm, binwidth = 10\n  )\n\n\n10000-person sample of heights\n\n\nShow code\nset.seed(123)\nlargesm <-rnorm(1e5, mean = 170, sd = 20)\nggplot2::qplot(\n  largesm, binwidth = 1\n  )\n\n\nHow can I use regression to infer a population parameter?\nWe can use R to investigate the average height of our imaginary population from which the preceding samples were randomly drawn. We do this in R by writing an “intercept-only” model as follows:\n\n\nShow code\nmodel <- lm(outcome ~ 1, data = datset)\nsummary(model)\n\nUsing the previous simulations:\nN = 10 random draws\n\n#write the model and get a nice table for it\nsjPlot::tab_model(\n  lm(sm ~ 1)\n)\n\n \n\n\nsm\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n171.81\n\n\n168.19 – 175.43\n\n\n<0.001\n\n\nObservations\n\n\n100\n\n\nR2 / R2 adjusted\n\n\n0.000 / 0.000\n\n\nN = 100 random draws\n\nsjPlot::tab_model(\n  lm(subsm ~ 1)\n)\n\n \n\n\nsubsm\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n171.49\n\n\n157.85 – 185.14\n\n\n<0.001\n\n\nObservations\n\n\n10\n\n\nR2 / R2 adjusted\n\n\n0.000 / 0.000\n\n\nN = 10,000 random draws\n\n\nShow code\nsjPlot::tab_model(\n  lm(largesm ~ 1)\n)\n\n \n\n\nlargesm\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n170.02\n\n\n169.90 – 170.14\n\n\n<0.001\n\n\nObservations\n\n\n100000\n\n\nR2 / R2 adjusted\n\n\n0.000 / 0.000\n\n\nWhat do we notice about the relationship between sample size the estimated population average?\n\n\nShow code\nsjPlot::tab_model(\n   lm(sm ~ 1),\n   lm(subsm ~ 1),\n   lm(largesm ~ 1)\n)\n\n \n\n\nsm\n\n\nsubsm\n\n\nlargesm\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\nEstimates\n\n\nCI\n\n\np\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n171.81\n\n\n168.19 – 175.43\n\n\n<0.001\n\n\n171.49\n\n\n157.85 – 185.14\n\n\n<0.001\n\n\n170.02\n\n\n169.90 – 170.14\n\n\n<0.001\n\n\nObservations\n\n\n100\n\n\n10\n\n\n100000\n\n\nR2 / R2 adjusted\n\n\n0.000 / 0.000\n\n\n0.000 / 0.000\n\n\n0.000 / 0.000\n\n\nRegression with a single co-variate\nDoes mother height predict daughter height? It seems so. By what how close are is the relationship?\nFrancis Galton is credited with inventing regression. Galton observed that the height of offspring tends to fall between parental height and the population average, what Galton termed: “regression to the mean.” Galton sought a method for educated guessing about heights, and this led to fitting a line of regression by a method called “least squares” (For a history see: here).\nThis following dataset is from “The heredity of height,” Karl Pearson and Alice Lee (1903)(Pearson and Lee 1903). I obtained the dataset from (Gelman, Hill, and Vehtari 2020). Let’s use this dataset to investigate the relationship between a mother’s height and a daughter’s height.\n\n\nShow code\nmd_df <- data.frame(read.table(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt\"), header=TRUE))\n# Center mother's height for later example\nmd_df <- md_df %>%\n  dplyr::mutate(mother_height_c = as.numeric(scale(mother_height, center = TRUE, scale = FALSE)))\ndplyr::glimpse(md_df)\nRows: 5,524\nColumns: 3\n$ daughter_height <dbl> 52.5, 52.5, 53.5, 53.5, 55.5, 55.5, 55.5, 55…\n$ mother_height   <dbl> 59.5, 59.5, 59.5, 59.5, 59.5, 59.5, 59.5, 59…\n$ mother_height_c <dbl> -2.9987328, -2.9987328, -2.9987328, -2.99873…\n\nPearson and Lee collected 5,524 observations from mother/daughter height pairs. Let’s examine the data, first by plotting the relationship.\nWhat what is happening here?\n\n\nShow code\nexplore_md <-ggplot2::ggplot(data = md_df, aes(y = daughter_height, x = mother_height)) + \n  geom_jitter(alpha = .2) + \n  labs(title = \"The relationship between mothers height and daughter's height\") +\n       ylab(\"Daughter's height\") +\n       xlab(\"Mother's height\") + theme_classic()\nexplore_md\n\n\nIs there a linear predictive relationship between these two parameters? In regression we examine the line of best fit.\n\n\nShow code\nm1 <- lm(daughter_height ~ mother_height, data = md_df)\nsjPlot::tab_model(m1)\n\n \n\n\ndaughter_height\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n29.80\n\n\n28.25 – 31.35\n\n\n<0.001\n\n\nmother_height\n\n\n0.54\n\n\n0.52 – 0.57\n\n\n<0.001\n\n\nObservations\n\n\n5524\n\n\nR2 / R2 adjusted\n\n\n0.252 / 0.252\n\n\nWe can plot the coefficient; in a model with one predictor isn’t too informative. As we continue in the course, however, we’ll see that plotting coefficients can be easier than deciphering the numbers in tables. Here are two methods for plotting.\n\n\nShow code\nt_m1<-parameters::model_parameters(m1,  \n                                   ci = 0.95)\nmethod1 <- plot(t_m1) +\n  labs(title = \"The relationship between mothers height and daughter's height\") + \n  ylab(\"Daughter's height\") \n\nmethod2 <-sjPlot::plot_model(m1)\n\nlibrary(patchwork)\nmethod1 / method2 + plot_annotation(title = \"Comparision of two coefficeint plots\",\n                                    subtitle = \"a: parameters see; b: sjPlot\", \n                                    tag_levels = \"a\")\n\n\nHow do we interpret the regression model?\nLet’s write the equation out in mathematics. How do we read this?2\n\n\nShow code\nlibrary(\"equatiomatic\")\nextract_eq(m1,  use_coefs = FALSE)\n\\[\n\\operatorname{daughter\\_height} = \\alpha + \\beta_{1}(\\operatorname{mother\\_height}) + \\epsilon\n\\]\n\nThe math says that the expected daughter’s height in a population is predicted by the average height of the population when mother’s height is set to zero units (note, this is impossible - we’ll come back to this) plus \\(\\beta ~\\times\\) units of daughter’s height (inches) for each additional unit of mother’s height (inches)\nWe can plug the output of the model directly into the equation as follows:\n\n\nShow code\nlibrary(\"equatiomatic\")\nextract_eq(m1,  use_coefs = TRUE)\n\\[\n\\operatorname{\\widehat{daughter\\_height}} = 29.8 + 0.54(\\operatorname{mother\\_height})\n\\]\n\nGraph the relationship between mother’s and daughter’s heights\n\nlibrary(ggeffects)\ntoplot<-ggeffects::ggpredict(m1, terms = \"mother_height\")\n\nheightplot<-plot(toplot, add.data = TRUE, dot.alpha = .1, jitter = TRUE) +   theme_classic()\nheightplot + labs(title = \"Predicted values of daughter's height from the Pearson/Fox 1903 dataset\")\n\n\nRegression to predict beyond the range of a dataset\nJoyte Amge is the world’s shortest woman at 25 inches. Sandy Allen was the world’s tallest woman at 91 inches. What is be the expected heights of their daughter, and of every intermediary woman in between?\n\n# use the `expand.grid` command to create a sequence of points for mother's height\nndat<-expand.grid(mother_height = c(25:91)) \n\n# use the `predict` function to create a new response \npr<- predict(m1, type = \"response\", interval = \"confidence\", newdata =ndat)\n\n# have a look at the object\ndplyr::glimpse(pr)\n num [1:67, 1:3] 43.4 44 44.5 45.1 45.6 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:67] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : chr [1:3] \"fit\" \"lwr\" \"upr\"\n# create a new dataframe for the new sequence of points for mother's height and the predicted data\nnewdata<-data.frame(ndat,pr)\nhead(newdata)\n  mother_height      fit      lwr      upr\n1            25 43.42183 42.49099 44.35266\n2            26 43.96676 43.06065 44.87288\n3            27 44.51170 43.63030 45.39310\n4            28 45.05664 44.19995 45.91332\n5            29 45.60157 44.76960 46.43355\n6            30 46.14651 45.33924 46.95378\n\nGraph the predicted results\n\n\nShow code\n# graph the expected results\npredplot<-ggplot(data = newdata, \n       aes(x= mother_height, y = fit))  + \n  geom_point() +  geom_errorbar(aes(ymin = lwr, ymax = upr), width = .1) + \n   expand_limits(x = c(20,91), y = c(0,81))  + theme_classic() + \n  labs(title = \"Predicted values for a broader population\")\n\n# plot the two graphs together (making the x and y axis at the same scale )\nlibrary(\"patchwork\")\n# rescale heightplot\n\n# old plot with the new axis and y axis scales, and remove points\n\nheightplot2<-plot(toplot, add.data = FALSE) +   theme_classic()\n\nnhp <- heightplot2 +  expand_limits(x = c(20,91), y = c(0,81) ) +  labs(title = \"Predicted values of daughter's height from the Pearson/Fox 1903 dataset\")\n\n# double graph\n nhp /predplot  + plot_annotation(title = \"What do you notie about these relationships?\", tag_levels = \"a\")\n\n\nA simple method for obtaining the predicted values form your fitted model is to obtain the ggeffects output without producing a graph.\n\nlibrary(ggeffects)\n\ntoplot<-ggeffects::ggpredict(m1, terms = \"mother_height\")\ntoplot\n# Predicted values of daughter_height\n# x = mother_height\n\n    x | Predicted |         95% CI\n----------------------------------\n52.50 |     58.41 | [58.15, 58.66]\n54.50 |     59.50 | [59.29, 59.70]\n57.50 |     61.13 | [60.99, 61.27]\n59.50 |     62.22 | [62.13, 62.32]\n61.50 |     63.31 | [63.25, 63.38]\n63.50 |     64.40 | [64.34, 64.47]\n65.50 |     65.49 | [65.40, 65.59]\n70.50 |     68.22 | [68.01, 68.42]\n\nNon-linear relationships\nLinear regression assumes linearity conditional on a model. Often your data will not be linear!\nConsider the following example:\n\n# Simulate nonlinear relationship between x and y\nb <- c(2, 0.75)\nset.seed(12)\nx <- rnorm(100)\nset.seed(12)\ny <- rnorm(100, mean = b[1] * exp(b[2] * x))\ndat1 <- data.frame(x, y)\n\not1 <-lm(y ~ x, data  = dat1)\n# performance::check_model(ot1)\n\n# Plot linear effect\nplot(ggeffects::ggpredict(ot1, terms = \"x\"), add.data =TRUE, dot.alpha = .4)\n\n\nNon-linear relationship as modelled by a polynomial regression:\n\nlibrary(splines)\not2 <-lm(y ~ x + I(x^2), data  = dat1)\nplot(ggeffects::ggpredict(ot2, terms = \"x\"), add.data =TRUE, dot.alpha = .4)\n\n\nHere is another approach:\n\nlibrary(splines)\not2.b <-lm(y ~ x + poly(x, 2), data  = dat1)\nplot(ggeffects::ggpredict(ot2.b, terms = \"x\"), add.data =TRUE, dot.alpha = .4)\n\n\nNon-linear relationship as modeled by a general additive model (spline)\n\nlibrary(splines)\n\not3 <-lm(y ~ bs(x), data  = dat1)\n\n#performance::check_model(ot2)\nplot(ggeffects::ggpredict(ot3, terms = \"x\"), add.data =TRUE, dot.alpha = .4)\n\n\nCentering\nAny linear transformation of a predictor is OK. Often we center (or center and scale) all indicators, which gives us an intercept that is meaninful (the expected population average when the other indicators are set their average).\n\n\nShow code\nlibrary(ggeffects)\n# original model\nm1 <- lm(daughter_height ~ mother_height, data = md_df)\nmc <-lm(daughter_height ~ mother_height_c, data=md_df)\nsjPlot::tab_model(m1,mc)\n\n \n\n\ndaughter_height\n\n\ndaughter_height\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n29.80\n\n\n28.25 – 31.35\n\n\n<0.001\n\n\n63.86\n\n\n63.80 – 63.92\n\n\n<0.001\n\n\nmother_height\n\n\n0.54\n\n\n0.52 – 0.57\n\n\n<0.001\n\n\n\n\n\n\n\n\nmother_height_c\n\n\n\n\n\n\n\n\n0.54\n\n\n0.52 – 0.57\n\n\n<0.001\n\n\nObservations\n\n\n5524\n\n\n5524\n\n\nR2 / R2 adjusted\n\n\n0.252 / 0.252\n\n\n0.252 / 0.252\n\n\nGraph model\n\n\nShow code\nplot(ggeffects::ggpredict(mc, terms = \"mother_height_c\"), add.data =TRUE, dot.alpha = .4)\n\n\nNote: when fitting a polynomial or any interaction, it is important to center your indicators. We’ll come back to this point in later lectures.\nModel evaluation\nA simple way to assess your model fit is to compare a model with one covariate with a simple intercept-only model and to assess improvement in either the AIC statistic or the BIC statistic. The BIC is similar to the AIC but adds a penalty for extra predictors. An absolute improvement in either statistic of n > 10 is considered to be a better model.\nWe can use the performance package to generate a table that compares fits.\n\nlibrary(performance)\n# intercept only\nionly <- lm(daughter_height ~ 1, data = md_df)\n\n# covariate added\ncovadded <- lm(daughter_height ~ mother_height, data = md_df)\n\n# evaluate\nperformance::compare_performance(ionly, covadded)\n# Comparison of Model Performance Indices\n\nName     | Model |       AIC |       BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n----------------------------------------------------------------------------\nionly    |    lm | 26299.969 | 26313.203 | 0.000 |     0.000 | 2.615 | 2.615\ncovadded |    lm | 24698.514 | 24718.365 | 0.252 |     0.252 | 2.262 | 2.262\n\nWhat was the model improvement?\n\n\nShow code\n# improved fit\nBIC(ionly)- BIC(covadded)\n[1] 1594.839\n\nGenerate a report\nThis is easy with the report package\nFor example:\n\nreport::report_statistics(covadded)\nbeta = 29.80, 95% CI [28.25, 31.35], t(5522) = 37.70, p < .001; Std. beta = 1.37e-14, 95% CI [-0.02, 0.02]\nbeta = 0.54, 95% CI [0.52, 0.57], t(5522) = 43.12, p < .001; Std. beta = 0.50, 95% CI [0.48, 0.52]\n\nOr if you want a longer report\n\nreport::report(covadded)\n\nThough use statistically significant in place of significant. This will avoid misleading your audience into thinking your result is important, when what you intend to communicate is that it is reliable.\nAssumptions of regression\nFrom Gelman and Hill (Gelman and Hill 2006)\nValidity\nLinearity*\nIndependence of errors\nEqual variance of errors\n\nunequal variance does not affect the most important aspect of a regression model, which is the form of the predictors XB (p.46)\n\nNormality of errors (statistical independence)\n\nThe regression assumption that is generally least important is that the errors are normally distributed. In fact, for the purpose of estimating the regression line (as compared to predicting individual data points), the assumption of normality is barely important at all. Thus, in contrast to many regression textbooks, we do not recommend diagnostics of the normality of re-gression residuals. (Gelman and Hill 2006) p. 46\n\n\nA good way to diagnose violations of some of the assumptions just considered (importantly, linearity) is to plot the residuals versus fitted values or simply individual predictors.(Gelman and Hill 2006)\n\nCommon confusions\nCausal inference is tricky\nPeople use the work “effect” but that is not what regression gives us (by default)\n“Normality assumption”\nAs Gelman and Hill note, the “normality” assumption is the least important. And the assumption pertains to the normality of residuals\nStatistical independence\nThis will be the main reason we do multi-level modelling: to condition on dependencies in the data.\nLevels (wrong population)\nWe sample from undergraduates, but infer about the human population.\nAcknowledgements\nRichard Mcelreath’s Statistical Rethinking (McElreath 2020)\nRegression and other stories (Gelman, Hill, and Vehtari 2020)\nAppendix 1: Conceptual Background\nSome preliminaries about science.\nScience begins with a question\nScience begins with a question about the world. The first step in science, then, is to clarify what you want to know.\nBecause science is a social practice, you will also need to clarify why your question is interesting: so what?\nIn short, know your question.\nScientific model (or theory)\nSometimes scientists are interested in specific features of the world: how did virus x originate? Such a question might have a forensic interest: what constellation of events gave rise to a novel infectious disease?\nMore typically, scientists seek generalisations. How do infectious diseases evolve? How do biological organisms evolve? Such questions have applied and fundamental interests. How can we better prevent infectious disease? How did life originate?\nA scientific model is a proposal for how nature is structured (and unstructured). For example, the theory of evolution by natural selection proposes that life emerges from variation, inheritance, and differential reproduction/survival.\nTo evaluate a scientific model, scientists must make generalisations beyond individual cases. This is where statistics shines.\nWhat is statistics?\nMathematics is a logic of certainty.\nStatistics is a logic of uncertainty.\nA statistical model uses the logic of probability to make better guesses.\nApplications of statistical models in science\nScientific models seek to explain how nature is structured. Where scientific models conflict, we can combine statistical models with data-collection to evaluate the credibility of of one theoretical model over others. To do this, a scientific model must make distinct, non-trivial predictions about the world.\nIf the predictions are not distinct, the observations will not enable a shift in credibility for one theory over another. Consider the theory that predicts any observation. Such a theory would be better classified as a conspiracy theory; it is compatible with any evidence whatsoever.\n\nToday we introduce a statistical method called regression. We will focus on how regression helps both to evaluate, and to make, informed predictions about the structures of the world.\nAppendix 2: How your computer sees your data\nUnder the hood\nUnder the hood, your computer sees your data as consisting of vectors and matrices. An algorithm searches to estimate (or in the case of Bayesian inference, to “solve”) an optimization problem to obtain a location for the unobserved parameter; in the case we examined above, this parameter is the relationship between daughter heights and mother heights.\n\\[\\begin{bmatrix}\n\\textbf{y}\\\\\n\\textit{daughter's height}\\\\\n51.5\\\\\n52.5 \\\\\n53.0 \\\\\n\\vdots\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\textbf{intercept} \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\vdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\textbf{x}\\\\\n\\textit{mother's height}\\\\\n59.5\\\\\n57.5 \\\\\n60.0 \\\\\n\\vdots\n\\end{bmatrix}\n\\begin{bmatrix}\n\\textbf{b}\\\\\n29.8 \\\\\n0.54\n\\end{bmatrix}\\]\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge university press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.\n\n\nPearson, Karl, and Alice Lee. 1903. “On the Laws of Inheritance in Man: I. Inheritance of Physical Characters.” Biometrika 2 (4): 357–462.\n\n\nThe relationship of probability distributions and data-generating processes is complex, intriguing, and both historically and philosophically rich \\(\\dots\\). Because our interests are applied, we will hardly touch up this richness in this course, alas.↩︎\nLater, we’ll prefer a different way of writing regression equations in math. (Note: writing math isn’t math - it’s just encoding the model that we’ve written).↩︎\n",
    "preview": "posts/5_1/distill-preview.png",
    "last_modified": "2021-03-30T11:10:57+13:00",
    "input_file": {},
    "preview_width": 2500,
    "preview_height": 2500
  },
  {
    "path": "posts/4_1/",
    "title": "Consolidation of skills",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-16",
    "categories": [],
    "contents": "\n\nContents\nGet data\nPreamble\nData carpentry continued\nDifferent methods for selecting columns\nRe-leveling a factor\nCreating factors from numerical indicators\nUsing ifelse to create factors\nTransformations of indicators: scaling, centering, and logs\nPro tip 1:\nCreate and work with dates a date\nCreate a timeline\nSlice\nLags and leads using timeseries data\nPro tip 2\n\nData summary\nSummarise all your data\nThe skimr package\nTable1 & other canned table packages\nCreate a table using pipe functions\nBar graphs\nMissing data graphs\nBoxplots\nCorrelation graphs\nThe report package\nMeasures\nOrder of your Method section\n\nAppendix 1A Sampling Procedure – NZAVS Time 10 (2018; conducted from 18.06.2018-28.09.2019)\nAppendix 1B Sampling Procedure – NZAVS Time 11 (2019; conducted from 29.09.2019-17.10.2020)\nAppendix 2 Johannes’s mini-lecture on the papaja package\nAppendix 3 Style advice about research methods\n\n\n\n\nGet data\n\n\n#libraries\nif (!require(skimr)) install.packages('skimr')\nif (!require(lubridate)) install.packages('lubridate')\n\nif (!requireNamespace(\"devtools\")) {\n  install.packages(\"devtools\")\n}\nif (!require(easystats)) devtools::install_github(\"easystats/easystats\")\n\n\n# Attaching packages (red = needs update)\n✔ insight     0.13.1.1   ✔ bayestestR  0.8.3.1 \n✔ performance 0.7.0.1    ✔ parameters  0.12.0.1\n✔ see         0.6.2.1    ⚠ effectsize  0.4.3.1 \n✔ correlation 0.6.0.1    ✔ modelbased  0.5.9   \n✔ report      0.2.0      \nWarnings or errors in CRAN checks for package(s) 'bayestestR', 'parameters', 'effectsize', 'correlation'.\nRestart the R-Session and update packages in red with 'easystats::easystats_update()'.\n\nif (!require(ggthemes)) install.packages('ggthemes')\nif (!require(pmdplyr)) install.packages(\"pmdplyr\")\nif (!require(kableExtra)) install.packages(\"kableExtra\")\n# this should be part of easystats but in case not:\nif (!require(report)) install.packages('report')\nif (!require(brms)) install.packages('brms')\nif (!require(lme4)) install.packages('lme4')\nif (!require(table1)) install.packages('table1')\nif (!require(modelsummary)) install.packages(\"modelsummary\")\nif (!require(naniar)) install.packages(\"naniar\")\nif (!require(ggraph)) install.packages(\"ggraph\")\nif (!require(gtsummary)) install.packages(\"gtsummary\")\n\n\n# load tidyverse\nlibrary(\"tidyverse\")\n\n# theme set\ntheme_set(theme_classic())\n\n# uncomment below and run this code\n# easystats::install_easystats_latest()\n\n\n\n\n\nnz_0 <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"))\n\n# take all characters and make them factors\n# also get rid of duplicate rows\n# Note the convention of renaming dataframe when creating a new one:\n# ` nz <-nz_0 %>%... `\n\nf<-c(\"None Of The Time\",\"A Little Of The Time\",\"Some Of The Time\",  \"Most Of The Time\", \"All Of The Time\")\n\nnz <-nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(-c(SWB.Kessler01,SWB.Kessler02,SWB.Kessler03,SWB.Kessler04,SWB.Kessler05, SWB.Kessler06))%>%\n  dplyr::mutate(Wave = as.factor(Wave))%>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless,f))%>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed,f))%>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless,f))%>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort,f))%>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless,f))%>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous,f))\n\n# not used\n# nz <- haven::zap_formats(nz)\n# nz <- haven::zap_label(nz)\n# nz <- haven::zap_widths(nz)\n# nz <- haven::zap_labels(nz)\n\n\n\nPreamble\nOne of the advantages of R is that allows us to create highly effective workflows. Today, we’ll reinforce and extend the workflow skills that you’ve started to develop in previous weeks. Below we’ll be working with the nz dataset, which is a reduced, truncated, and jittered version of waves 10 and 11 of the New Zealand Attitudes and Values Study. This dataset is for teaching only, if you’d like to learn more about the study to which it belongs, go here or here.\nData carpentry continued\nDifferent methods for selecting columns\nSuppose we want to select all variables that start with Believe. We can do this in a number of ways.\nFirst there is explicit selection:\n\n\n# explicit selection \nnz %>%\n  select(\"Believe.God\", \"Believe.Spirit\")%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ Believe.God    <fct> Not Believe God, Not Believe God, Believe God…\n$ Believe.Spirit <fct> Not Believe Spirit, Not Believe Spirit, Belie…\n\nWe can select all instances of a column that start with a certain name. For this you by using starts_with\n\n\nnz %>%\n  select(starts_with(\"Believe\"))%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ Believe.God    <fct> Not Believe God, Not Believe God, Believe God…\n$ Believe.Spirit <fct> Not Believe Spirit, Not Believe Spirit, Belie…\n\nBy the same token, we can select all instances of a variable that ends with a certain string by using ends_with\n\n\nnz %>%\n  select(ends_with(\"conditions\"))%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ NZ.Social.Conditions   <dbl> 4, 6, 3, 0, 4, 3, 4, 5, 4, 6, 5, 5, 6…\n$ NZ.Business.Conditions <dbl> 8, 6, 4, 7, 5, 3, 4, 6, 6, 6, 5, 6, 5…\n\nWe can cast a broader net and select all instances of a variable within a string by using contains\n\n\nnz %>%\n  select(contains(\"Believe\"))%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 3\n$ Religion.Believe.Cats <dbl> 4, 4, 1, 1, 1, 1, 4, NA, 3, 3, 4, 4, 2…\n$ Believe.God           <fct> Not Believe God, Not Believe God, Beli…\n$ Believe.Spirit        <fct> Not Believe Spirit, Not Believe Spirit…\n\nAs we can see, the net that we cast using contains was too broad. We don’t want the Religion.Believe.Cats.\nIn R, you can programme your way out of this corner as follows:\n\n\nnz %>%\n  select(contains(\"Believe\") &  -  Religion.Believe.Cats)%>%\n   glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ Believe.God    <fct> Not Believe God, Not Believe God, Believe God…\n$ Believe.Spirit <fct> Not Believe Spirit, Not Believe Spirit, Belie…\n\nHowever, that’s inelegant; better to drop contains altogether and revert to another method.\nRe-leveling a factor\nDeath, taxes, and factors are consequence of living. Let’s look at the BigDoms variable in the nz, which is a factor identifying large religious denominations\n\n\nnz %>%\n  dplyr::select(BigDoms)%>%\n  table(useNA =\"ifany\")\n\n\n.\n Buddhist Christian    Muslim   Not_Rel TheOthers      <NA> \n       37      1176        13      2697       136        77 \n\nNote the use of ifany to print the NAs in this table. It’s almost never sensible to ignore missing values!\nSuppose we wanted to make “Not Rel” our base category for this factor. We could do so as follows:\n\n\n## suppose we want \"Not_Rel\" as the base category, and rearrange the other levels\nlibrary(forcats) # this is part of the tidyverse package. \nnz1<-nz %>%\n  dplyr::select(BigDoms, KESSLER6sum) %>%\n  dplyr::mutate(BigDoms =  \n                  forcats::fct_relevel(BigDoms, c(\"Not_Rel\",\"Christian\",\"Buddhist\",\"Muslim\",\"TheOthers\")))\n\n#inspect data\nnz1%>%\n  group_by(BigDoms)%>%\n  count()\n\n\n# A tibble: 6 x 2\n# Groups:   BigDoms [6]\n  BigDoms       n\n  <fct>     <int>\n1 Not_Rel    2697\n2 Christian  1176\n3 Buddhist     37\n4 Muslim       13\n5 TheOthers   136\n6 <NA>         77\n\nThe reordering makes for a more sensible model because the base category is now Not_Rel or not-religious. Hence comparisons are to this category.\n\n\nm0<- glm( KESSLER6sum ~ BigDoms, data = nz1 )\nparameters::model_parameters(m0)  %>%\n  print_html(caption = \"Model of Distress by Denomination with the base category is `No Religion'\")\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#hngmbjbciu .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#hngmbjbciu .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#hngmbjbciu .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#hngmbjbciu .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#hngmbjbciu .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#hngmbjbciu .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#hngmbjbciu .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#hngmbjbciu .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#hngmbjbciu .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#hngmbjbciu .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#hngmbjbciu .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#hngmbjbciu .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#hngmbjbciu .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#hngmbjbciu .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#hngmbjbciu .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#hngmbjbciu .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#hngmbjbciu .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#hngmbjbciu .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#hngmbjbciu .gt_left {\n  text-align: left;\n}\n\n#hngmbjbciu .gt_center {\n  text-align: center;\n}\n\n#hngmbjbciu .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#hngmbjbciu .gt_font_normal {\n  font-weight: normal;\n}\n\n#hngmbjbciu .gt_font_bold {\n  font-weight: bold;\n}\n\n#hngmbjbciu .gt_font_italic {\n  font-style: italic;\n}\n\n#hngmbjbciu .gt_super {\n  font-size: 65%;\n}\n\n#hngmbjbciu .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nModel of Distress by Denomination with the base category is `No Religion'\n    \n    Parameter\n      Coefficient\n      SE\n      95% CI\n      t(4026)\n      p\n    (Intercept)\n      5.17\n      0.08\n      (5.02, 5.32)\n      67.55\n      < .001\n    BigDoms (Christian)\n      -0.25\n      0.14\n      (-0.53, 0.02)\n      -1.82\n      0.069 \n    BigDoms (Buddhist)\n      -0.32\n      0.66\n      (-1.60, 0.97)\n      -0.48\n      0.631 \n    BigDoms (Muslim)\n      -0.25\n      1.10\n      (-2.41, 1.91)\n      -0.23\n      0.820 \n    BigDoms (TheOthers)\n      0.82\n      0.35\n      (0.13, 1.51)\n      2.33\n      0.020 \n    \n\nWe can see the results better using a coefficient graph, which visualises the information presented in the table.\n\n\nplot(parameters::model_parameters(m0) ) + labs(title = \"Comparison of Religious groups to secular people\", \nsubtitle = \"Christians are a little more chilled out, \\n Other denoms are less chilled out\")\n\n\n\n\nThe base category is the comparison class. Should we infer that “The Others” denomination causes greater distress? We’ll return to this, and related questions, in the upcoming weeks. For now let’s just leave it at “probably not.”\nCreating factors from numerical indicators\nIt is almost never a good idea to transform continuous data into categorical data. However, occassionally, you will need to do so. For example, we might want to break the KESSLER6 distress indicator into its medically diagnostic components for “mild distress,” “moderate distress,” and “severe distress.” We may achieve this task using the cut function as follows:\n\n\nnz <-nz %>%\n  dplyr::mutate(k6cats = cut(\n    KESSLER6sum,\n    breaks = c(-Inf, 5, 13, Inf),   # create Kessler 6 diagnostic categories\n    labels = c(\"Low Distress\", \"Moderate Distress\", \"Serious Distress\"), \n    right = TRUE\n  ))\ntable(nz$k6cats, useNA = \"ifany\")\n\n\n\n     Low Distress Moderate Distress  Serious Distress \n             2560              1372               172 \n             <NA> \n               32 \n\nUsing ifelse to create factors\nI prefer to maintain control over how I am making the categories. For example, in the previous example, I didn’t remember whether cut includes a value to the left or to the right. I had to look this up. However, I can use ifelse function to explicitly create the relevant categories:\n\n\nnz %>%\n  dplyr::mutate(k6cats1 =  as.factor(ifelse(\n    KESSLER6sum <= 5,\n    \"Low Distress\",\n    ifelse(KESSLER6sum <= 13,  \"Moderate Distress\", \"Serious Distress\")\n  ))) %>%\n  group_by(k6cats1) %>%\n  count()\n\n\n# A tibble: 4 x 2\n# Groups:   k6cats1 [4]\n  k6cats1               n\n  <fct>             <int>\n1 Low Distress       2560\n2 Moderate Distress  1372\n3 Serious Distress    172\n4 <NA>                 32\n\n#check this is the same as the previous method\nnz %>%\n  group_by(k6cats) %>%\n  count()\n\n\n# A tibble: 4 x 2\n# Groups:   k6cats [4]\n  k6cats                n\n  <fct>             <int>\n1 Low Distress       2560\n2 Moderate Distress  1372\n3 Serious Distress    172\n4 <NA>                 32\n\nWe can see that this method returns the same values as the cut method above.\nTransformations of indicators: scaling, centering, and logs\nThroughout this course, we’ll be standardising and centering indicators. Occasionally, we’ll need to perform log transformations. You’ll need to know how to do this.\nSuppose we want to standardise the Relid indicator. This will transform the Relid indicator into standard deviation units. In later seminars, we’ll explain why this transformation is useful. For now, this is how you do it:\n\n\nnz1 <- nz %>%\n  select(Relid)%>%\n  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE))\nhead(nz1)\n\n\n# A tibble: 6 x 2\n  Relid religousid_s[,1]\n  <dbl>            <dbl>\n1     0           -0.624\n2     0           -0.624\n3     6            1.71 \n4     6            1.71 \n5     3            0.545\n6     3            0.545\n\nWhat happened? The variable name for our standardised variable looks weird: religious_s[ ,1]\nThis isn’t a worry. We use the variable as we would any other and all is fine.1\n\n\nsjPlot::tab_model(lm(religousid_s ~ 1 , data = nz1))\n\n\n\n \n\n\nreligousid_s\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n-0.00\n\n\n-0.03 – 0.03\n\n\n1.000\n\n\nObservations\n\n\n3993\n\n\nR2 / R2 adjusted\n\n\n0.000 / 0.000\n\n\nPro tip 1:\nTransform your data as the last step in your pipe workflow.\nThis is because if you filter cases, you’ll end up with a variable that isn’t measured standard deviations units\n\n\nnza <- nz %>%\n  select(Relid, BigDoms)%>%\n  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE)) \nnzb <- nz %>%\n  select(Relid, BigDoms)%>%\n  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE)) %>%\n  filter(BigDoms !=\"Not_Rel\")\n\n# compare\nsummary(nza$religousid_s)\n\n\n       V1         \n Min.   :-0.6237  \n 1st Qu.:-0.6237  \n Median :-0.6237  \n Mean   : 0.0000  \n 3rd Qu.: 0.5453  \n Max.   : 2.1040  \n NA's   :143      \n\n# with\nsummary(nzb$religousid_s)\n\n\n       V1        \n Min.   :-0.234  \n 1st Qu.: 0.935  \n Median : 1.325  \n Mean   : 1.298  \n 3rd Qu.: 2.104  \n Max.   : 2.104  \n NA's   :66      \n\nWhen we filter last, the mean value in the dataset is 1.3 – everything has changed!\n\n\nnz1 <- nz1 %>%\n  select(Relid)%>%\n  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE))\nhead(nz1)\n\n\n# A tibble: 6 x 2\n  Relid religousid_s[,1]\n  <dbl>            <dbl>\n1     0           -0.624\n2     0           -0.624\n3     6            1.71 \n4     6            1.71 \n5     3            0.545\n6     3            0.545\n\nor simply:\n\n\nnz1 <- nz1 %>%\n  select(Relid) %>%\n  mutate(religousid_s = scale(Relid))\n\nhead(nz1)\n\n\n\nTo center a variable we set scale = FALSE, center = TRUE\n\n\nnz1 <- nz %>%\n  mutate(religousid_c = scale(Relid, scale = FALSE, center  = TRUE))\n\n# inspect new indicator\nnz1%>%\n  select(Relid,religousid_c)%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ Relid        <dbl> 0, 0, 6, 6, 3, 3, NA, NA, 0, 0, 0, 0, 1, 1, 3, …\n$ religousid_c <dbl[,1]> <matrix[23 x 1]>\n\nWe use the log transformation for extreme values. We can create a new indicator by combining mutate and log as follows:\n\n\nnz1 <- nz %>%\n  mutate(charitydonate_log = log(CharityDonate + 1))\n\n# inspect new indicator\nnz1 %>%\n  select(CharityDonate,charitydonate_log)%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ CharityDonate     <dbl> 1000, 0, 500, 500, 50, 20, 20, 100, 400, 3…\n$ charitydonate_log <dbl> 6.908755, 0.000000, 6.216606, 6.216606, 3.…\n\nNote that we have to add \\[+1\\] to the log transformation, as you will recall that the log of zero is undefined. You cannot obtain zero by raising it to the power of another number.\nCreate and work with dates a date\n\n\nnz <- nz %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE)  # first data of data collection in this study\n\n\n\nWe can analyze dates, for example, for how many minutes were data collected?\n\n\nnz %>%\n  select(date)%>%\n  summary()\n\n\n      date           \n Min.   :2018-01-24  \n 1st Qu.:2018-08-08  \n Median :2019-10-03  \n Mean   :2019-05-15  \n 3rd Qu.:2019-12-05  \n Max.   :2020-10-10  \n\nint<-lubridate::interval(ymd(\"2018-01-02\"), ymd(\"2020-10-06\"))\n\n#time in years\ntime_length(int, \"year\")\n\n\n[1] 2.759563\n\n#time in minutes\ntime_length(int, \"minutes\")\n\n\n[1] 1451520\n\nFun! So much so you have some homework that will work with dates.\nCreate a timeline\nHere we’re going to graph the number of responses each day for the years of data collection.\n\n\nlibrary(lubridate)\nlibrary(ggplot2)\n\n\ndatrep <- nz %>%\n  count(day = floor_date(date, \"day\"))%>%\n  dplyr::mutate(Year = factor(ifelse(\n    day < \"2018-01-01\",\n    2017,\n    ifelse(day < \"2019-01-01\", 2018,\n           ifelse(day < \"2020-01-01\", 2019, 2020))\n  ))) %>%\n  arrange(day)\n\n# create the graph\nggplot(datrep, aes(day, n)) +\n  geom_col(aes(fill = Year)) +\n  scale_x_date(date_labels = \"%b/%Y\")  +\n  xlab(\"Days\") + ylab(\"Count of Responses\") +\n  ggtitle(\"Our Dataset's Daily Counts\")  +\n  theme_classic()  +\n  scale_fill_viridis_d()\n\n\n\n\nNote that we can use the datrep dataframe that we created to explore aspects of data collection. For example we can arrange the dataset by day in descending order of participants sampled:\n\n\ndatrep%>%\n  arrange(desc(n))\n\n\n# A tibble: 623 x 3\n   day            n Year \n   <date>     <int> <fct>\n 1 2018-06-21   121 2018 \n 2 2018-06-22   104 2018 \n 3 2018-06-20    88 2018 \n 4 2018-06-24    80 2018 \n 5 2019-12-03    67 2019 \n 6 2018-06-23    65 2018 \n 7 2018-06-25    62 2018 \n 8 2019-12-05    46 2019 \n 9 2018-10-20    45 2018 \n10 2018-06-26    44 2018 \n# … with 613 more rows\n\nTake not of that code, you might need it for your workbook.\nWhat might we do with dates? Well we might ask, were there any inherently stressful days?\nTo see this, we can take average stress levels by day, and then see where the high average stress days fall.\n\n\ntn<-nz %>%\n  select(date,KESSLER6sum,Id) %>%\n  group_by(date)%>%\n  summarise(\n   av_distress =  mean(KESSLER6sum, na.rm = TRUE),\n   n = n_distinct(Id)\n  ) %>%\n  arrange(desc(av_distress))%>%\n  glimpse()\n\n\nRows: 623\nColumns: 3\n$ date        <date> 2020-05-10, 2020-05-23, 2020-04-22, 2020-04-26,…\n$ av_distress <dbl> 19.00000, 19.00000, 17.00000, 15.00000, 15.00000…\n$ n           <int> 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 2, 3, 1, 1, 1, …\n\nGraphing the densities reveals the following\n\n\ntn%>%\n  ggplot(., aes(date, av_distress)) + \n  geom_col(aes(fill =(n))) + scale_x_date(date_labels = \"%b/%Y\")  + theme_classic() + scale_fill_viridis_c()\n\n\n\n\nClearly the “stressful days” are an artifact of days with low numbers of participant respondents.\nLet’s see whether there are any stressful days of the week. We do this by creating a weekday variable using the wday function in the lubridate package. Let’s graph our results using a pipe %>% workflow:\n\n\nnz %>%\n  select(Id, date, KESSLER6sum) %>%\n  mutate(weekdays = wday(date, label = TRUE)) %>%\n  group_by(weekdays) %>%\n  summarise(\n    mn_k6 =  mean(KESSLER6sum, na.rm = TRUE),\n    sd_k6 =  sd(KESSLER6sum, na.rm = TRUE),\n    n_k6w = n()\n  ) %>%\n  mutate(\n    se_k6 = sd_k6 / sqrt(n_k6w),\n    lw_ci = mn_k6 - qt(1 - (0.05 / 2), n_k6w - 1) * se_k6,\n    up_ci = mn_k6 + qt(1 - (0.05 / 2), n_k6w - 1) * se_k6\n  ) %>%\n  ggplot(., aes(x = weekdays, y = mn_k6, colour = mn_k6)) +\n  geom_errorbar(aes(ymin = lw_ci, ymax = up_ci), width = .1) +\n  geom_point(size = 3)  +\n  scale_y_continuous(limits = c(0,7)) + \n  theme_classic() + scale_fill_viridis_d()\n\n\n\n\nDespite the variability over the two years of data collection, the bars of the graph overlap: we don’t find differences in distress by days.\n“Ok Boomer,” you ask, “what if we were to calculate distress by generational cohorts?”\nMy reply, I’m not a boomer, I’m a GenX-er. I’m keen to check it out:\n\n\nnz$hour\n\n\nNULL\n\nnz %>%\n  select(GenCohort, KESSLER6sum) %>%\n  group_by(GenCohort) %>%\n  summarise(\n    mn_k6 =  mean(KESSLER6sum, na.rm = TRUE),\n    sd_k6 =  sd(KESSLER6sum, na.rm = TRUE),\n    n_k6w = n()\n  ) %>%\n  mutate(\n    se_k6 = sd_k6 / sqrt(n_k6w),\n    lw_ci = mn_k6 - qt(1 - (0.05 / 2), n_k6w - 1) * se_k6,\n    up_ci = mn_k6 + qt(1 - (0.05 / 2), n_k6w - 1) * se_k6\n  ) %>%\n  ggplot(., aes(x = GenCohort, y = mn_k6, colour = GenCohort)) +\n  geom_errorbar(aes(ymin = lw_ci, ymax = up_ci), width = .1) +\n  geom_point(size = 3)  +\n  scale_y_continuous(limits = c(0, 7)) +\n  theme_classic() +\n  geom_hline(yintercept = 5,\n             colour = \"red\",\n             linetype = \"dashed\") +\n  scale_y_continuous(limits = c(0, 10)) +\n  theme(\n    legend.text = element_text(size = 6),\n    legend.title = element_text(size = 8),\n    axis.text.x = element_blank()\n  ) +\n  xlab(\"Birth Generation Cohort\") +\n  ylab(\"Kessler 6 Distress\") +\n  labs(title = \"Average Distress by Birth Cohort\",\n       subtitle = \"Red line indicates clinically moderate distress threshold\") +\n  scale_colour_viridis_d() \n\n\n\n\nLater, we’ll ask why you’re so stressed out.\nSlice\nDplyr’s slice function can be handy. Say we only want the first four rows\n\n\ndatrep%>%\n  arrange(desc(n)) %>%\n  slice(1:4)\n\n\n# A tibble: 4 x 3\n  day            n Year \n  <date>     <int> <fct>\n1 2018-06-21   121 2018 \n2 2018-06-22   104 2018 \n3 2018-06-20    88 2018 \n4 2018-06-24    80 2018 \n\nSay we only want the 1st row, the 3rd row, and the 20th row\n\n\ndatrep%>%\n  dplyr::arrange(desc(n)) %>%\n  dplyr::slice(c(1,3,20))\n\n\n# A tibble: 3 x 3\n  day            n Year \n  <date>     <int> <fct>\n1 2018-06-21   121 2018 \n2 2018-06-20    88 2018 \n3 2019-12-12    36 2019 \n\nLags and leads using timeseries data\nCreate a difference variable for change in Kessler 6\n\n\nlibrary(\"pmdplyr\")\ndf <-nz %>%\n  dplyr::filter(!is.na(KESSLER6sum))%>%\n  mutate(wave = as.numeric(Wave))%>%\n   mutate(lag_k6 = tlag(KESSLER6sum,\n    .i = Id, # id variable\n    .t = wave # time series variable, needs to be numeric\n  ))%>%\n  mutate(diff_k6 = lag_k6 - KESSLER6sum) %>%\n  select(Id,Wave,KESSLER6sum,diff_k6,Emp.JobSecure,Employed)%>%\n  arrange(desc(diff_k6)) \n\n\n\nWhat to do with this new variable. Well, we might explore whether employment security relates to distress change:\n\n\ndf %>%\n  filter(Wave == 2019) %>%\n  mutate(employed_employsecurity = as.factor(ifelse(Employed ==1, Emp.JobSecure,0)))%>%\n  ggplot(data = ., aes(x = diff_k6, fill = employed_employsecurity) )+\n   geom_histogram() + \n  xlab(\"Difference in K6 eleveation (cases above 5)\") + \n  ylab(\"Counts of cases\") + \n  labs(subtitle =\"No clear relationship between unemployment insecurity and distress change\")+\n  scale_fill_discrete(name=\"Employment Security 1-7\") + \n  scale_fill_viridis_d() + theme_classic() + \n  theme(legend.position = \"bottom\")\n\n\n\n\nAnd remarkably we don’t see much evidence in the cross-sectional analysis.\n\n\n# create data frame with new variable Zero is for the unemeployed. \ndfnew <- df %>%\n  filter(Wave == 2019) %>%\n  mutate(employed_employsecurity = as.numeric(ifelse(Employed == 1, Emp.JobSecure, 0)))%>%\nfilter(!is.na(employed_employsecurity))\n\nhead(dfnew)\n\n\n# A tibble: 6 x 7\n     Id Wave  KESSLER6sum diff_k6 Emp.JobSecure Employed\n  <dbl> <fct>       <dbl>   <dbl>         <dbl>    <dbl>\n1  1713 2019            1      20             4        1\n2   564 2019            4      13            NA        0\n3  1001 2019            0      12             4        1\n4   517 2019            4      11            NA        0\n5   610 2019           14       9             7        1\n6   922 2019            5       9             7        1\n# … with 1 more variable: employed_employsecurity <dbl>\n\n# Graph\nggplot(dfnew, aes(y = diff_k6, employed_employsecurity)) +\n  geom_jitter(alpha = .2) +\n  geom_smooth(method = lm) +\n  xlab(\"employed_employsecurity\") +\n  ylab(\"Kessler 6 distress jumps over 5\") +\n  ggtitle(\"Jumps in distress change not related to employement insecurity\") +\n  scale_fill_viridis_d() + theme_classic()\n\n\n\n\nHowever, perhaps our indicator is misleading us. We can formally model the relationship between employment security and Kessler6 distress across two years\n\n\n# create dataframe with the variables we need\ndfnew2 <- df %>%\n  mutate(employed_employsecurity = as.numeric(ifelse(Employed == 1, Emp.JobSecure, 0))) %>%\n  filter(!is.na(employed_employsecurity)) %>%\n  dplyr::mutate(employsecurity_s = scale(employed_employsecurity))\n\n# multi-level model \n\nm00a<-lme4::lmer(KESSLER6sum ~  employsecurity_s * Wave + (1|Id), data = dfnew2)\nplot(ggeffects::ggpredict(m00a, terms=c(\"employsecurity_s\", \"Wave\")),\n     add.data = TRUE, jitter = 0.2, dot.alpha =.05) + geom_hline(yintercept = 5,\n             colour = \"red\",\n             linetype = \"dashed\") + \n  labs(title = \"There is a relationship between employment security and Kessler6 distress\")\n\n\n\n\nThis suggests a stable negative relationship between employment security and (low) distress. So is there are causal relationship? Not necessarily. Again, we return to casual inference in the upcoming weeks. For now, we want to alert you to an important lesson:\nPro tip 2\nDo not read too much into your descriptive analysis!\nThis is especially true when creating new variables. Just because you can make a variable doesn’t mean you should use it, or interpret it!\nPut differently, our workflow will require much more than descriptive statistics.\nData summary\nSummarise all your data\nThe skimr package\nThe skimmer package can be helpful in detecting problems. A drawback note that it is interpreting all factors as numbers).\nFor example. ( I won’t run the following code, you will do so for your homework).\n\n\nlibrary(\"skimr\")\nnz %>%\n  select(-date) %>% #not useful\n  dplyr::group_by(Wave) %>%\n  skim()\n\n\n\nHowever, I want to point out that skimr works with individual columns, and it accepts a tidy workflow.\n\n\nnz %>%\n  dplyr::group_by(Wave) %>%\n  select(KESSLER6sum,HLTH.SleepHours)  %>%\n  skim() \n\n\n\nTable 1: Data summary\n\n\n\n\n\n\nName\n\n\nPiped data\n\n\nNumber of rows\n\n\n4136\n\n\nNumber of columns\n\n\n3\n\n\n_______________________\n\n\n\n\nColumn type frequency:\n\n\n\n\nnumeric\n\n\n2\n\n\n________________________\n\n\n\n\nGroup variables\n\n\nWave\n\nVariable type: numeric\n\nskim_variable\n\n\nWave\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\nKESSLER6sum\n\n\n2018\n\n\n15\n\n\n0.99\n\n\n5.07\n\n\n3.93\n\n\n0\n\n\n2\n\n\n4\n\n\n7\n\n\n23\n\n\n▇▅▂▁▁\n\n\nKESSLER6sum\n\n\n2019\n\n\n17\n\n\n0.99\n\n\n5.20\n\n\n4.01\n\n\n0\n\n\n2\n\n\n4\n\n\n7\n\n\n22\n\n\n▇▅▂▁▁\n\n\nHLTH.SleepHours\n\n\n2018\n\n\n97\n\n\n0.95\n\n\n6.93\n\n\n1.13\n\n\n3\n\n\n6\n\n\n7\n\n\n8\n\n\n12\n\n\n▁▅▇▁▁\n\n\nHLTH.SleepHours\n\n\n2019\n\n\n69\n\n\n0.97\n\n\n6.96\n\n\n1.14\n\n\n2\n\n\n6\n\n\n7\n\n\n8\n\n\n12\n\n\n▁▃▇▁▁\n\n\nTable1 & other canned table packages\nIn earlier seminars, we encountered the table1 package, which makes really great html tables:\n\n\nlibrary(table1)\n\ntable1::table1(~Age  +\n                 GenCohort +\n                 Male + \n                 Edu +\n                 Pol.Orient + \n                 Relid + \n                 BigDoms   | Wave, data = nz,\n               overall = FALSE)\n\n\n\n2018(N=2068)\n2019(N=2068)\nAge\n\n\nMean (SD)\n50.1 (13.5)\n51.8 (13.4)\nMedian [Min, Max]\n52.0 [18.0, 90.0]\n54.1 [19.6, 91.1]\nGenCohort\n\n\nGen Boombers: born >= 1946 & b.< 1961\n693 (33.5%)\n693 (33.5%)\nGen_Silent: born< 1946\n50 (2.4%)\n50 (2.4%)\nGenX: born >=1961 & b.< 1980\n886 (42.8%)\n886 (42.8%)\nGenY: born >=1980 & b.< 1996\n387 (18.7%)\n387 (18.7%)\nGenZ: born >= 1996\n52 (2.5%)\n52 (2.5%)\nMale\n\n\nMale\n772 (37.3%)\n772 (37.3%)\nNot_Male\n1290 (62.4%)\n1289 (62.3%)\nMissing\n6 (0.3%)\n7 (0.3%)\nEdu\n\n\nMean (SD)\n5.39 (2.78)\n5.64 (2.72)\nMedian [Min, Max]\n6.00 [0, 10.0]\n7.00 [0, 10.0]\nMissing\n80 (3.9%)\n65 (3.1%)\nPol.Orient\n\n\nMean (SD)\n3.53 (1.39)\n3.57 (1.37)\nMedian [Min, Max]\n4.00 [1.00, 7.00]\n4.00 [1.00, 7.00]\nMissing\n116 (5.6%)\n63 (3.0%)\nRelid\n\n\nMean (SD)\n1.67 (2.57)\n1.53 (2.56)\nMedian [Min, Max]\n0 [0, 7.00]\n0 [0, 7.00]\nMissing\n44 (2.1%)\n99 (4.8%)\nBigDoms\n\n\nBuddhist\n18 (0.9%)\n19 (0.9%)\nChristian\n619 (29.9%)\n557 (26.9%)\nMuslim\n7 (0.3%)\n6 (0.3%)\nNot_Rel\n1322 (63.9%)\n1375 (66.5%)\nTheOthers\n70 (3.4%)\n66 (3.2%)\nMissing\n32 (1.5%)\n45 (2.2%)\n\n\nUnfortunately, the table1 package only prints html tables.\nFor publications, I might use the modelsummary package\n\n\nlibrary(\"modelsummary\")\nnnz<-nz %>%\n  dplyr::select(Age, \n                Male,\n                BigDoms,\n                Edu,\n                GenCohort,\n                Relid,\n                Pol.Orient,\n                Wave)\nmodelsummary::datasummary_balance( ~ Wave, data=nnz, dinm=FALSE, output = 'table.tex')\n\n\n\nI’ll put the \\(LaTeX\\) output into my document because I generally prefer to write in \\(LaTeX\\)\nHowever if you want to print inline, you can simply use:\n\n\nlibrary(\"modelsummary\")\nnnz<-nz %>%\n  dplyr::select(Age, \n                Male,\n                BigDoms,\n                Edu,\n                GenCohort,\n                Relid,\n                Pol.Orient,\n                Wave)\nmodelsummary::datasummary_balance( ~ Wave, data=nnz, dinm=FALSE)\n\n\n\n\n\n\n2018 (N=2068)\n\n\n\n\n2019 (N=2068)\n\n\n\n\n\n\n\nMean\n\n\nStd. Dev.\n\n\nMean\n\n\nStd. Dev.\n\n\nAge\n\n\n\n\n50.1\n\n\n13.5\n\n\n51.8\n\n\n13.4\n\n\nEdu\n\n\n\n\n5.4\n\n\n2.8\n\n\n5.6\n\n\n2.7\n\n\nRelid\n\n\n\n\n1.7\n\n\n2.6\n\n\n1.5\n\n\n2.6\n\n\nPol.Orient\n\n\n\n\n3.5\n\n\n1.4\n\n\n3.6\n\n\n1.4\n\n\n\n\n\n\nN\n\n\n%\n\n\nN\n\n\n%\n\n\nMale\n\n\nMale\n\n\n772\n\n\n37.3\n\n\n772\n\n\n37.3\n\n\n\n\nNot_Male\n\n\n1290\n\n\n62.4\n\n\n1289\n\n\n62.3\n\n\nBigDoms\n\n\nBuddhist\n\n\n18\n\n\n0.9\n\n\n19\n\n\n0.9\n\n\n\n\nChristian\n\n\n619\n\n\n29.9\n\n\n557\n\n\n26.9\n\n\n\n\nMuslim\n\n\n7\n\n\n0.3\n\n\n6\n\n\n0.3\n\n\n\n\nNot_Rel\n\n\n1322\n\n\n63.9\n\n\n1375\n\n\n66.5\n\n\n\n\nTheOthers\n\n\n70\n\n\n3.4\n\n\n66\n\n\n3.2\n\n\nGenCohort\n\n\nGen Boombers: born >= 1946 & b.< 1961\n\n\n693\n\n\n33.5\n\n\n693\n\n\n33.5\n\n\n\n\nGen_Silent: born< 1946\n\n\n50\n\n\n2.4\n\n\n50\n\n\n2.4\n\n\n\n\nGenX: born >=1961 & b.< 1980\n\n\n886\n\n\n42.8\n\n\n886\n\n\n42.8\n\n\n\n\nGenY: born >=1980 & b.< 1996\n\n\n387\n\n\n18.7\n\n\n387\n\n\n18.7\n\n\n\n\nGenZ: born >= 1996\n\n\n52\n\n\n2.5\n\n\n52\n\n\n2.5\n\n\nCreate a table using pipe functions\nAbove we saw how to create a clunky table using table(x). However, R has lots of functionality to enable better.\n\n\nlibrary(kableExtra)\nnz %>%\n  select(k6cats, Wave) %>%\n  filter(!is.na(k6cats))%>%\n  group_by( Wave, k6cats) %>%\n  summarise(n = n())%>%\n  kbl(caption = \"Distress by Year\") %>%\n   kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)%>%\n  collapse_rows()\n\n\n\nTable 2: Distress by Year\n\n\nWave\n\n\nk6cats\n\n\nn\n\n\n2018\n\n\nLow Distress\n\n\n1296\n\n\nModerate Distress\n\n\n673\n\n\nSerious Distress\n\n\n84\n\n\n2019\n\n\nLow Distress\n\n\n1264\n\n\nModerate Distress\n\n\n699\n\n\nSerious Distress\n\n\n88\n\n\nNote that we can use the pivot_wider function to spread the dataframe to enable a table that is easier to interpret.\nCredit where credit is due: I just learned about pivot_wider from Johannes and Thorven. I’m keen to get pivot_longer and pivot_wider into my vocabulary, and to do more things, like this:\n\n\nnz %>%\n  select(k6cats, Wave) %>%\n  filter(!is.na(k6cats))%>%\n  group_by( Wave, k6cats) %>%\n  summarise(n = n())%>%\n pivot_wider(names_from = Wave, values_from = n) %>%\n   kbl(caption = \"Distress counts by year\") %>%\n   kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\n\n\n\nTable 3: Distress counts by year\n\n\nk6cats\n\n\n2018\n\n\n2019\n\n\nLow Distress\n\n\n1296\n\n\n1264\n\n\nModerate Distress\n\n\n673\n\n\n699\n\n\nSerious Distress\n\n\n84\n\n\n88\n\n\nNice!\nBar graphs\nFor categorical data, in place of tables we can use bar graphs\nHere’s the table:\n\n\ntable(nz$BigDoms)\n\n\n\n Buddhist Christian    Muslim   Not_Rel TheOthers \n       37      1176        13      2697       136 \n\nHere’s the bar graph:\n\n\nggplot(nz) + \n  geom_bar(mapping = aes(x = BigDoms))\n\n\n\n\nNote that we can re-order the factor levels to produce a nicer output, using fct_infreq\n\n\nggplot(nz) + \n  geom_bar(mapping = aes(x = fct_infreq(BigDoms))  )\n\n\n\n\nMissing data graphs\nWhat do you notice about the patterns of missingness in this graph?\n\n\nlibrary(\"naniar\")\nnaniar::vis_miss(nz) #What do you notice? \n\n\n\n\nHere, we find all the problem cases:\n\n\ngg_miss_upset(nz)\n\n\n\n\nWhat explains these feature of missingess?\nBoxplots\nA box plot provides visual information for the following statistics:\nMinimum – (0p) min outlier\nMaximum – (100p) max outlier\nMedian – (50th p)\nFirst Quartile (Q1 or 25p)\nThird Quartile (Q3 or 75p)\nInterquartile range (IQR), whcih is the distance between Q1 and Q3\nOptional: the notch displays a confidence interval around the median. This is +/- 1.58 X IQR/sqrt(n). We use notches to compare differences between groups; overlap implies uncertainty about whether the medians differ.\nThere’s a simple explanation here\nWe can use base R to investigate differences in distress among big denominations:\n\n\n# using base R\nboxplot(KESSLER6sum ~ BigDoms, data = nz, notch = TRUE, col = c(\"cadetblue1\",\"orange\",\"red\",\"darkblue\",\"brown\"))\n\n\n\n\nHere’s a ggplot boxplot:\n\n\nggplot(data = nz, aes(x = KESSLER6sum, y = BigDoms, fill = BigDoms)) + \n  geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + \n  ggtitle(\"If the notches don't overlap, there's likely a difference\") + \n  geom_jitter(alpha = .05)\n\n\n\n\nHere’s a ggplot2 boxplot with points overlaid, and jittered. This allows us to se the differences in sample sizes\n\n\nggplot(data = nz, aes(x = KESSLER6sum, y = BigDoms, fill = BigDoms)) + \n  geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + \n  ggtitle(\"If the notches don't overlap, there's likely a difference\") + \n  geom_jitter(alpha = .07)\n\n\n\n\nWe could look at differences by wave:\n\n\nggplot(data = nz, aes(x = KESSLER6sum, y = BigDoms, fill = BigDoms)) + \n  geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + \n   geom_jitter(alpha = .07) + \n  facet_grid(Wave ~ .) +\n  ggtitle(\"If the notches don't overlap, there's likely a difference\") \n\n\n\n\nCorrelation graphs\nJohannes will describe a method for making a correlation plot. Here is another method.\n\n\nbzsec<-nz%>%\n  select(\n    Your.Future.Security,\n    Standard.Living,\n    NZ.Economic.Situation,\n    NZ.Business.Conditions,\n    Emp.JobSecure,\n    CharityDonate,\n    Id\n  ) %>%\n  mutate_all(., as.numeric) %>% #make numeric \n  mutate(Id = as.factor(Id),\n         CharityDonate = log(CharityDonate + 1))# make Id a factor for the \n\n# make a correlation plot using the \"correlation\" package from easystates\n\nlibrary(correlation)\np1<-bzsec %>%\n  correlation(partial = FALSE, multilevel = TRUE ) %>%\n  plot()\n\n\n\nPrint summary\n\n\nbzsec %>%\n  correlation(partial = FALSE, multilevel = TRUE ) %>%\n  summary()\n\n\nParameter              | CharityDonate | Emp.JobSecure | NZ.Business.Conditions | NZ.Economic.Situation | Standard.Living\n-------------------------------------------------------------------------------------------------------------------------\nYour.Future.Security   |       0.10*** |       0.22*** |                0.47*** |               0.32*** |         0.38***\nStandard.Living        |       0.09*** |       0.16*** |                0.26*** |               0.30*** |                \nNZ.Economic.Situation  |       0.07*** |       0.10*** |                0.43*** |                       |                \nNZ.Business.Conditions |          0.04 |       0.13*** |                        |                       |                \nEmp.JobSecure          |          0.03 |               |                        |                       |                \n\nLet’s set multilevel to FALSE.\n\n\nlibrary(correlation)\np2<-bzsec %>%\n  select(-Id)%>% # get rid of grouping variable\n  correlation(partial = FALSE, multilevel = FALSE ) %>%\n  plot()\n#print summary\nbzsec %>%\n  select(-Id)%>% # get rid of grouping variable\n  correlation(partial = FALSE, multilevel = FALSE ) %>%\n  summary()\n\n\n# Correlation Matrix (pearson-method)\n\nParameter              | CharityDonate | Emp.JobSecure | NZ.Business.Conditions | NZ.Economic.Situation | Standard.Living\n-------------------------------------------------------------------------------------------------------------------------\nYour.Future.Security   |       0.22*** |       0.31*** |                0.53*** |               0.42*** |         0.53***\nStandard.Living        |       0.25*** |       0.26*** |                0.32*** |               0.36*** |                \nNZ.Economic.Situation  |       0.12*** |       0.14*** |                0.52*** |                       |                \nNZ.Business.Conditions |       0.10*** |       0.20*** |                        |                       |                \nEmp.JobSecure          |       0.10*** |               |                        |                       |                \n\np-value adjustment method: Holm (1979)\n\n\n\nlibrary(patchwork)\n# create a two panel plot\np1 / p2 + \n  plot_annotation(title = \"Plot of multilevel (a) and single-level (b) correlation\", tag_levels = 'a')\n\n\n\n\nWe can see an even greater correlations between the variables. This is because the model does not adjust for the repeated measures, which create dependencies in the data.\nThe report package\nThe reports package from the easystats group is powerful tool for saving tame. Before extolling its virtues, I’d like to point out two major limitations.\nFirst, the package is in development. Currently, it has lots of bugs.\nSecond, the package uses terminology that won’t work for all contexts and purposes. For example, it uses the term “significant” to describe p values that are below the traditional p = .05 threshold.\nIf you learn nothing else from this course, you should learn never to use “significant” to describe a p value. You may, if you like, use “statistically signficant” however it would be better altogether if you simply dropped p-values from data analysis. We’ll show you how. With those provisos in mind, consider some useful functionality from the report package.\n\n\n# create a demographic dataframe\nnz_demagraphics <- nz %>%\n  select(Age, GenCohort, Male, Edu, Pol.Orient, Relid, BigDoms, Wave)\n\n# now a nice way to save you time when reporting\npaste(\n  report::report_participants(\n    nz_demagraphics, \n    group = \"Wave\", \n    age = \"Age\",\n    sex = \"Male\",\n    education = \"Edu\",\n    spell_n = TRUE),\n  \"were recruited in the study by through enticement by lollipops. Those who did not volunteer were coerced.\"\n  )\n\n\n[1] \"For the 'Wave - 2018' group: Two Thousand, Sixty Eight participants (Mean age = 50.1, SD = 13.5, range: [18, 90]; 0.0% females; Mean education = 5.4, SD = 2.8, range: [0, 10]) and for the 'Wave - 2019' group: Two Thousand, Sixty Eight participants (Mean age = 51.8, SD = 13.4, range: [19.6, 91.08]; 0.0% females; Mean education = 5.6, SD = 2.7, range: [0, 10]) were recruited in the study by through enticement by lollipops. Those who did not volunteer were coerced.\"\n\nThe table function of report isn’t great yet. However it has some nice features. For example you should always report your session information, and doing so in tabluar form clarifies the elements\nTry running the following code on your own:\n\n\nr <- report_table(sessionInfo())\nr\n\n\n\nHere is another method, which you can try on your own\n\n\ncite_packages()\n\n\n\nHere’s a demographic table (try on your own)\n\n\nreport_table(nz_demagraphics)\n\n\n\nHere’s a data summary\n\n\nlibrary(\"report\")\nnz %>%\n  group_by(Wave)%>%\n  select(\n    \"Wave\", \n    \"Age\",\n    \"Male\",\n    \"Edu\",\n    \"Relid\",\n    \"Pol.Orient\",\n    \"KESSLER6sum\",\n    \"FeelHopeless\",\n    \"FeelDepressed\",\n    \"FeelRestless\",\n    \"EverythingIsEffort\",\n    \"FeelWorthless\",\n    \"FeelNervous\"\n    )%>%\n  report() %>% \n  summary()\n\n\nThe data contains 4136 observations, grouped by Wave, of the following variables:\n\n- 2018 (n = 2068):\n  - Age: Mean = 50.07, SD = 13.49, range: [18, 90]\n  - Male: 2 levels, namely Male (n = 772), Not_Male (n = 1290) and missing (n = 6)\n  - Edu: Mean = 5.39, SD = 2.78, range: [0, 10], 3.87% missing\n  - Relid: Mean = 1.67, SD = 2.57, range: [0, 7], 2.13% missing\n  - Pol.Orient: Mean = 3.53, SD = 1.39, range: [1, 7], 5.61% missing\n  - KESSLER6sum: Mean = 5.07, SD = 3.93, range: [0, 23], 0.73% missing\n  - FeelHopeless: 5 levels, namely None Of The Time (n = 1046), A Little Of The Time (n = 600), Some Of The Time (n = 338), Most Of The Time (n = 54), All Of The Time (n = 4) and missing (n = 26)\n  - FeelDepressed: 5 levels, namely None Of The Time (n = 1469), A Little Of The Time (n = 361), Some Of The Time (n = 170), Most Of The Time (n = 43), All Of The Time (n = 5) and missing (n = 20)\n  - FeelRestless: 5 levels, namely None Of The Time (n = 535), A Little Of The Time (n = 763), Some Of The Time (n = 594), Most Of The Time (n = 138), All Of The Time (n = 16) and missing (n = 22)\n  - EverythingIsEffort: 5 levels, namely None Of The Time (n = 551), A Little Of The Time (n = 795), Some Of The Time (n = 501), Most Of The Time (n = 158), All Of The Time (n = 40) and missing (n = 23)\n  - FeelWorthless: 5 levels, namely None Of The Time (n = 1433), A Little Of The Time (n = 356), Some Of The Time (n = 186), Most Of The Time (n = 67), All Of The Time (n = 7) and missing (n = 19)\n  - FeelNervous: 5 levels, namely None Of The Time (n = 620), A Little Of The Time (n = 789), Some Of The Time (n = 482), Most Of The Time (n = 146), All Of The Time (n = 13) and missing (n = 18)\n\n- 2019 (n = 2068):\n  - Age: Mean = 51.83, SD = 13.45, range: [19.55, 91.08]\n  - Male: 2 levels, namely Male (n = 772), Not_Male (n = 1289) and missing (n = 7)\n  - Edu: Mean = 5.64, SD = 2.72, range: [0, 10], 3.14% missing\n  - Relid: Mean = 1.53, SD = 2.56, range: [0, 7], 4.79% missing\n  - Pol.Orient: Mean = 3.57, SD = 1.37, range: [1, 7], 3.05% missing\n  - KESSLER6sum: Mean = 5.20, SD = 4.01, range: [0, 22], 0.82% missing\n  - FeelHopeless: 5 levels, namely None Of The Time (n = 977), A Little Of The Time (n = 639), Some Of The Time (n = 349), Most Of The Time (n = 79), All Of The Time (n = 4) and missing (n = 20)\n  - FeelDepressed: 5 levels, namely None Of The Time (n = 1434), A Little Of The Time (n = 377), Some Of The Time (n = 193), Most Of The Time (n = 35), All Of The Time (n = 5) and missing (n = 24)\n  - FeelRestless: 5 levels, namely None Of The Time (n = 517), A Little Of The Time (n = 757), Some Of The Time (n = 608), Most Of The Time (n = 146), All Of The Time (n = 15) and missing (n = 25)\n  - EverythingIsEffort: 5 levels, namely None Of The Time (n = 544), A Little Of The Time (n = 804), Some Of The Time (n = 502), Most Of The Time (n = 156), All Of The Time (n = 35) and missing (n = 27)\n  - FeelWorthless: 5 levels, namely None Of The Time (n = 1407), A Little Of The Time (n = 375), Some Of The Time (n = 195), Most Of The Time (n = 60), All Of The Time (n = 9) and missing (n = 22)\n  - FeelNervous: 5 levels, namely None Of The Time (n = 568), A Little Of The Time (n = 850), Some Of The Time (n = 468), Most Of The Time (n = 133), All Of The Time (n = 24) and missing (n = 25)\n\nNotes:\nMore about the report package: here\nThis package is brought to you by easystats\nMeasures\nWhen reporting your study, it is extremely important to include information about your measure. For example:\nWe measure psychological distress using the Kessler-6 scale (R. C. Kessler et al. 2002), which exhibits strong diagnostic concordance for moderate and severe psychological distress in large, cross-cultural samples Prochaska et al. (2012). Participants rated during the past 30 days, how often did… (a) “\\(\\dots\\) you feel hopeless”; (b) “\\(\\dots\\) you feel so depressed that nothing could cheer you up”; (c) \\(\\dots\\) you feel restless or fidgety”; (d)“\\(\\dots\\) you feel that everything was an effort”; (e) “\\(\\dots\\) you feel worthless”; (f) “\\(\\dots\\) you feel nervous?” Ordinal response alternatives for the Kessler-6 are: “None of the time”; “A little of the time”; “Some of the time”; “Most of the time”; “All of the time.”\nWe report sample descriptive statistics for indicators of personal Kessler-6 distress below in Table1.\nTable 1\n\n\nlibrary(gtsummary)\ntb1 <-nz %>%\n  dplyr::select(\n    KESSLER6sum,\n    FeelHopeless,\n    FeelDepressed,\n    FeelRestless,\n    EverythingIsEffort,\n    FeelWorthless,\n    FeelNervous,\n    Wave,\n  ) %>%\n  gtsummary::tbl_summary(\n    by = Wave,\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} / {N} ({p}%)\"\n    ),\n    digits = all_continuous() ~ 2,\n    missing_text = \"(Missing)\"\n  )%>%\n  bold_labels() \ntb1\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#myqyvvhdwo .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#myqyvvhdwo .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#myqyvvhdwo .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#myqyvvhdwo .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#myqyvvhdwo .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#myqyvvhdwo .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#myqyvvhdwo .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#myqyvvhdwo .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#myqyvvhdwo .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#myqyvvhdwo .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#myqyvvhdwo .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#myqyvvhdwo .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#myqyvvhdwo .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#myqyvvhdwo .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#myqyvvhdwo .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#myqyvvhdwo .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#myqyvvhdwo .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#myqyvvhdwo .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#myqyvvhdwo .gt_left {\n  text-align: left;\n}\n\n#myqyvvhdwo .gt_center {\n  text-align: center;\n}\n\n#myqyvvhdwo .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#myqyvvhdwo .gt_font_normal {\n  font-weight: normal;\n}\n\n#myqyvvhdwo .gt_font_bold {\n  font-weight: bold;\n}\n\n#myqyvvhdwo .gt_font_italic {\n  font-style: italic;\n}\n\n#myqyvvhdwo .gt_super {\n  font-size: 65%;\n}\n\n#myqyvvhdwo .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nCharacteristic\n      2018, N = 2,0681\n      2019, N = 2,0681\n    KESSLER6sum\n      5.07 (3.93)\n      5.20 (4.01)\n    (Missing)\n      15\n      17\n    FeelHopeless\n      \n      \n    None Of The Time\n      1,046 / 2,042 (51%)\n      977 / 2,048 (48%)\n    A Little Of The Time\n      600 / 2,042 (29%)\n      639 / 2,048 (31%)\n    Some Of The Time\n      338 / 2,042 (17%)\n      349 / 2,048 (17%)\n    Most Of The Time\n      54 / 2,042 (2.6%)\n      79 / 2,048 (3.9%)\n    All Of The Time\n      4 / 2,042 (0.2%)\n      4 / 2,048 (0.2%)\n    (Missing)\n      26\n      20\n    FeelDepressed\n      \n      \n    None Of The Time\n      1,469 / 2,048 (72%)\n      1,434 / 2,044 (70%)\n    A Little Of The Time\n      361 / 2,048 (18%)\n      377 / 2,044 (18%)\n    Some Of The Time\n      170 / 2,048 (8.3%)\n      193 / 2,044 (9.4%)\n    Most Of The Time\n      43 / 2,048 (2.1%)\n      35 / 2,044 (1.7%)\n    All Of The Time\n      5 / 2,048 (0.2%)\n      5 / 2,044 (0.2%)\n    (Missing)\n      20\n      24\n    FeelRestless\n      \n      \n    None Of The Time\n      535 / 2,046 (26%)\n      517 / 2,043 (25%)\n    A Little Of The Time\n      763 / 2,046 (37%)\n      757 / 2,043 (37%)\n    Some Of The Time\n      594 / 2,046 (29%)\n      608 / 2,043 (30%)\n    Most Of The Time\n      138 / 2,046 (6.7%)\n      146 / 2,043 (7.1%)\n    All Of The Time\n      16 / 2,046 (0.8%)\n      15 / 2,043 (0.7%)\n    (Missing)\n      22\n      25\n    EverythingIsEffort\n      \n      \n    None Of The Time\n      551 / 2,045 (27%)\n      544 / 2,041 (27%)\n    A Little Of The Time\n      795 / 2,045 (39%)\n      804 / 2,041 (39%)\n    Some Of The Time\n      501 / 2,045 (24%)\n      502 / 2,041 (25%)\n    Most Of The Time\n      158 / 2,045 (7.7%)\n      156 / 2,041 (7.6%)\n    All Of The Time\n      40 / 2,045 (2.0%)\n      35 / 2,041 (1.7%)\n    (Missing)\n      23\n      27\n    FeelWorthless\n      \n      \n    None Of The Time\n      1,433 / 2,049 (70%)\n      1,407 / 2,046 (69%)\n    A Little Of The Time\n      356 / 2,049 (17%)\n      375 / 2,046 (18%)\n    Some Of The Time\n      186 / 2,049 (9.1%)\n      195 / 2,046 (9.5%)\n    Most Of The Time\n      67 / 2,049 (3.3%)\n      60 / 2,046 (2.9%)\n    All Of The Time\n      7 / 2,049 (0.3%)\n      9 / 2,046 (0.4%)\n    (Missing)\n      19\n      22\n    FeelNervous\n      \n      \n    None Of The Time\n      620 / 2,050 (30%)\n      568 / 2,043 (28%)\n    A Little Of The Time\n      789 / 2,050 (38%)\n      850 / 2,043 (42%)\n    Some Of The Time\n      482 / 2,050 (24%)\n      468 / 2,043 (23%)\n    Most Of The Time\n      146 / 2,050 (7.1%)\n      133 / 2,043 (6.5%)\n    All Of The Time\n      13 / 2,050 (0.6%)\n      24 / 2,043 (1.2%)\n    (Missing)\n      18\n      25\n    \n        \n          1\n          \n           \n          Mean (SD); n / N (%)\n          \n      \n    \n\nNote that you can use the gtsummary package to create in-line referencing. For example: Average Kessler-6 distress in 2018 was 5.07 (3.93) and in 2019 was 5.20 (4.01).\nOrder of your Method section\nThe following is a brief guide to describing your method. We’ll be returning to report writing in future weeks. For now, I just want to put this on the table for you. The advice is just a guide.\nHeading\nInclude\nParticipants\nParticipant or subject characteristics\nSampling procedures\nSample size and power\n\nMaterials\nPrimary and secondary measures\nQuality of measurements\n\nProcedure\nData collection methods\nResearch design (e.g., experimental, correlational, or descriptive)\nData processing and diagnostics (e.g., outlier removal)\nData analysis strategy (e.g., comparison or regression tests)\n\nBelow are the sampling procedures from the New Zealand Attitudes and Values Study, from where the nz teaching dataset was drawn.\nAppendix 1A Sampling Procedure – NZAVS Time 10 (2018; conducted from 18.06.2018-28.09.2019)\nThe Time 10 (2018) NZAVS contained responses from 47,951 participants (18,010 retained from one or more previous wave. The sample retained 2,964 participants from the Time 1 (2009) sample (a retention rate of 45.5%). The sample retained 14,049 participants from Time 9 (2017; a retention rate of 82.3% from the previous year). Participants who provided an email address were first emailed and invited to complete an online version if they preferred. Participants who did not complete the online version (or did not provide an email) were then posted a copy of the questionnaire, with a second postal follow-up two months later. We staggered the time of contact, so that participants who had completed the previous wave were contacted approximately one year after they last completed the questionnaire. We offered a prize draw for participation (five draws each for $1000 grocery vouchers, $5000 total prize pool). All participants were posted a Season’s Greetings card from the NZAVS research team and informed that they had been automatically entered into a bonus seasonal grocery voucher prize draw. Participants were also emailed an eight-page newsletter about the study.\nTo boost sample size and increase sample diversity for subsequent waves, a booster sample was conducted by selecting people from the New Zealand electoral roll. As with previous booster samples, sampling was conducted without replacement (i.e., people included in previous sample frames were identified and removed from the 2018 roll). The sample frame consisted of 325,000 people aged from 18-65 randomly selected from the 2018 New Zealand Electoral Roll, who were currently residing in New Zealand (one can be registered to vote in New Zealand but living overseas). The electoral roll contained ~3,250,000 registered voters. The New Zealand Electoral Roll contains participants’ date of birth (within a one-year window), and we limited our frame to people who 65 or younger, due to our aim of retaining participants longitudinally. We concurrently advertised the survey on Facebook via a $5000 paid promotion of a link to a YouTube video describing the NZAVS and the large booster sample we were conducting. The advertisement targeted men and women aged 18-65+ who lived in New Zealand and ran for 14 days. This paid promotion reached 147,296 people, with 4,721 link clicks (i.e., clicking to watch the video), according to Facebook. The goal of the paid promotion was twofold: (a) to increase name recognition of the NZAVS during the period in which questionnaires were being posted, and (b) to help improve retention by potentially reaching previous participants who happened to see the advertisement. A total of 29,293 participants who were contained in our sample frame completed the questionnaire (response rate = 9.2% when adjusting for the 98.2% accuracy of the 2018 electoral roll). A further 648 participants completed the questionnaire, but were unable to be matched to our sample frame (for example, due to a lack of contact information) or were unsolicited opt-ins. Informal analysis indicates that unsolicited opt-ins were often the partners of existing participants.\nAppendix 1B Sampling Procedure – NZAVS Time 11 (2019; conducted from 29.09.2019-17.10.2020)\nThe Time 11 wave was conducted during COVID-19 pandemic. Procedures thus differed in that there was an increased focus on online deliver using email reminders and extensive Facebook advertising, no Christmas card, and incomplete phoning of non-respondents.\nThe Time 11 (2019) NZAVS contained responses from 42,684 participants (36,522 retained from one or more previous wave. The sample retained 2,506 participants from the Time 1 (2009) sample (a retention rate of 38.4%). The sample retained 34,782 participants from Time 10 (2018; a retention rate of 72.5% from the previous year). Participants who provided an email address were first emailed and invited to complete an online version if they preferred. Participants who did not complete the online version (or did not provide an email) were then posted a copy of the questionnaire, with a second postal follow-up two months later. We staggered the time of contact, so that participants who had completed the previous wave were contacted approximately one year after they last completed the questionnaire. A second reminder email was sent approximately four months following the first email attempt. We offered a prize draw for participation (five draws each for $1000 grocery vouchers, $5000 total prize pool). Participants were also emailed an eight-page newsletter about the study. As in past years, three attempts were made to phone non-respondents using each available cell and landline number. However, due to the university closure during COVID-19 lockdowns, phoning attempts were made for only 54% of the phoning pool (11,687 from a total of 21,636 non-respondents who provided at least one phone number).\nTwo additional forms of recruitment were also introduced during Time 11. The first was a large information box in the questionnaire (taking a full page on the paper version), which asked people: ‘Do you have a partner who would also like to join the NZAVS?’ with additional details for how partners might join the study (see questionnaire for the full text). The second was a Facebook advertisement. The advertisement targeted men and women aged 18-65+ who lived in New Zealand and ran from and 4th April 2020 – 4th July 2020 (overlapping with New Zeeland’s first lockdown period and recovery), and again from 18th August 2020 – 4th September (during the second Auckland lockdown). Given the unprecedented nature of the COVID-19 lockdowns, we thought it important to maximise sampling during these periods. The goal of the Facebook advertisement was threefold: (a) to increase name recognition of the NZAVS and remind people to complete the paper/online version already posted/emailed to them, (b) to help improve retention by potentially reaching previous lost participants who happened to see the advertisement, and (c) to recruit new participants (and also the partners of existing participants) while people were at home with some possibly having more free time during lockdown. This last goal was indirect and not explicitly stated it in the advertisement.\nThe Facebook advertisement read as follows: “Participate in the New Zealand Attitudes and Values Study. Complete the 2020 Questionnaire online” with the body of text: “If you are part of the NZAVS, but have not heard from us in the last year, then please consider completing the 2020 questionnaire online. The study is more important than ever as we aim to understand the impact of COVID-19 on mental health, wellbeing and resilience in our communities. We wish you all the best at this time and hope you keep well and stay safe.” This paid promotion reached 883,969 people, with 37,850 link clicks (i.e., clicking the link for the Qualtrics survey) according to Facebook. A total of 6106 people continued complete the questionnaire and provide full contact details, and were thus included in the dataset (4734 were new participants opting in to the study, and 1372 were previously ‘lost’ participants).\nAppendix 2 Johannes’s mini-lecture on the papaja package\nLecture\nPapaja R markdown template\nAppendix 3 Style advice about research methods\nAPA style advice here\n\n\n\nKessler, R C, G Andrews, L J Colpe, E Hiripi, D K Mroczek, S L T Normand, E E Walters, and A M Zaslavsky. 2002. “Short Screening Scales to Monitor Population Prevalences and Trends in Non-Specific Psychological Distress.” Psychol. Med. 32 (6): 959–76.\n\n\nKessler, Ronald C, Jennifer Greif Green, Michael J Gruber, Nancy A Sampson, Evelyn Bromet, Marius Cuitan, Toshi A Furukawa, et al. 2010. “Screening for Serious Mental Illness in the General Population with the K6 Screening Scale: Results from the WHO World Mental Health (WMH) Survey Initiative.” Int. J. Methods Psychiatr. Res. 19 Suppl 1 (June): 4–22.\n\n\nProchaska, Judith J, Hai-Yen Sung, Wendy Max, Yanling Shi, and Michael Ong. 2012. “Validity Study of the K6 Scale as a Measure of Moderate Mental Distress Based on Mental Health Treatment Need and Utilization.” Int. J. Methods Psychiatr. Res. 21 (2): 88–97.\n\n\nNotice, the intercept here is zero. This because we centered the new indicator at zero, and we wrote a model that is estimating the population average for this outcome (an intercept-only model). Don’t worry if you don’t know what an intercept is, we’ll get to regression in a few weeks.↩︎\n",
    "preview": "posts/4_1/op.png",
    "last_modified": "2021-03-31T20:43:47+13:00",
    "input_file": {},
    "preview_width": 10241,
    "preview_height": 8450
  },
  {
    "path": "posts/3_1/",
    "title": "Visualisation",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\n\nContents\nData visualisation with ggplot2\nIntroduction\nCreating a graph\nUsing ggplot 2 to highlight elements of interest.\nFacets\nUnderstanding your data through graphs\nTransforming data\nRevisiting logical operators\nCommand filter: keeps rows matching criteria\nTask\nCommand select: picks columns by column name\nCommand arrange reorders rows\nCommand mutate add new variable name\nCommand summarise reduce variables to values\nMultiple pipe operators\nOther functions\n\n\n\nData visualisation with ggplot2\n\n\n\nIntroduction\nIn this lecture we’ll first introduce you to the ggplot2 package, and vocabulary, for creating graphs in R. We’ll mostly follow the approach described in the book “R for data science,” which can be found here.\nWe’ll then turn to data-wrangling using the dplyr package.\nBoth ggplot2 and dplyr can be found in library(tidyverse)\nCreating a graph\nStep 1, load tidyverse:\n\n\nlibrary(\"tidyverse\")\n\n\n\nStep 2, Make sure your dataset is loaded. We’ll start with the mpg dataset\n\n\n#inspect the mpg dataset\nhead(mpg)\n\n\n# A tibble: 6 x 11\n  manufacturer model displ  year   cyl trans   drv     cty   hwy fl   \n  <chr>        <chr> <dbl> <int> <int> <chr>   <chr> <int> <int> <chr>\n1 audi         a4      1.8  1999     4 auto(l… f        18    29 p    \n2 audi         a4      1.8  1999     4 manual… f        21    29 p    \n3 audi         a4      2    2008     4 manual… f        20    31 p    \n4 audi         a4      2    2008     4 auto(a… f        21    30 p    \n5 audi         a4      2.8  1999     6 auto(l… f        16    26 p    \n6 audi         a4      2.8  1999     6 manual… f        18    26 p    \n# … with 1 more variable: class <chr>\n\nStep 3. Inspect the\"Negative relationship between highway fuel efficiency and a cars engine size (which is given by the variable displ).\n\n\n# Create graph\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Negative relationship between engine displacement and fuel efficiency.\")\n\n\n\n\nA basic problem with this graph is that we don’t know what it is representing. To avoid this problem, it is useful to get into the habit of adding titles to your graphs, and also of using informative axis labels. We do this by adding additional layers.\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Negative relationship between engine displacement and fuel efficiency.\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\")\n\n\n\n\nLet’s walk through the logic of the ggplot2 “grammar”:\nFirst we call the data\n\n\n# here we are calling up the data\nggplot(data = mpg)\n\n\n\nNext, we add a layer of points, by calling the relevant columns and rows of this dataset\n\n\n# Here, we add a layer of points, by calling the relevant columns and rows of this dataset\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\nThen we add the title\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title =  \"Negative relationship between engine displacement and fuel efficiency.\")\n\n\n\n\nThen we add the labels\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\")   + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\nWe can change the axis starting positions:\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\")   + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") + expand_limits(x = 0, y = 0)\n\n\n\n\nThe generic method for adding layers is as follows:\n\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\nUsing ggplot 2 to highlight elements of interest.\nHere we can use the “color =” option.1\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) \n\n\n\n\nHere’s a shape command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape = class)) \n\n\n\n\nHere’s a size command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, size = cty)) \n\n\n\n\nHere’s the fill command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, fill = cty)) \n\n\n\n\nHere’s the alpha command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, alpha  = .1)) \n\n\n\n\nHere’s the alpha command combined with the fill command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, alpha = cty, size = cty)) \n\n\n\n\nFacets\nWe can create multiple graphs using facets\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) + \n   facet_wrap(~ class, nrow = 2)\n\n\n\n\nWe use facet_grid for graphing the Negative relationship between two variables.\nNote the difference betwen these two graphs:\nHere the focus is on the negative relationship between class and the x variable, displacement\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_grid(class ~ .)  + theme(legend.position = \"none\") \n\n\n\n\nHere the focus is on the relationship betwen class and the y variable, highway milage.\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_grid(. ~ class) + theme(legend.position = \"none\") \n\n\n\n\nWe can focus on Negative relationship between class and the x and y variables simultaneously. Here we add the ’year` indicator and we do not see much of an improvement in highway milage for the different classes, adjusting for displacement:\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_grid(class ~ year) + theme(legend.position = \"bottom\") +\n  labs(title =\"Negative relationship between engine displacement and fuel efficiency by class.\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\nUnderstanding your data through graphs\nWe can create a graph of relationships:\n\n\n# set better theme\ntheme_set(theme_classic())\nggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy)) + \n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\") +\n  xlab(\"Engine displacement in (units)\") +\n  ylab(\"Highway miles per liter\") \n\n\n\n\nAdd points as a layer\n\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  geom_smooth(mapping = aes(x = displ, y = hwy)) +\n  theme(legend.position = \"bottom\") +\n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\") +\n  xlab(\"Engine displacement in (units)\") +\n  ylab(\"Highway miles per liter\") \n\n\n\n\nWe can write this more compactly, by including the mapping with the data layer\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\nThen we can include mappings for specific layers\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n\n\n\n\nWe can add a grouping factor e.g. for “drv”, thus creating multiple lines\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, group = drv)) + \n  geom_point(aes(color = class)) + \n  geom_smooth()\n\n\n\n\nWe can replace the smooths with linear models\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, linetype = drv)) + \n  geom_point(aes(color = class)) + \n  geom_smooth(method = \"lm\")\n\n\n\n\nTransforming data\nFirst we’ll get the flights data\n\n\nlibrary(nycflights13)\nhead(flights)\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\nNext we’ll create some data frames to help us illustrate points\n\n\ndf <- data.frame(\ncolour = c(\"blue\", \"black\", \"blue\", \"blue\", \"black\"), value = 1:5)\nhead(df)\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n5  black     5\n\nRevisiting logical operators\nRecall our logical operators. These will be essential for data wrangling\n\n\nknitr::include_graphics(\"logic.png\")\n\n\n\n\nCommand filter: keeps rows matching criteria\nKeep only blue rows:\n\n\ndf%>%\nfilter(colour == \"blue\")\n\n\n  colour value\n1   blue     1\n2   blue     3\n3   blue     4\n\nKeep only values 1 and 4\n\n\ndf%>%\n  filter (value %in% c(1,4))\n\n\n  colour value\n1   blue     1\n2   blue     4\n\nKeep values 1 through 4\n\n\ndf %>%\n  filter (value %in% c(1:4))\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n\nAnother way to do the same\n\n\ndf %>%\n  filter (value != 5)\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n\nTask\nHow can we find all flights that left in January?\n\n\nhead(flights)\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\n\nflights%>%\n  dplyr::filter(month ==1)\n\n\n# A tibble: 27,004 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 26,994 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nFlights delayed by more than 15 mintutes that arrived on time\n\n\nflights%>%\n  dplyr::filter (dep_delay >15 & arr_delay <=0)\n\n\n# A tibble: 4,314 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1     1025            951        34     1258\n 2  2013     1     1     1033           1017        16     1130\n 3  2013     1     1     2052           2029        23     2349\n 4  2013     1     1     2107           2040        27     2354\n 5  2013     1     2      727            645        42     1024\n 6  2013     1     2     1004            945        19     1251\n 7  2013     1     2     1031           1015        16     1135\n 8  2013     1     2     1500           1430        30     1741\n 9  2013     1     2     1737           1720        17     1908\n10  2013     1     2     1831           1815        16     2130\n# … with 4,304 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nCommand select: picks columns by column name\nSelect the colour column\n\n\ndf%>%\n  dplyr::select ( colour )\n\n\n  colour\n1   blue\n2  black\n3   blue\n4   blue\n5  black\n\nAnother way?\n\n\ndf%>%\n  dplyr::select ( !value )\n\n\n  colour\n1   blue\n2  black\n3   blue\n4   blue\n5  black\n\nor\n\n\ndf%>%\n  dplyr::select ( -c(value ))\n\n\n  colour\n1   blue\n2  black\n3   blue\n4   blue\n5  black\n\nCommand arrange reorders rows\n\n\ndf %>%\n  arrange(value)\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n5  black     5\n\n\n\ndf %>%\n  arrange(desc(value))\n\n\n  colour value\n1  black     5\n2   blue     4\n3   blue     3\n4  black     2\n5   blue     1\n\nTask: how would we order flights by departure data and time ?\n\n\nflights %>%\n  arrange(month, day, dep_time)\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nTask which flights have the greated difference between departure delay and arrival delay?\n\n\nflights%>%\n  arrange(desc(dep_delay - arr_delay))\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     6    13     1907           1512       235     2134\n 2  2013     2    26     1000            900        60     1513\n 3  2013     2    23     1226            900       206     1746\n 4  2013     5    13     1917           1900        17     2149\n 5  2013     2    27      924            900        24     1448\n 6  2013     7    14     1917           1829        48     2109\n 7  2013     7    17     2004           1930        34     2224\n 8  2013    12    27     1719           1648        31     1956\n 9  2013     5     2     1947           1949        -2     2209\n10  2013    11    13     2024           2015         9     2251\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nNot this could be written briefly as this:\n\n\narrange(flights, desc(dep_delay - arr_delay))\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     6    13     1907           1512       235     2134\n 2  2013     2    26     1000            900        60     1513\n 3  2013     2    23     1226            900       206     1746\n 4  2013     5    13     1917           1900        17     2149\n 5  2013     2    27      924            900        24     1448\n 6  2013     7    14     1917           1829        48     2109\n 7  2013     7    17     2004           1930        34     2224\n 8  2013    12    27     1719           1648        31     1956\n 9  2013     5     2     1947           1949        -2     2209\n10  2013    11    13     2024           2015         9     2251\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nCommand mutate add new variable name\n\n\ndf %>%\n  mutate(double_value = 2 * value)\n\n\n  colour value double_value\n1   blue     1            2\n2  black     2            4\n3   blue     3            6\n4   blue     4            8\n5  black     5           10\n\nOrder flights by greatest difference between departure delay and arrival delay?\n\n\nflights %>%\n  mutate(diff_dep_arr = dep_delay - arr_delay)%>%\n  select(flight,diff_dep_arr)%>%\n  arrange(desc(diff_dep_arr))\n\n\n# A tibble: 336,776 x 2\n   flight diff_dep_arr\n    <int>        <dbl>\n 1   4377          109\n 2     51           87\n 3     51           80\n 4   1465           79\n 5     51           76\n 6    673           74\n 7   1532           74\n 8   1284           73\n 9    612           73\n10    427           72\n# … with 336,766 more rows\n\nCommand summarise reduce variables to values\nSum all values in the df dataset\n\n\ndf %>%\n  summarise (total = sum(value))\n\n\n  total\n1    15\n\nSummaries the values by colour groups, and give the number of items per colour group\n\n\ndf %>%\n  group_by(colour) %>%\n  summarise(total = sum(value),\n            n = n())\n\n\n# A tibble: 2 x 3\n  colour total     n\n  <chr>  <int> <int>\n1 black      7     2\n2 blue       8     3\n\nUseful summary functions are:\nmin(x)\nmax(x)\nmean(x)\nn\nn_distinct\nsum(x)\nsum(x > 10)\nmean(x > 10)\nsd(x)\nvar(x)\nTask, how many flights flew on Christmas?\n\n\nhead(flights)\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\n\nflights %>%\n  filter( month == 12, day == 25)%>%\n  summarise (n = n())\n\n\n# A tibble: 1 x 1\n      n\n  <int>\n1   719\n\nCalculate average delay:\n\n\nflights %>%\n  summarise(delay = mean(dep_delay, na.rm = TRUE))\n\n\n# A tibble: 1 x 1\n  delay\n  <dbl>\n1  12.6\n\n\n\nsummarise(flights, delay = mean(dep_delay, na.rm = TRUE))\n\n\n# A tibble: 1 x 1\n  delay\n  <dbl>\n1  12.6\n\nMultiple pipe operators\nHere we:\nGroup flights by destination.\nSummarise to compute distance, average delay, and number of flights.\nRemove Honolulu airport, because it is so far away\n\n\ndelays <- flights %>% \n  group_by(dest) %>% \n  summarise(\n    count = n(),\n    dist = mean(distance, na.rm = TRUE),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) %>% \n  filter(dest != \"HNL\")\nhead(delays)\n\n\n# A tibble: 6 x 4\n  dest  count  dist delay\n  <chr> <int> <dbl> <dbl>\n1 ABQ     254 1826   4.38\n2 ACK     265  199   4.85\n3 ALB     439  143  14.4 \n4 ANC       8 3370  -2.5 \n5 ATL   17215  757. 11.3 \n6 AUS    2439 1514.  6.02\n\n\n\nflights %>% \n  filter(!is.na(dep_delay), !is.na(arr_delay)) %>% # not cancelled\n   group_by(tailnum) %>% # group by unique aircraft\n  summarise(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  ) %>%\n  ggplot(mapping = aes(x = n, y = delay)) + \n  geom_point(alpha = 1/10)  + \n  labs(title = \"Variation in average delay by tailnumber \") \n\n\n\n\nOther functions\nSuppose you only wanted to keep your mutated variables, in this case you can use transmute\n\n\nnew_flights <-transmute(flights,\n  gain = dep_delay - arr_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n)\nhead(new_flights)\n\n\n# A tibble: 6 x 3\n   gain hours gain_per_hour\n  <dbl> <dbl>         <dbl>\n1    -9  3.78         -2.38\n2   -16  3.78         -4.23\n3   -31  2.67        -11.6 \n4    17  3.05          5.57\n5    19  1.93          9.83\n6   -16  2.5          -6.4 \n\nTo learn more, go to https://dplyr.tidyverse.org/\n\nRemoving the axis and labels here just to keep the code compact↩︎\n",
    "preview": "posts/3_1/op.jpg",
    "last_modified": "2021-03-17T01:36:04+13:00",
    "input_file": {}
  },
  {
    "path": "posts/2_1/",
    "title": "Coding basics",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-03-02",
    "categories": [],
    "contents": "\n\nContents\nTo do:\nFundamentals of R\nHow to use R as calculator\nInspecting data\ncheck head of dataset sing head\ncheck data types using str\nnames\nview rownames\ntable (and use of $)\nchange column names\n\nBasic data structures\nvectors of numbers ( and use of c and i:x )\nvectors of characters\ndataframes (2 dimensional square arrays of vectors)\nrename columns of a data frame\nmatrix\nlists\n\nClasses in R\nnumeric and integer\nfactors\n\nIndexing in R\ncolumns\nrows\nrows and columns\nselection by negation\n\nBasic data wrangling in R\nuse of $ and [i:x]\n\nIndexing for logical operations\ndefinitions\nevaluation using logical operators\n\nThe basic structure of R commands\nmean\nsd\nsummary\nCoding\nInstalling package\n\nRolling your own code\nCoding etiquette\n\nusing R!\ndata summary\nmodel\nresults\ngraph predicted effects\nWhat is the advantage of this graph?\ntry another model\nAppendix: # symbol is for commenting code\nRounding\n\n\nTo do:\nA good, and free introduction to R for Data Science Read chapters: 2-8 (they are short chapters.)\nFundamentals of R\nA console runs all your code in R\nA source page runs all your code in R.\nA working directory is where R will look for raw data and other material.\nIn R-studio, (for starters) you can can use the file tab to import and save your material.\nSimilarly, in R-studio, (for starters) you can can use install packages by clicking the Install tab in the package pane (generally lower right pane) to install packagees.\nHow to use R as calculator\nWe can use R as a calculator. You can run any mathematical operation you would normally use by entering it into the console:\n\n\n## Addition\nx <- 3 + 2\nx\n\n\n[1] 5\n\n\n## Subtraction\nx <-  3 - 2\nx\n\n\n[1] 1\n\n\n## Multiplication\nx <-  3 * 2\nx\n\n\n[1] 6\n\n\n## Division\nx <-  3 / 2\nx\n\n\n[1] 1.5\n\n\n## Modulus (Remainder from division)\nx <-  3 %% 2\nx\n\n\n[1] 1\n\n\n## Exponentiation\nx = 3 ^ 2\nx\n\n\n[1] 9\n\n\n## Integer Division (Number of times denominator fits in numerator)\nx = 3 %/% 2\nx\n\n\n[1] 1\n\nInspecting data\nIn a moment, we’ll teach you how to import data into R. For now, let’s work with a dataset that is already present in your R environment, the iris dataset.\nHere are some useful commands for inspecting data\ncheck head of dataset sing head\n\n\n# the top rows and columns of the dataset\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ncheck data types using str\n\n\nstr(iris)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nnames\n\n\n#names of the columns\nnames(iris)\n\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\"\n[4] \"Petal.Width\"  \"Species\"     \n\nview rownames\n\n\n# view rownames\nrownames(iris)\n\n\n  [1] \"1\"   \"2\"   \"3\"   \"4\"   \"5\"   \"6\"   \"7\"   \"8\"   \"9\"  \n [10] \"10\"  \"11\"  \"12\"  \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\" \n [19] \"19\"  \"20\"  \"21\"  \"22\"  \"23\"  \"24\"  \"25\"  \"26\"  \"27\" \n [28] \"28\"  \"29\"  \"30\"  \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\" \n [37] \"37\"  \"38\"  \"39\"  \"40\"  \"41\"  \"42\"  \"43\"  \"44\"  \"45\" \n [46] \"46\"  \"47\"  \"48\"  \"49\"  \"50\"  \"51\"  \"52\"  \"53\"  \"54\" \n [55] \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"60\"  \"61\"  \"62\"  \"63\" \n [64] \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"70\"  \"71\"  \"72\" \n [73] \"73\"  \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"80\"  \"81\" \n [82] \"82\"  \"83\"  \"84\"  \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"90\" \n [91] \"91\"  \"92\"  \"93\"  \"94\"  \"95\"  \"96\"  \"97\"  \"98\"  \"99\" \n[100] \"100\" \"101\" \"102\" \"103\" \"104\" \"105\" \"106\" \"107\" \"108\"\n[109] \"109\" \"110\" \"111\" \"112\" \"113\" \"114\" \"115\" \"116\" \"117\"\n[118] \"118\" \"119\" \"120\" \"121\" \"122\" \"123\" \"124\" \"125\" \"126\"\n[127] \"127\" \"128\" \"129\" \"130\" \"131\" \"132\" \"133\" \"134\" \"135\"\n[136] \"136\" \"137\" \"138\" \"139\" \"140\" \"141\" \"142\" \"143\" \"144\"\n[145] \"145\" \"146\" \"147\" \"148\" \"149\" \"150\"\n\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntable (and use of $)\n\n\n# create a table\ntable(iris$Species)\n\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\nchange column names\n\n\n# change column names\n# create new dataset for \nirisdat2 <- iris\n# chage names of columns\nnames(irisdat2)[] <- c(\"s_l\", \"s_w\", \"p_l\", \"p_w\", \"sp\")\n#inspect\nhead(irisdat2)\n\n\n  s_l s_w p_l p_w     sp\n1 5.1 3.5 1.4 0.2 setosa\n2 4.9 3.0 1.4 0.2 setosa\n3 4.7 3.2 1.3 0.2 setosa\n4 4.6 3.1 1.5 0.2 setosa\n5 5.0 3.6 1.4 0.2 setosa\n6 5.4 3.9 1.7 0.4 setosa\n\nBasic data structures\nvectors of numbers ( and use of c and i:x )\n\n\ngo_vector <- c(1:5)\ngo_vector\n\n\n[1] 1 2 3 4 5\n\nvectors of characters\n\n\ngo_vector2 <- c(\"hello\", \"world\")\ngo_vector2\n\n\n[1] \"hello\" \"world\"\n\n\n\nas.vector(irisdat2$s_l)\n\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8\n [14] 4.3 5.8 5.7 5.4 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0\n [27] 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 5.5 4.9 4.4\n [40] 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4\n [53] 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6\n [66] 6.7 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7\n [79] 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 5.5\n [92] 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3\n[105] 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5\n[118] 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1 6.4 7.2\n[131] 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8\n[144] 6.8 6.7 6.7 6.3 6.5 6.2 5.9\n\ndataframes (2 dimensional square arrays of vectors)\n2 x dimensional “square” array with equal column and row lengths. Can contain data with multiple formats characters, facotors, integers, etc.\n\n\nyuk <- data.frame(c(\"the\", \"enumeration\", \"of\", \"the\", \"constitution\"), 6:10)\nyuk\n\n\n  c..the....enumeration....of....the....constitution..\n1                                                  the\n2                                          enumeration\n3                                                   of\n4                                                  the\n5                                         constitution\n  X6.10\n1     6\n2     7\n3     8\n4     9\n5    10\n\nrename columns of a data frame\n\n\nnames(yuk)[] <- c(\"short\", \"best\")\nyuk\n\n\n         short best\n1          the    6\n2  enumeration    7\n3           of    8\n4          the    9\n5 constitution   10\n\nmatrix\nSame as a dataframe but can only contain one format (e.g. numbers or characters)\n\n\nyok <- as.matrix(yuk)\nyok\n\n\n     short          best\n[1,] \"the\"          \" 6\"\n[2,] \"enumeration\"  \" 7\"\n[3,] \"of\"           \" 8\"\n[4,] \"the\"          \" 9\"\n[5,] \"constitution\" \"10\"\n\nlists\nArrays with constraints on “squareness” or data types.\n\n\nlok <- list(yok, yuk)\nlok\n\n\n[[1]]\n     short          best\n[1,] \"the\"          \" 6\"\n[2,] \"enumeration\"  \" 7\"\n[3,] \"of\"           \" 8\"\n[4,] \"the\"          \" 9\"\n[5,] \"constitution\" \"10\"\n\n[[2]]\n         short best\n1          the    6\n2  enumeration    7\n3           of    8\n4          the    9\n5 constitution   10\n\nClasses in R\nnumeric and integer\nnumeric means number\n\n\nis.numeric(4.2)\n\n\n[1] TRUE\n\ninteger means a number that is not a fraction\n\n\nis.integer(4.2)\n\n\n[1] FALSE\n\nNote the default here:\n\n\nis.integer(4)\n\n\n[1] FALSE\n\n\n\nis.integer(as.integer(4))\n\n\n[1] TRUE\n\nWe’ll need to ensure that certain numbers are integers later on, when we are estimating poisson models and/or doing bayesian data analysis.\ncharacters\nCharacters are strings:\n\n\n# this is a character\nis.character(\"chapeau\")\n\n\n[1] TRUE\n\n# this is not\nis.character(4)\n\n\n[1] FALSE\n\nfactors\nA factor is a category. It can be ordered (e.g. an ordinal scale) or unordered (say a participant in a study, or a wave in a longitidunal study)\n\n\nstr(iris)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nIt’s really important to check that ordered categories are really ordered categories in your dataset.\nThere was is a famous retraction recently where a group found that religion was associated with immorality, however the countries in the the study had been coded as numbers, not as factors. The study’s effect entirely disappeared once this error was corrected!\nIndexing in R\ncolumns\n\n\n# select second column of \"yuk\"\nyuk[, 2]\n\n\n[1]  6  7  8  9 10\n\nrows\n\n\n# select second row of yuk\nyuk[2, ]\n\n\n        short best\n2 enumeration    7\n\nrows and columns\n\n\n#select first row and first column of yuk\nyuk[1, 2]\n\n\n[1] 6\n\nselection by negation\n\n\n# negate the first column of yuk\nyuk[, -1]\n\n\n[1]  6  7  8  9 10\n\n\n\n# negate the second column of yuk\nyuk[,-2]\n\n\n[1] \"the\"          \"enumeration\"  \"of\"          \n[4] \"the\"          \"constitution\"\n\nBasic data wrangling in R\nc\n\n\n# select only the first and second cols of iris\niris_short2 <- iris[ ,c( 1, 2 ) ]\nhead(iris_short2)\n\n\n  Sepal.Length Sepal.Width\n1          5.1         3.5\n2          4.9         3.0\n3          4.7         3.2\n4          4.6         3.1\n5          5.0         3.6\n6          5.4         3.9\n\n-c\n\n\n# select all columns but the first and second of iris\niris_short <- iris[ ,-c( 1, 2 ) ]\nhead(iris_short)\n\n\n  Petal.Length Petal.Width Species\n1          1.4         0.2  setosa\n2          1.4         0.2  setosa\n3          1.3         0.2  setosa\n4          1.5         0.2  setosa\n5          1.4         0.2  setosa\n6          1.7         0.4  setosa\n\ncbind\n\n\n# for use with dataframes and matrices -- note that srings a are c\nyokyuk<-cbind(yok,yuk)\nyokyuk\n\n\n         short best        short best\n1          the    6          the    6\n2  enumeration    7  enumeration    7\n3           of    8           of    8\n4          the    9          the    9\n5 constitution   10 constitution   10\n\nstr(yokyuk)\n\n\n'data.frame':   5 obs. of  4 variables:\n $ short: chr  \"the\" \"enumeration\" \"of\" \"the\" ...\n $ best : chr  \" 6\" \" 7\" \" 8\" \" 9\" ...\n $ short: chr  \"the\" \"enumeration\" \"of\" \"the\" ...\n $ best : int  6 7 8 9 10\n\nrbind\n\n\nrbind(yuk[,],yok[2:3])\n\n\n         short best\n1          the    6\n2  enumeration    7\n3           of    8\n4          the    9\n5 constitution   10\n6  enumeration   of\n\nuse of $ and []\n\n\n#select the fifth row of the column\niris_short$Petal.Width[5]\n\n\n[1] 0.2\n\nuse of $ and [i:x]\n\n\n#select the 5th-25th row of the column\niris_short$Petal.Width[5:25]\n\n\n [1] 0.2 0.4 0.3 0.2 0.2 0.1 0.2 0.2 0.1 0.1 0.2 0.4 0.4 0.3\n[15] 0.3 0.3 0.2 0.4 0.2 0.5 0.2\n\nIndexing for logical operations\ndefinitions\n== means “equals to”\n!= means “not equals to”\n> means “greater than”\n< means “less than”\n>=means “greater than or equal”\n<= means “less than or equal”\n! means “not”\n& means “and”\n| means “or”!\nis.na means “is missing” (missing values are coded in R as NA)\n> -9999 == 666 > TRUE !!! :)\nevaluation using logical operators\ncreate dataframe\n\n\n# create data frame\ndf<-data.frame( x = c(1:10),y = c(11:20) )\n\n\n\nevaluate cases\n\n\n#evaluate cases in y that greater  than 15\ndf[,\"y\"] > 15\n\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[10]  TRUE\n\nsum instances\n\n\n# count these cases\nsum(df[,\"y\"] > 15)\n\n\n[1] 5\n\nsum instances with a different operator\n\n\n# count cases greater than or equal to 15\nsum(df[,\"y\"] >= 15)\n\n\n[1] 6\n\nanother methods\n\n\n# another approach\nsum(df$y >= 15)\n\n\n[1] 6\n\nusing the or function\n\n\n# combine operators\nsum(df$y >= 15 | df$y <=11)\n\n\n[1] 7\n\ngo meta\n\n\n# go \"meta\"\nsum(df$y >= 15 | df$y <= 11) == sum(df$x >=5 | df$x <=1 )\n\n\n[1] TRUE\n\ngo meta-meta!\n\n\n# go meta-meta\nsum(sum(df$y >= 15 | df$y <= 11) == sum(df$x >=5 | df$x <=1 ))\n\n\n[1] 1\n\nuse operators to modify data\n\n\n# using assignment to modify data\ndf$x[df$x >=5 ] <- NA\ndf\n\n\n    x  y\n1   1 11\n2   2 12\n3   3 13\n4   4 14\n5  NA 15\n6  NA 16\n7  NA 17\n8  NA 18\n9  NA 19\n10 NA 20\n\nusing is.na and !is.na\n\n\nsum(is.na(df$x))\n\n\n[1] 6\n\n\n\nsum(!is.na(df$x))\n\n\n[1] 4\n\n\n\nsum(is.na(df$x)) + sum(!is.na(df$x)) \n\n\n[1] 10\n\nThe basic structure of R commands\nThe four main elements of every R code are:\nobjects,\nfunctions,\narguments\noperators.\nFigure 1 provides a simple example, that produces a new object which contains the mean of variable x.\n\n\n\nFigure 1: The Basic Syntax of R\n\n\n\nmean\nThe function mean generates the arithmetic mean of an input object:\n\n\n# a function to assess the mean of a Sepal.Length\nmean(iris$Sepal.Length)\n\n\n[1] 5.843333\n\nsd\nThe function sd gives us the standard deviation:\n\n\n# standard deviation of Sepal.Length\nsd(iris$Sepal.Length)\n\n\n[1] 0.8280661\n\nsummary\n\n\n# summary of the \"Sepal Length\" column\nsummary(iris$Sepal.Length)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.300   5.100   5.800   5.843   6.400   7.900 \n\n\n\n# summary of the Iris data set\nsummary(iris)\n\n\n  Sepal.Length    Sepal.Width     Petal.Length  \n Min.   :4.300   Min.   :2.000   Min.   :1.000  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600  \n Median :5.800   Median :3.000   Median :4.350  \n Mean   :5.843   Mean   :3.057   Mean   :3.758  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100  \n Max.   :7.900   Max.   :4.400   Max.   :6.900  \n  Petal.Width          Species  \n Min.   :0.100   setosa    :50  \n 1st Qu.:0.300   versicolor:50  \n Median :1.300   virginica :50  \n Mean   :1.199                  \n 3rd Qu.:1.800                  \n Max.   :2.500                  \n\nCoding\nThe object needs to be specified inside of the function brackets as the x argument, in this case we define x = df .\nLast, we assign the result of this function for later us via the <- operator to an object which we decided to call object.\nIn other words, we create a new object that can be further manipulated and contains information about the mean of a previously created object “x”. This structure represents the foundation of most operations in R.\nInstead calling the mean function as above one could manually add all values of x dividing it by the number of x values.\nNevertheless, this would be very cumbersome.\nFor this reason, functions (pre-assembled lines of code) exist to reduce the amount of coding necessary.\nThese functions can be bundled into packages. R’s capacity for creating packages is main appeal of R as a statistical tool because community developed functions are available from a central repository called CRAN in the form of packages.\nThese packages can be installed in R with the command install.packages(“package name”).\nIt is important that you only need to install a package once on your machine, expect if you want to upgrade the package. Generally speaking you regularly want to upgrade your packages, but keep a permanent note in your code which version of a package you used when it was initially written.\nInstalling package\ninstall these packages (we’ll be using them later)\n\n\ninstall.packages(\"devtools\") # installing packages\n\n\n\n\n\ninstall.packages(\"remotes\") # installing packages\n\n\n\n\n\ninstall.packages(\"tidyverse\") ## data wrangling and visualisation\n\n\n\n\n\ninstall.packages(\"lme4\") # multilevel modelling\n\n\n\n\n\ninstall.packages(\"patchwork\") # arranging multiple graphs\n\n\n\n\n\nlibrary(devtools)\ndevtools::install_github(\"strengejacke/sjPlot\") # plots and tables\n\n\n\n\n\ninstall.packages(\"papaja\")  # writing APA documents\n\n\n\n\n\ninstall.packages(\"table1\") # summary tables\n\n\n\nextra credit\n\n\ndevtools::install_github(\"easystats/easystats\")\n\n\n\n\n\ndevtools::install_github(\"strengejacke/ggeffects\")\n\n\n\nsuper extra credit\n\n\nif (!requireNamespace(\"remotes\")) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"paul-buerkner/brms\")\n\n\n\n\n\ndevtools::install_github(\"stan-dev/cmdstanr\")\n\n\n\nRolling your own code\nLet’s use R to write a function. Recall that a factorial for a number \\(n\\) is the product of all positive inters less than or equal to \\(n\\). Thus the factorial for 5 = \\[1 \\times 2 \\times 3 \\times 4 \\times 5\\]\nIn R we can write a function:\n\n\n# create a function to perform the factorial operation \ngo_factorial <- function(x) {\n  y <- 1\n  for (i in 1:x) {\n    y <- y * ((1:x)[i])\n  }\n  print(y)\n}\n\n\n\nLet’s try it out\n\n\n# test of the `go_factorial` function\ngo_factorial(5)\n\n\n[1] 120\n\nLet’s see if this is the number that R’s factorial function produces:\n\n\n# R's native factorial function\nfactorial(5)\n\n\n[1] 120\n\nWe can use R’s == relational operator to evaluate whether the two functions are the same\n\n\n# are the two functions equivalent for factorial five\ngo_factorial(5) == factorial(5)\n\n\n[1] 120\n[1] TRUE\n\nFor more information about relational operators type the following into your console:\n\n\n?`==`\n\n\n\nWe can make more complicated functions:\n\n\n# function for factorial that throws warnings when the data that are entered are not appropriate. \ngo_bayes_factorial <- function (x) {\n  # check is the number is negative, positive or zero\n  if (x  < 0) {\n    print(\"not even Ashley Bloomfield could make a factorial for a negative number\")\n  } else if (x == 0) {\n    print(\"the factorial of zero is defined as 1\")\n  } else {\n    for (i in 1:x)\n      y <- 1\n    for (i in 1:x) {\n      y <- y * ((1:x)[i])\n    }\n    print(y)\n  }\n}\n\n\n\nWe’ll come back to functions later. It’s useful to look at an example of a function so that you can see that R is much more than a calcultor. It is a tool to empower you for doing data anlysis in new and creative ways.\nCoding etiquette\nKeep your code legible and annotate\nWhy is this bad code?\n\n\ndf1<-data.frame(a=rnorm(10,1,1),b=rnorm(10,4,8),c=rnorm(10,8,1),d=rnorm(10,7,2))\n\n\n\nWhy is this better code?\n\n\n\n# Create a data frame with four columns of randomly generated numbers specifying different means and standard deviations \ndf1 <- data.frame(\n  a = rnorm( 10, mean = 1, sd = 1 ),\n  b = rnorm( 10, mean = 4, sd = 8 ),\n  c = rnorm( 10, mean = 8, sd = 1 ),\n  d = rnorm( 10, mean = 7, sd = 2 )\n)\n\n\n\nusing R!\ndata summary\n\n\n# basic summary\nsummary(iris)\n\n\n  Sepal.Length    Sepal.Width     Petal.Length  \n Min.   :4.300   Min.   :2.000   Min.   :1.000  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600  \n Median :5.800   Median :3.000   Median :4.350  \n Mean   :5.843   Mean   :3.057   Mean   :3.758  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100  \n Max.   :7.900   Max.   :4.400   Max.   :6.900  \n  Petal.Width          Species  \n Min.   :0.100   setosa    :50  \n 1st Qu.:0.300   versicolor:50  \n Median :1.300   virginica :50  \n Mean   :1.199                  \n 3rd Qu.:1.800                  \n Max.   :2.500                  \n\n\n\ntable1::table1(~ Sepal.Length   + Sepal.Width   + Petal.Length  + Petal.Width |Species, data = iris  )\n\n\n\nsetosa(N=50)\nversicolor(N=50)\nvirginica(N=50)\nOverall(N=150)\nSepal.Length\n\n\n\n\nMean (SD)\n5.01 (0.352)\n5.94 (0.516)\n6.59 (0.636)\n5.84 (0.828)\nMedian [Min, Max]\n5.00 [4.30, 5.80]\n5.90 [4.90, 7.00]\n6.50 [4.90, 7.90]\n5.80 [4.30, 7.90]\nSepal.Width\n\n\n\n\nMean (SD)\n3.43 (0.379)\n2.77 (0.314)\n2.97 (0.322)\n3.06 (0.436)\nMedian [Min, Max]\n3.40 [2.30, 4.40]\n2.80 [2.00, 3.40]\n3.00 [2.20, 3.80]\n3.00 [2.00, 4.40]\nPetal.Length\n\n\n\n\nMean (SD)\n1.46 (0.174)\n4.26 (0.470)\n5.55 (0.552)\n3.76 (1.77)\nMedian [Min, Max]\n1.50 [1.00, 1.90]\n4.35 [3.00, 5.10]\n5.55 [4.50, 6.90]\n4.35 [1.00, 6.90]\nPetal.Width\n\n\n\n\nMean (SD)\n0.246 (0.105)\n1.33 (0.198)\n2.03 (0.275)\n1.20 (0.762)\nMedian [Min, Max]\n0.200 [0.100, 0.600]\n1.30 [1.00, 1.80]\n2.00 [1.40, 2.50]\n1.30 [0.100, 2.50]\n\n\n\n\n# plot relationship (what is happening here? )\nplot( Sepal.Length   ~ Sepal.Width , data = iris )\n\n\n\n\nmodel\n\n\nlibrary(\"tidyverse\")  # plotting\nlibrary(\"ggeffects\")  # plotting\nlibrary(\"ggplot2\")  # plotting\nlibrary(\"patchwork\") # arrange multiple plots\nlibrary(\"sjPlot\")  # tables and plots\n\n# basic model\nm1<- lm(Sepal.Length ~ Sepal.Width, data = iris)\nsummary(m1)\n\n\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5561 -0.6333 -0.1120  0.5579  2.2226 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   6.5262     0.4789   13.63   <2e-16 ***\nSepal.Width  -0.2234     0.1551   -1.44    0.152    \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8251 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\nresults\n\n\n# better summary method\nsjPlot::tab_model(m1)\n\n\n\n \n\n\nSepal.Length\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n6.53\n\n\n5.58 – 7.47\n\n\n<0.001\n\n\nSepal.Width\n\n\n-0.22\n\n\n-0.53 – 0.08\n\n\n0.152\n\n\nObservations\n\n\n150\n\n\nR2 / R2 adjusted\n\n\n0.014 / 0.007\n\n\n\n\n# plot the coefficients\nsjPlot::plot_model(m1)\n\n\n\n\ngraph predicted effects\n\n\n# plot the predicted relationship of Sepal Width on Sepal Length\np1 <- ggeffects::ggpredict(m1, terms = \"Sepal.Width\")\nplot(p1)\n\n\n\n\nWhat is the advantage of this graph?\n\n\npp1 <- plot(p1,\n            add.data = TRUE,\n            dot.alpha = .8,\n            jitter = .2)\npp1\n\n\n\n\ntry another model\n\n\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nsummary (m2 <- lm(Petal.Length ~ Petal.Width, data = iris)) \n\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.33542 -0.30347 -0.02955  0.25776  1.39453 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.08356    0.07297   14.85   <2e-16 ***\nPetal.Width  2.22994    0.05140   43.39   <2e-16 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4782 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\n\npp2<-plot(\n  p2 <- ggeffects::ggpredict(m2, terms = \"Petal.Width\"),\n  add.data = TRUE,\n  dot.alpha = .8,\n  jitter = .2\n) \npp2\n\n\n\n\n\n\n## arange plots\nlibrary(patchwork)\npp1 / pp2 + plot_annotation(title = \"two plots\", tag_levels = \"i\")\n\n\n\n\nAppendix: # symbol is for commenting code\nIn case you haven’t figured it out yet, the hash symbol # is for commenting:\n\n\nr_comments <- 200 # here I am creating the variable for the number of time Jack says R is great\n\njill_roll <- 199 # here I'm creating a variable for the number of times Jill rolls here eyes\n\noutcome <- log(r_comments) * sqrt(jill_roll) * pi # here I am illustrating some functions in r using the variables I just created\n\noutcome # print outcome\n\n\n[1] 234.8088\n\nRounding\nIt is often useful to round numbers:\n\n\nround(outcome, digits = 2) # illustrate the useful `round` function.\n\n\n[1] 234.81\n\n\n\n\n## to be continued\n```{.r .distill-force-highlighting-css}\n\n\n",
    "preview": "posts/2_1/syntax.png",
    "last_modified": "2021-03-31T13:53:54+13:00",
    "input_file": {},
    "preview_width": 1285,
    "preview_height": 478
  },
  {
    "path": "posts/1_1/",
    "title": "Course basics",
    "description": "Goals: (1) Download Rstudio (2) Get Git (3) Teach you the essentials of Rmarkdown (4) Integrate (1)-(3).",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-02-23",
    "categories": [],
    "contents": "\n\nContents\nTo do:\nDownload R\nDownload R-Studio desktop\nRead about Rmarkdown\nRecommended reading\nWho is this course for?\nHow will you benefit from this course?\n\nOur approach to teaching and learning\nWhat is R?\nIn a nutshell\nHistory\nPurpose\n\nWhat is R Studio?\nThe IDE\nA quick walk through R Studio\n\nGitHub\nWhat is Git/GitHub?\nInstall Git\nCreate a repository\nNext copy the location\nThen open a new project in Rstudio as a GitHub project\n\nThen open an Rmarkdown document, write and save it\nThen commit your Rmarkdown document by including a message, and “pushing” to GitHub\nNext commit!\nExtra information\n\nRMarkdown\nWhat is Rmarkdown?\nWhy is Rmarkdown useful?\nExtra information\nTips and tricks (JB)\nFinal thought: why quantitative psychology has to change (Why can’t I use SPSS, JK)\n\nFinal thought\nLecture slides\n\nTo do:\nDownload R\nR is freely available for download at: https://www.r-project.org/ Please make sure that you have a current version of R-studio desktop on your machine\n\n\n\nDownload R-Studio desktop\nRstudio desktop is also freely available for download at:\nhttps://rstudio.com/products/rstudio/download/#download\nPlease make sure that you have the current version of R-studio desktop on your machine\n\n\n\nRead about Rmarkdown\nRead Danielle Navarro’s brief account of Rmarkdown here\nRecommended reading\nThis is a thorough rmarkdown workshop. You might quickly feel lost. Don’t worry about. We only expect you to acquire the basics this week.\nRmarkdown workshop\n\n\n\n\n\n\n\n\n\nWho is this course for?\nFor those of you who always wanted to learn R but never thought they could, this course is for you.\nHow will you benefit from this course?\nYou’ll learn how to use R, and Github, and we’ll teach you the fundamentals of statistics with a focus thinking not on rules.\nYou’ll learn how to learn – that is, how to obtain the resources you need to address a problem at hand.\nBy the end of the course, you’ll know how to:\nData skills:\nperform your data analysis in R\ndocument your analysis and collaborate in GitHub\ncreate a publication-ready article, with tables and graphs\ncreate a free personal website on github.\nStatistical skills:\nlearn the importance of knowing your question\nlearn how to collect data that bears on your question\nlearn how to explore your data visually\nlearn how to avoid common modelling pitfalls\nlearn how to improve your inference using multi-level models\nOur approach to teaching and learning\n\n\nThis course is designed to provide you with basic understanding, useful tips, and some guide rails for learning. Our main task is to give you the confidence, and the inspiration, for independent learning.\nWhat is R?\nIn a nutshell\nR is a free programming language and software environment for statistical computing (for download links see here: Windows, Mac).\nHistory\nR is the brainchild of Ross Ihaka and [Robert Gentleman](https://en.wikipedia.org/wiki/Robert_Gentleman_(statistician). It was created at the University of Auckland, where Ross Ihaka remains a professor of statistics.\nPurpose\nR was conceived to be a flexible language for data analysis usable by researchers. Since the initial beta release of R in 2000 the language has gained substantial popularity inside and outside of academia (have a look at this blog post for an interesting analysis). New versions of R are released periodically and can be downloaded and installed to replace the older R version.\nWhat is R Studio?\nThe IDE\nThe are many ways for using R on your computer. For the purposes of getting started, we will be using the Integrated Development Environment (IDE): R Studio.\nR Studio provides an interface with a number of user-friendly options, including a separate console and editor that has various help and syntax-auto-complete functions, and various tools for plotting, history, data visualization, debugging and work space management. It is important to remember that R and R Studio are not the same thing.\n\nA quick walk through R Studio\n\n\nGitHub\nWhat is Git/GitHub?\nGithub is a version control system. It is similar to Google docs, though for code. It is useful for collaboration because code easily breaks. It is only rarely possible to simultaneously work in real time on the same code because it will eventually break. Where and how is not easy to assess.\nA second function of GitHub is that it allows us to reconstruct histories of analysis. This is critical for open and reproducible science. This is the main function that we will be examining here.\nA third function, which pertains to single users, is that when writing code you can rewind and recover from your mistakes. This will save you a whole lot of time in the end.\nNote that GitHub has an interface with Rstudio. You will be using GitHub with Rstudio throughout this course.\nInstall Git\nWe suggested installing the educational version because this will allow you to have private repositories.\nIf you haven’t done that, but want to get started you can open a free account and retrospectively add your educational account later.\nPRO TIP Pick a user name that will be OK for professional purposes. If in doubt use your name.\nDirections for installing Git can be found http://github.com\nCreate a repository\nFirst create a repository on GitHub\n\n\n\nNext copy the location\n\n\n\nThen open a new project in Rstudio as a GitHub project\n\n\n\nThen open an Rmarkdown document, write and save it\nFirst, make sure that Rmarkdown is installed:\n\n\n# run this code\nif (!requireNamespace(\"devtools\"))\n  install.packages('devtools')\ndevtools::install_github('rstudio/rmarkdown')\n\n\n\nNext, create a document\n\n\n\nMake sure you save your document\nPress ⌘ + S  is the command for “save”\nThen commit your Rmarkdown document by including a message, and “pushing” to GitHub\nNote that we don’t want want to push .Rproj files to GitHub (this will mess up your collaborations), so I edited my .gitignore file.\nTypically you won’t want to be pushing large html files back and forth to GitHub (that can cause GitHub to freeze).\nYou can edit your gitingore file by adding a * like so:\n/*.html\nsee: https://git-scm.com/docs/gitig\nNext commit!\n\n\n\nVoila!\nExtra information\nJB’s recommendations for using Git and Rstudio\nThis is a very good tutorial on github and Rstudio: link\nVideo link\nA very brief setup video for Mac Users Link\nJK’s recommendations for using Git and Rstuio\nsetup\nRMarkdown\nWhat is Rmarkdown?\nIn the example above we breezed through Rmarkdown without exampling it. What is Rmarkdown?\nRmarkdown is a format for combining data-analysis with ordinary writing using a simple markup language.\nThe Rmarkdown code we used to write the opening paragraph looks like this.\n\ndiv.blue{ background-color:#e6f0ff}\n\n## To do\nRead Danielle Navarro's brief account of \nRmarkdown [here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThe:\n\n##\n\nmakes a heading. Then we write as we ordinarily would write:\n\nRead Danielle Navarro’s brief account of Rmarkdown\n\nand we include a link by typing\n\n[here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThink of rmarkdown as writing in word but without having to use your mouse all the time, and you can write up your analysis while writing in a one0stop shop.\nRmarkdown is just an efficient method for composing text without having to reach for your mouse, and a way of documenting and reporting your analysis\nWhy is Rmarkdown useful?\nRmarkdown merges two very powerful ideas: 1. R as a coding based tool to make your analysis repeatable; 2. markdown an approach to writing text that allows for the direct embedding of code output.\nThis is an immensely powerful approach that can be used for everything, from writing research papers, to writing complex technical documentation. This website is written Rmarkdown.\nYou will be creating a website similar to Johannes Karl’swebsite and Joseph Bulbulia’s websiteand you will do this by written using Rmarkdown).\nYou might think that writing in R markdown is only a nice technical trick for people really into coding, but in reality it addresses a central problem of statistical analysis.\nThe majority of errors in quantitative research papers (some meta-researchers indicate values as high as 80%) are human errors in transcribing values from the statistical software they are using to the final document (for a marginally entertaining story around this issue see this post).\nExtra information\ncoding tips Rmarkdown website\nJB’s recommendation for a very short introduction to Rmarkdown: https://rpubs.com/bpbond/626346\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips and tricks (JB)\nOne day, someone might ask you to collaborate in \\(\\LaTeX\\) (pronounced “Lay-Tek”). \\(\\LaTeX\\) is a document preparation system developed by Leslie Lamport in the 1980s that uses \\(\\TeX\\), a typesetting system that Donald Knuth developed in the 1970s to create mathematical documents. Writing in LaTeX is only a little more complicated than writing in markdown. For example, instead of writing # Heading, ## Subheading, ## Subsubheading, you would write \\section{Heading} \\subsection{Subheading}, \\subsubsection{Subsubheading}. However, the principles of mouseless composition that make Rmarkdown so nice, also make LaTeX nice. Rmarkdown shares features for bibliographic referencing with LaTeX that we’ll cover in later weeks. For now, since we are teaching you about Rmarkdown, we thought it’d be useful to teach about LaTeX too. Stay tuned for more.\nFinal thought: why quantitative psychology has to change (Why can’t I use SPSS, JK)\nQuantitative psychology has long struggled with replicability of it’s results both in substantive and also statistical areas. Concerns around these topics have already been raised on works by authors such as Joseph Banks Rhine the founder of modern parapsychology in the 1930s. Numerous authors, even at the time criticized both methods of the experiment and of the analysis [@gulliksenExtraSensoryPerceptionWhat1938]. In modern times, Deryl Bem’s article “Feeling the Future” that reported evidence in favor of Extra Sensory Perception revived this debate and led to an increased uptake of Open Science methods. Importantly, this is not only an issue in psychology, but instead affects all quantitative fields such as biology, chemistry, and physics. Out of the many issues that are addressed as part of the open science movement (if you are interested in getting active in it have a look at ANZORN) we will focus mostly on aspects of reproducability in analysis.\nUntil recently IBMs SPSS (Statistical Package for the Social Sciences), which originally launched in 1968 dominated the research space in psychology. If you never had the fortune of working with SPSS this is what it looked like:\n\n\n\nSPSS presented the user with a GUI (Graphical User Interface) through which they could run tests on their data. The big issue was that each statistical test has many different options researchers can choose (you will often hear people talk about researcher degrees of freedom) and a GUI makes it very difficult to accurately record every small setting a researcher has chosen. As a work around researchers could either store their output of the analysis which recorded some settings, but even for moderately complex analysis this output could stretch in the hundredth of pages. Alternatively, researchers could save the underlying code that SPSS used, but this was also very clunky and extremely arcane to understand. To give you a sense of scope below you see a snippet from a widely used analysis in SPSS aimed at examining the similarity of factor structures across groups. This code has a total of 130 lines that researchers would have needed to largely enter by hand and double check for any potential coding errors.\n\n\n\nAdditionally, some changes made by researchers were extremely difficult to account for. For example, when a researcher re-coded a variable say reversing its direction there was no way of knowing that this had taken place if you later looked at the data set. Together with the rise in complex analysis in psychology this has led to a steady decline in the use of SPSS and most psychology departments, as well as private, and governmental stake holder now require a certain fluency in R or similar coding based languages.\nFinal thought\nR, Rstudio, GitHub, R-markdown \\(\\dots\\) these are just tools that fit our task\nOur main task in this course is to develop statistical intuition and workflows that will enable you to do better science.\nLecture slides\nClick here to go to the lecture slides.\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\n\n\n\n",
    "preview": "posts/1_1/Rlogo.png",
    "last_modified": "2021-04-10T13:11:44+12:00",
    "input_file": {},
    "preview_width": 724,
    "preview_height": 561
  },
  {
    "path": "posts/1_2/",
    "title": "R basics",
    "description": "Some fundamentals",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-02-23",
    "categories": [],
    "contents": "\n\nContents\nFundamentals of RUsing R as calculator\nThe basic structure of R commands\nRecommended reading\n\n\nFundamentals of R\nA consoleruns all your code in R\nA source page runs all your code in R.\nA working directory is where R will look for raw data and other material.\nIn R-studio, (for starters) you can can use the file tab to import and save your material.\nSimilarly, in R-studio, (for starters) you can can use install packages by clicking the Install tab in the package pane (generally lower right pane) to install packagees.\nUsing R as calculator\nWe can use R as a calculator. You can run any mathematical operation you would normally use by entering it into the console:\n\n\n## Addition\nx = 3 + 2\nx\n\n\n[1] 5\n\n## Subtraction\nx = 3 - 2\nx\n\n\n[1] 1\n\n## Multiplication\nx = 3 * 2\nx\n\n\n[1] 6\n\n## Division\nx = 3 / 2\nx\n\n\n[1] 1.5\n\n## Modulus (Remainder from division)\nx = 3 %% 2\nx\n\n\n[1] 1\n\n## Exponentiation\nx = 3 ^ 2\nx\n\n\n[1] 9\n\n## Integer Division (Number of times denominator fits in numerator)\nx = 3 %/% 2\nx\n\n\n[1] 1\n\nThe basic structure of R commands\nWhile using R as calculator might be interesting, it does not get us very far in analysing our data.\nTo really unlock the full potential of R we first need to understand the basic structure of most R code and learn some terms.\nThe four main elements of every R code are:\nobjects,\nfunctions,\narguments\noperators.\nFigure 1 provides a simple example, that produces a new object which contains the mean of variable x.\n\n\n\nFigure 1: The Basic Syntax of R\n\n\n\nThe function mean generates the arithmetic mean of an input object.\nThe object needs to be specified inside of the function brackets as the x argument, in this case we define x = df .\nLast, we assign the result of this function for later us via the <- operator to an object which we decided to call object.\nIn other words, we create a new object that can be further manipulated and contains information about the mean of a previously created object “x”. This structure represents the foundation of most operations in R.\nInstead calling the mean function as above one could manually add all values of x dividing it by the number of x values.\nNevertheless, this would be very cumbersome.\nFor this reason, functions (pre-assembled lines of code) exist to reduce the amount of coding necessary.\nThese functions can be bundled into packages. R’s capacity for creating packages is main appeal of R as a statistical tool because community developed functions are available from a central repository called CRAN in the form of packages.\nThese packages can be installed in R with the command install.packages(“package name”).\nIt is important that you only need to install a package once on your machine, expect if you want to upgrade the package. Generally speaking you regularly want to upgrade your packages, but keep a permanent note in your code which version of a package you used when it was initially written.\nRecommended reading\nAn introduction to R and Rstudio\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-19T18:35:55+13:00",
    "input_file": {}
  },
  {
    "path": "posts/1_3/",
    "title": "Set up",
    "description": "The purpose of this week is to get you set up to write in R markdown",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-02-23",
    "categories": [],
    "contents": "\n\nContents\nTO DO\nMarkdown is a format for writing\nRmarkdown and You\nHow to keep track of everything\nSome useful sites\n\nTO DO\nRead Daniell Navarro’s brief account of Rmarkdown here\nMarkdown is a format for writing\nRmarkdown is a format for combing code with ordinary writing.\nThe Rmarkdown code we used to write the opening paragraph looks like this.\n\ndiv.blue{ background-color:#e6f0ff}\n\n## TO DO\nRead Danielle Navarro's brief account of \nRmarkdown [here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThe:\n\n##\n\nmakes a heading. We write as ordinary,\n\nRead Danielle Navarro’s brief account of Rmarkdown\n\nand we include a link by typing\n\n[here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThink of rmarkdown as writing in word but without having to use your mouse all the time. The really great thing about Rmarkdown is that you can write document and do the analysis in a single stop shop. Figure 1 shows Rmarkdown in the rconsole (upper left).\n\n\n\nFigure 1: Screenshot of Rmarkdown document (upper left)\n\n\n\nRmarkdown and You\nRmarkdown merges two very powerful ideas: 1. R as a coding based tool to make your analysis repeatable; 2. markdown an approach to writing text that allows for the direct embedding of code output.\nThis is an immensely powerful approach that can be used for everything, from writing research papers, to writing complex technical documentation. This website is written Rmarkdown.\nYou will be creating a website similar to Johannes Karl’swebsite and Joseph Bulbulia’s websiteand you will do this by written using Rmarkdown).\nYou might think that writing in R markdown is only a nice technical trick for people really into coding, but in reality it addresses a central problem of statistical analysis.\nThe majority of errors in quantitative research papers (some meta-researchers indiate values as high as 80%) are human errors in transcribing values from the statistical software they are using to the final document (for a marginally entertaining story around this issue see this post).\nHow to keep track of everything\nNow that we have our repeatable code, our repeatable document, the last thing we need is a transparent way to document what we are doing and share with others. For that we come to our last tool that in a similar confusing way to R and Rstudio is split in to parts; git and Github.\nSome useful sites\ncoding tips Rmarkdown website\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-19T18:36:56+13:00",
    "input_file": {}
  }
]
