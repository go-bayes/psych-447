[
  {
    "path": "posts/12_1/",
    "title": "Bayesian inference and consolidation of statistics",
    "description": "\"With an excursion into the whens and when nots of mediation\"",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-25",
    "categories": [],
    "contents": "\n\nContents\n\n\n\n\n\nProbability\nWe have been working with probability throughout this course.\nSuppose there is a test that is 99% accurate at detecting COVID if you have it.\nVery rarely it throws up a false positive,say one in a thousand.\nYou just tested positve. What is the probability that you have COVID? Our intuition is that we probably have COVID. However, let’s assume COVID is rare. Currently in NZ, there are about 50 cases, so 1 in 100,000. The background rate matters.\nBayes rule says\n\\[ Pr(COVID|Positive) = \\frac{Pr(Positive|COVID)\\times Pr (COVID}{Pr(Positive)}\n\\]\nWe plug in the numbers:\nPr_Positive_COVID <- 0.99 Pr_Positive_Healthy <- 0.01 Pr_COVID <- 0.00001\nCalculate the background probability of testing positive\nPr_Positive <- Pr_Positive_COVID * Pr_COVID + Pr_Positive_Healthy * ( 1 - Pr_COVID )\nNow calculated your probability of testing positive\nPr_COVID_Positive <- Pr_Positive_COVID * Pr_COVID / Pr_Positive Pr_COVID_Positive\nUncertainty\nFuture Horizons\n\n\n\n",
    "preview": "posts/12_1/distill-preview.png",
    "last_modified": "2021-03-22T22:51:50+13:00",
    "input_file": "lecture_12.utf8.md",
    "preview_width": 1023,
    "preview_height": 876
  },
  {
    "path": "posts/11_1/",
    "title": "Missing data and measurement error",
    "description": "\"With an excursion into the whens and when nots of mediation\"",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-18",
    "categories": [],
    "contents": "\n\nContents\n\n\n\n\n\nRemember: parameters are not observed\nMeasurement error\nMissing eata\n\n\n\n",
    "preview": "posts/11_1/distill-preview.png",
    "last_modified": "2021-02-23T15:06:35+13:00",
    "input_file": {},
    "preview_width": 1106,
    "preview_height": 553
  },
  {
    "path": "posts/10_1/",
    "title": "Longitudinal data: within and between-individual effects",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-11",
    "categories": [],
    "contents": "\n\nContents\nLongitudional data and the multi-level model\nMediation from the vantage point of causal inference (when and when not)\n\nLongitudional data and the multi-level model\n\n\n\nMediation from the vantage point of causal inference (when and when not)\n\n\n\n",
    "preview": "posts/10_1/op2.png",
    "last_modified": "2021-02-23T15:06:10+13:00",
    "input_file": {},
    "preview_width": 1800,
    "preview_height": 600
  },
  {
    "path": "posts/9_1/",
    "title": "Introduction to multilevel models: group-level slopes",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-04",
    "categories": [],
    "contents": "\n\nContents\nCovariance\n\nCovariance\n\n\n\n\n\n\n",
    "preview": "posts/9_1/op.png",
    "last_modified": "2021-02-23T15:05:58+13:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1536
  },
  {
    "path": "posts/8_1/",
    "title": "Introduction to multilevel models: group-level intercepts",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-27",
    "categories": [],
    "contents": "\n\nContents\nPartial pooling\nRandom intercepts\n\nPartial pooling\nRandom intercepts\n\n\n\n\n\n\n",
    "preview": "posts/8_1/op2.png",
    "last_modified": "2021-02-23T15:05:36+13:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1536
  },
  {
    "path": "posts/7_1/",
    "title": "Modelling binary, count data, and ordinal data",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\n\nContents\nBinary Data\nCount data\nZero infalted data\nOrdinal data\n\nBinary Data\nCount data\nZero infalted data\nOrdinal data\n\n\n\n\n\n\n",
    "preview": "posts/7_1/op.png",
    "last_modified": "2021-02-23T15:05:19+13:00",
    "input_file": {},
    "preview_width": 1000,
    "preview_height": 1000
  },
  {
    "path": "posts/6_1/",
    "title": "An introduction to directed acyclic graphs ",
    "description": "\"Control is killing your model\"",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-30",
    "categories": [],
    "contents": "\n\nContents\nWhat is causal confounding?\nThe difference between prediction and explanation?\n\nWhat is causal confounding?\n\n\n\nThe difference between prediction and explanation?\n\n\n\n",
    "preview": "posts/6_1/op2.png",
    "last_modified": "2021-02-23T15:05:04+13:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 800
  },
  {
    "path": "posts/5_1/",
    "title": "Elements of a linear model",
    "description": "\"What is a statistical model?\"",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-23",
    "categories": [],
    "contents": "\n\nContents\n\n\n\n\nShow code\nknitr::include_graphics(\"op.png\")\n\n\n\n\nShow code\n# libraries\nlibrary(\"tidyverse\")\nlibrary(\"patchwork\")\nlibrary(\"brms\")\nlibrary(\"lubridate\")\nlibrary(\"splines\")\nif (!require(equatiomatic)) {\n  remotes::install_github(\"datalorax/equatiomatic\")\n  }\n# set theme\n# theme set\ntheme_set(theme_classic())\n\n\n\nShow code\n\n# Import data\n# read data\nnz_0 <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"))\n\n# to relevel kessler 6 variables\nf<-c(\"None Of The Time\",\"A Little Of The Time\",\"Some Of The Time\",  \"Most Of The Time\", \"All Of The Time\")\n\n# get data into shape\nlibrary(\"tidyverse\")\nnz <- nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(\n    -c(\n      SWB.Kessler01,\n      SWB.Kessler02,\n      SWB.Kessler03,\n      SWB.Kessler04,\n      SWB.Kessler05,\n      SWB.Kessler06\n    )\n  ) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\n  dplyr::mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\n  dplyr::mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\n  dplyr::mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\n  dplyr::mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\n  dplyr::mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) %>%\n  dplyr::mutate(height_m = HLTH.Height * 100,\n         weight_kg =  HLTH.Weight) # better height vars\n\n\n\nOverview\nIn Part 1 we will introduce regression.\nIn Part 2 we will:\nlearn how to use R to generate random numbers\nuse random numbers to simulate data\nLearning outcomes\nBy learning regression, you will better equipped to do psychological science and to evaluate psychological research.\nBy beginning to learn how to simulate data you will better understand how what a regression model is doing, to evaluate your regression model, and to plan research. Note, that we will not expect you to simulate a dataset just yet. Instead, we will illustrate the use of tools for simulation, and ask that you try out very basic simulation functions. It will not be until after the April break that you will simulate any data for the purposes of assisting in statistical inference.\nPart 1: What is regression?\nBroadly speaking, a regression model is method for inferring the expected average features of a population, and the variance of a population, conditional on other features of the a population as embodied in measurements obtained from a sample.\nWe’ll see that regression encompasses more than this definition, however, the definition makes a start.\nTo understand regression we need to understand the following jargon words: population, sample, measurement, and inference.\nWhat is a population?\nIn science, a population is a hypothetical construct. It is the set of all potential members of a set of things. In psychological science that set is typically a collection of individuals. We want to understand “The population of all human beings?” or “The New Zealand adult population”; or “The population of undergraduates who may be recruited for IPRP in New Zealand.”\nWhat is a sample?\nA sample is a randomly realised sub-population from the larger abstract population that a scientific community hopes to generalise about.\nThink of selecting balls randomly from an urn. If pulled at random, the balls may inform us about the contents of the urn. For example, if we select one white ball and one black ball, we may infer that the balls in the urn are not all white or all black.\nWhat is “measurement?”\nA measure is tool or method for obtaining numerical descriptions of a sample. We often call measures “scales.” We can think of a bathroom weight scale as a tool and method for tracking body weight.\nA measurement is the numerical description we obtain from sensors such as statistical surveys, census data, twitter feeds, & etc.\nIn the course, we have encountered numerical scales, ordinal scales, and factors. The topic of measurement in psychological is, to say the least, very broad.\nFor now, it is important to keep in mind that, similar to bathroom scales, measures can be prone to error.\nAlso similar to bathroom scales, error prone scales may nevertheless be useful. We need to investigate the utility of error prone scales against the backdrop of specific interests and purposes.\nWhat is a parameter?\nIn regression, we combine measurements on samples with probability theory to guess about the properties of a population we will never observe. We call these properties “parameters.”\nWhat is statistical inference?\nThe bulk of statistical inference consists of educated guessing about population parameters.\nProbability distributions and statistical guessing\nInference is possible because the parameters of naturally occurring populations are structured by data generating processes that are approximated by probability distributions. A probability distribution is a mathematical function that describes the probability of a random event. Today we will be focusing on height.1\nToday we will be talking about the “normal” or “Gaussian distribution.” A very large number of data-generating processes in nature conform the normal distribution.\nLet’s consider some examples of randomly generated samples, which we will obtain using R’s rnorm function.\n100-person sample of heights\n\n\nShow code\nset.seed(123)\nsm<-rnorm(100, mean = 170, sd = 20)\nggplot2::qplot(sm, binwidth = 10)\n\n\n10-person sample of heights\n\n\nShow code\nset.seed(123)\nsubsm <-rnorm(10, mean = 170, sd = 20)\n\nggplot2::qplot(\n  subsm, binwidth = 10\n  )\n\n\n10000-person sample of heights\n\n\nShow code\nset.seed(123)\nlargesm <-rnorm(1e5, mean = 170, sd = 20)\nggplot2::qplot(\n  largesm, binwidth = 1\n  )\n\n\nHow can I use regression to infer a population parameter?\nWe can use R to investigate the average height of our imaginary population from which the preceding samples were randomly drawn. We do this in R by writing an “intercept-only” model as follows:\n\n\nShow code\nmodel <- lm(outcome ~ 1, data = datset)\nsummary(model)\n\nUsing the previous simulations:\nN = 10 random draws\n\n#write the model and get a nice table for it\nsjPlot::tab_model(\n  lm(sm ~ 1)\n)\n\n \n\n\nsm\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n171.81\n\n\n168.19 – 175.43\n\n\n<0.001\n\n\nObservations\n\n\n100\n\n\nR2 / R2 adjusted\n\n\n0.000 / 0.000\n\n\nN = 100 random draws\n\nsjPlot::tab_model(\n  lm(subsm ~ 1)\n)\n\n \n\n\nsubsm\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n171.49\n\n\n157.85 – 185.14\n\n\n<0.001\n\n\nObservations\n\n\n10\n\n\nR2 / R2 adjusted\n\n\n0.000 / 0.000\n\n\nN = 10,000 random draws\n\n\nShow code\nsjPlot::tab_model(\n  lm(largesm ~ 1)\n)\n\n \n\n\nlargesm\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n170.02\n\n\n169.90 – 170.14\n\n\n<0.001\n\n\nObservations\n\n\n100000\n\n\nR2 / R2 adjusted\n\n\n0.000 / 0.000\n\n\nWhat do we notice about the relationship between sample size the estimated population average?\n\n\nShow code\nsjPlot::tab_model(\n   lm(sm ~ 1),\n   lm(subsm ~ 1),\n   lm(largesm ~ 1)\n)\n\n \n\n\nsm\n\n\nsubsm\n\n\nlargesm\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\nEstimates\n\n\nCI\n\n\np\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n171.81\n\n\n168.19 – 175.43\n\n\n<0.001\n\n\n171.49\n\n\n157.85 – 185.14\n\n\n<0.001\n\n\n170.02\n\n\n169.90 – 170.14\n\n\n<0.001\n\n\nObservations\n\n\n100\n\n\n10\n\n\n100000\n\n\nR2 / R2 adjusted\n\n\n0.000 / 0.000\n\n\n0.000 / 0.000\n\n\n0.000 / 0.000\n\n\nRegression with a single co-variate\nDoes mother height predict daughter height? It seems so. By what how close are is the relationship?\nFrancis Galton is credited with inventing regression. Galton observed that the height of offspring tends to fall between parental height and the population average, what Galton termed: “regression to the mean.” Galton sought a method for educated guessing about heights, and this led to fitting a line of regression by a method called “least squares” (For a history see: here).\nThis following dataset is from “The heredity of height,” Karl Pearson and Alice Lee (1903)(Pearson and Lee 1903). I obtained the dataset from (Gelman, Hill, and Vehtari 2020). Let’s use this dataset to investigate the relationship between a mother’s height and a daughter’s height.\n\n\nShow code\nmd_df <- data.frame(read.table(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt\"), header=TRUE))\n# Center mother's height for later example\nmd_df <- md_df %>%\n  dplyr::mutate(mother_height_c = as.numeric(scale(mother_height, center = TRUE, scale = FALSE)))\ndplyr::glimpse(md_df)\nRows: 5,524\nColumns: 3\n$ daughter_height <dbl> 52.5, 52.5, 53.5, 53.5, 55.5, 55.5, 55.5, 55…\n$ mother_height   <dbl> 59.5, 59.5, 59.5, 59.5, 59.5, 59.5, 59.5, 59…\n$ mother_height_c <dbl> -2.9987328, -2.9987328, -2.9987328, -2.99873…\n\nPearson and Lee collected 5,524 observations from mother/daughter height pairs. Let’s examine the data, first by plotting the relationship.\nWhat what is happening here?\n\n\nShow code\nexplore_md <-ggplot2::ggplot(data = md_df, aes(y = daughter_height, x = mother_height)) + \n  geom_jitter(alpha = .2) + \n  labs(title = \"The relationship between mothers height and daughter's height\") +\n       ylab(\"Daughter's height\") +\n       xlab(\"Mother's height\") + theme_classic()\nexplore_md\n\n\nIs there a linear predictive relationship between these two parameters? In regression we examine the line of best fit.\n\n\nShow code\nm1 <- lm(daughter_height ~ mother_height, data = md_df)\nsjPlot::tab_model(m1)\n\n \n\n\ndaughter_height\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n29.80\n\n\n28.25 – 31.35\n\n\n<0.001\n\n\nmother_height\n\n\n0.54\n\n\n0.52 – 0.57\n\n\n<0.001\n\n\nObservations\n\n\n5524\n\n\nR2 / R2 adjusted\n\n\n0.252 / 0.252\n\n\nWe can plot the coefficient; in a model with one predictor isn’t too informative. As we continue in the course, however, we’ll see that plotting coefficients can be easier than deciphering the numbers in tables. Here are two methods for plotting.\n\n\nShow code\nt_m1<-parameters::model_parameters(m1,  \n                                   ci = 0.95)\nmethod1 <- plot(t_m1) +\n  labs(title = \"The relationship between mothers height and daughter's height\") + \n  ylab(\"Daughter's height\") \n\nmethod2 <-sjPlot::plot_model(m1)\n\nlibrary(patchwork)\nmethod1 / method2 + plot_annotation(title = \"Comparision of two coefficeint plots\",\n                                    subtitle = \"a: parameters see; b: sjPlot\", \n                                    tag_levels = \"a\")\n\n\nHow do we interpret the regression model?\nLet’s write the equation out in mathematics. How do we read this?2\n\n\nShow code\nlibrary(\"equatiomatic\")\nextract_eq(m1,  use_coefs = FALSE)\n\\[\n\\operatorname{daughter\\_height} = \\alpha + \\beta_{1}(\\operatorname{mother\\_height}) + \\epsilon\n\\]\n\nThe math says that the expected daughter’s height in a population is predicted by the average height of the population when mother’s height is set to zero units (note, this is impossible - we’ll come back to this) plus \\(\\beta ~\\times\\) units of daughter’s height (inches) for each additional unit of mother’s height (inches)\nWe can plug the output of the model directly into the equation as follows:\n\n\nShow code\nlibrary(\"equatiomatic\")\nextract_eq(m1,  use_coefs = TRUE)\n\\[\n\\operatorname{\\widehat{daughter\\_height}} = 29.8 + 0.54(\\operatorname{mother\\_height})\n\\]\n\nGraph the relationship between mother’s and daughter’s heights\n\nlibrary(ggeffects)\ntoplot<-ggeffects::ggpredict(m1, terms = \"mother_height\")\n\nheightplot<-plot(toplot, add.data = TRUE, dot.alpha = .1, jitter = TRUE) +   theme_classic()\nheightplot + labs(title = \"Predicted values of daughter's height from the Pearson/Fox 1903 dataset\")\n\n\nRegression to predict beyond the range of a dataset\nJoyte Amge is the world’s shortest woman at 25 inches. Sandy Allen was the world’s tallest woman at 91 inches. What is be the expected heights of their daughter, and of every intermediary woman in between?\n\n# use the `expand.grid` command to create a sequence of points for mother's height\nndat<-expand.grid(mother_height = c(25:91)) \n\n# use the `predict` function to create a new response \npr<- predict(m1, type = \"response\", interval = \"confidence\", newdata =ndat)\n\n# have a look at the object\ndplyr::glimpse(pr)\n num [1:67, 1:3] 43.4 44 44.5 45.1 45.6 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:67] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : chr [1:3] \"fit\" \"lwr\" \"upr\"\n# create a new dataframe for the new sequence of points for mother's height and the predicted data\nnewdata<-data.frame(ndat,pr)\nhead(newdata)\n  mother_height      fit      lwr      upr\n1            25 43.42183 42.49099 44.35266\n2            26 43.96676 43.06065 44.87288\n3            27 44.51170 43.63030 45.39310\n4            28 45.05664 44.19995 45.91332\n5            29 45.60157 44.76960 46.43355\n6            30 46.14651 45.33924 46.95378\n\nGraph the predicted results\n\n\nShow code\n# graph the expected results\npredplot<-ggplot(data = newdata, \n       aes(x= mother_height, y = fit))  + \n  geom_point() +  geom_errorbar(aes(ymin = lwr, ymax = upr), width = .1) + \n   expand_limits(x = c(20,91), y = c(0,81))  + theme_classic() + \n  labs(title = \"Predicted values for a broader population\")\n\n# plot the two graphs together (making the x and y axis at the same scale )\nlibrary(\"patchwork\")\n# rescale heightplot\n\n# old plot with the new axis and y axis scales, and remove points\n\nheightplot2<-plot(toplot, add.data = FALSE) +   theme_classic()\n\nnhp <- heightplot2 +  expand_limits(x = c(20,91), y = c(0,81) ) +  labs(title = \"Predicted values of daughter's height from the Pearson/Fox 1903 dataset\")\n\n# double graph\n nhp /predplot  + plot_annotation(title = \"What do you notie about these relationships?\", tag_levels = \"a\")\n\n\nA simple method for obtaining the predicted values form your fitted model is to obtain the ggeffects output without producing a graph.\n\nlibrary(ggeffects)\ntoplot<-ggeffects::ggpredict(m1, terms = \"mother_height\")\ntoplot\n# Predicted values of daughter_height\n# x = mother_height\n\n    x | Predicted |         95% CI\n----------------------------------\n52.50 |     58.41 | [58.15, 58.66]\n54.50 |     59.50 | [59.29, 59.70]\n57.50 |     61.13 | [60.99, 61.27]\n59.50 |     62.22 | [62.13, 62.32]\n61.50 |     63.31 | [63.25, 63.38]\n63.50 |     64.40 | [64.34, 64.47]\n65.50 |     65.49 | [65.40, 65.59]\n70.50 |     68.22 | [68.01, 68.42]\n\nNon-linear relationships\nLinear regression assumes linearity conditional on a model. Often your data will not be linear!\nConsider the following example:\n\n# Simulate nonlinear relationship between x and y\nb <- c(2, 0.75)\nset.seed(12)\nx <- rnorm(100)\nset.seed(12)\ny <- rnorm(100, mean = b[1] * exp(b[2] * x))\ndat1 <- data.frame(x, y)\n\not1 <-lm(y ~ x, data  = dat1)\n# performance::check_model(ot1)\n\n# Plot linear effect\nplot(ggeffects::ggpredict(ot1, terms = \"x\"), add.data =TRUE, dot.alpha = .4)\n\n\nNon-linear relationship as modelled by a polynomial regression:\n\nlibrary(splines)\not2 <-lm(y ~ x + I(x^2), data  = dat1)\nplot(ggeffects::ggpredict(ot2, terms = \"x\"), add.data =TRUE, dot.alpha = .4)\n\n\nHere is another approach:\n\nlibrary(splines)\not2.b <-lm(y ~ x + poly(x, 2), data  = dat1)\nplot(ggeffects::ggpredict(ot2.b, terms = \"x\"), add.data =TRUE, dot.alpha = .4)\n\n\nNon-linear relationship as modeled by a general additive model (spline)\n\nlibrary(splines)\n\not3 <-lm(y ~ bs(x), data  = dat1)\n\n#performance::check_model(ot2)\nplot(ggeffects::ggpredict(ot3, terms = \"x\"), add.data =TRUE, dot.alpha = .4)\n\n\nCentering\nAny linear transformation of a predictor is OK. Often we center (or center and scale) all indicators, which gives us an intercept that is meaninful (the expected population average when the other indicators are set their average).\n\n\nShow code\nlibrary(ggeffects)\n# original model\nm1 <- lm(daughter_height ~ mother_height, data = md_df)\nmc <-lm(daughter_height ~ mother_height_c, data=md_df)\nsjPlot::tab_model(m1,mc)\n\n \n\n\ndaughter_height\n\n\ndaughter_height\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n29.80\n\n\n28.25 – 31.35\n\n\n<0.001\n\n\n63.86\n\n\n63.80 – 63.92\n\n\n<0.001\n\n\nmother_height\n\n\n0.54\n\n\n0.52 – 0.57\n\n\n<0.001\n\n\n\n\n\n\n\n\nmother_height_c\n\n\n\n\n\n\n\n\n0.54\n\n\n0.52 – 0.57\n\n\n<0.001\n\n\nObservations\n\n\n5524\n\n\n5524\n\n\nR2 / R2 adjusted\n\n\n0.252 / 0.252\n\n\n0.252 / 0.252\n\n\nGraph model\n\n\nShow code\nplot(ggeffects::ggpredict(mc, terms = \"mother_height_c\"), add.data =TRUE, dot.alpha = .4)\n\n\nNote: when fitting a polynomial or any interaction, it is important to center your indicators. We’ll come back to this point in later lectures.\nModel evaluation\nA simple way to assess your model fit is to compare a model with one covariate with a simple intercept-only model and to assess improvement in either the AIC statistic or the BIC statistic. The BIC is similar to the AIC but adds a penalty for extra predictors. An absolute improvement in either statistic of n > 10 is considered to be a better model.\nWe can use the performance package to generate a table that compares fits.\n\nlibrary(performance)\n# intercept only\nionly <- lm(daughter_height ~ 1, data = md_df)\n\n# covariate added\ncovadded <- lm(daughter_height ~ mother_height, data = md_df)\n\n# evaluate\nperformance::compare_performance(ionly, covadded)\n# Comparison of Model Performance Indices\n\nName     | Model |       AIC |       BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n----------------------------------------------------------------------------\nionly    |    lm | 26299.969 | 26313.203 | 0.000 |     0.000 | 2.615 | 2.615\ncovadded |    lm | 24698.514 | 24718.365 | 0.252 |     0.252 | 2.262 | 2.262\n\nWhat was the model improvement?\n\n\nShow code\n# improved fit\nBIC(ionly)- BIC(covadded)\n[1] 1594.839\n\nGenerate a report\nThis is easy with the report package\nFor example:\n\nreport::report_statistics(covadded)\nbeta = 29.80, 95% CI [28.25, 31.35], t(5522) = 37.70, p < .001; Std. beta = 1.37e-14, 95% CI [-0.02, 0.02]\nbeta = 0.54, 95% CI [0.52, 0.57], t(5522) = 43.12, p < .001; Std. beta = 0.50, 95% CI [0.48, 0.52]\n\nOr if you want a longer report\n\nreport::report(covadded)\n\nThough use statistically significant in place of significant. This will avoid misleading your audience into thinking your result is important, when what you intend to communicate is that it is reliable.\nAssumptions of regression\nFrom Gelman and Hill (Gelman and Hill 2006)\nValidity\nLinearity*\nIndependence of errors\nEqual variance of errors\n\nunequal variance does not affect the most important aspect of a regression model, which is the form of the predictors XB (p.46)\n\nNormality of errors (statistical independence)\n\nThe regression assumption that is generally least important is that the errors are normally distributed. In fact, for the purpose of estimating the regression line (as compared to predicting individual data points), the assumption of normality is barely important at all. Thus, in contrast to many regression textbooks, we do not recommend diagnostics of the normality of re-gression residuals. (Gelman and Hill 2006) p. 46\n\n\nA good way to diagnose violations of some of the assumptions just considered (importantly, linearity) is to plot the residuals versus fitted values or simply individual predictors.(Gelman and Hill 2006)\n\nCommon confusions\nCausal inference is tricky\nPeople use the work “effect” but that is not what regression gives us (by default)\n“Normality assumption”\nAs Gelman and Hill note, the “normality” assumption is the least important. And the assumption pertains to the normality of residuals\nStatistical independence\nThis will be the main reason we do multi-level modelling: to condition on dependencies in the data.\nLevels (wrong population)\nWe sample from undergraduates, but infer about the human population.\nPART 2: Functions for simulation\nrnorm\nrnorm is a R’s random number generator. Within this function:\nnspecifies the number of observations\nsd specifies the value of the standard deviation\nmean specifies the value of the mean\n\nset.seed(12345)\n# generate random numbers\nds <- rnorm(n = 1000, mean = 0, sd = 1)\ndplyr::glimpse(ds)\n num [1:1000] 0.586 0.709 -0.109 -0.453 0.606 ...\n\nWe can create a histogram:\n\np1 <- ggplot2::qplot(ds) + labs(title = \"1st random number list\")\np1\n\n\nWe use shorthand for generating numbers:\n\nset.seed(54321)\nds_0 <- rnorm(1000)\ndplyr::glimpse(ds_0)\n num [1:1000] -0.179 -0.928 -0.784 -1.651 -0.408 ...\n\nNote that the first and the second graphs differ:\n\n\nShow code\np2 <- ggplot2::qplot(ds_0) + labs(title = \"2d random number list\")\np1 + p2 + plot_annotation(\"The two graphs differ\", tag_levels = 'i')\n\n\nOr more formally we can ask R to test the equivalence:\n\nidentical(ds, ds_0)\n[1] FALSE\n\nBecause we want to have reproducible code, we will use the set.seed() function in R to ensure the same random numbers are generated each time.\n\nset.seed(123)\nt1 <-stats::rnorm(100)\nset.seed(123)\nt2 <-stats::rnorm(100)\n\n# test\nidentical(t1, t2)\n[1] TRUE\n\nrunif\nWe use r uniform to generate continuous data within a point range\n\n\nShow code\nset.seed(123)\nds1 <- runif(n =100, min = 0, max = 50)\ndplyr::glimpse(ds1)\n num [1:100] 14.4 39.4 20.4 44.2 47 ...\n\nShow code\nhist(ds1)\n\n\nSay we want to simulate a range of values between two endpoints. This is useful for simulating explanatory variables.\n{r.  code_folding=FALSE} set.seed(123) exp <- runif(n =100, min = 130, max = 220) dplyr::glimpse(exp) hist(exp)\nrep\nFrequently we’ll need to generate random factors. For this, R’s rep function, letters function, and LETTERS function make happy friends.\nHere’s how these functions work\nLower case letters:\n\nletters[1:3]\n[1] \"a\" \"b\" \"c\"\n\nUpper case letters:\n\nLETTERS[4:10]\n[1] \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n\nCreating sequences using each\n\nrep( letters[1:3], each = 3 )\n[1] \"a\" \"a\" \"a\" \"b\" \"b\" \"b\" \"c\" \"c\" \"c\"\n\nCreating sequences using times\n\nrep( letters[1:3], times = 3 )\n[1] \"a\" \"b\" \"c\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\"\n\nCreate a sequences using length.out\n\nrep( letters[1:3], length.out = 5 )\n[1] \"a\" \"b\" \"c\" \"a\" \"b\"\n\nCreating uneven sequences\n\nrep( letters[1:3], times = c(3, 1, 4) )\n[1] \"a\" \"a\" \"a\" \"b\" \"c\" \"c\" \"c\" \"c\"\n\ncombining each + times\n\nrep(letters[1:3], each = 2, times = 3)\n [1] \"a\" \"a\" \"b\" \"b\" \"c\" \"c\" \"a\" \"a\" \"b\" \"b\" \"c\" \"c\" \"a\" \"a\" \"b\" \"b\"\n[17] \"c\" \"c\"\n\nlength.out\n\nrep(letters[1:3], each = 2, length.out = 17)\n [1] \"a\" \"a\" \"b\" \"b\" \"c\" \"c\" \"a\" \"a\" \"b\" \"b\" \"c\" \"c\" \"a\" \"a\" \"b\" \"b\"\n[17] \"c\"\n\nNote length.out take priority over times – use length.out if you have a fixed vector length.\nseq\nCreate a vector of numbers of a specific length\n\nseq(from = 1, to = 45, by = 1)\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22\n[23] 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[45] 45\n\n17 unit steps:\n\nseq(from = 1, to = 45, by = 17)\n[1]  1 18 35\n\n17 steps:\n\nseq(from = 1, to = 45, length.out = 17)\n [1]  1.00  3.75  6.50  9.25 12.00 14.75 17.50 20.25 23.00 25.75 28.50\n[12] 31.25 34.00 36.75 39.50 42.25 45.00\n\nFlexible functions\nWe can use vectors within random number generation\n\nset.seed(123)\nvdf<-rnorm(n = 20, mean = c(0, 500, 1000, 10000), sd = c(5,50,100,1000))\n\n# we created a vector \ndplyr::glimpse(vdf)\n num [1:20] -2.802 488.491 1155.871 10070.508 0.646 ...\nqplot(vdf, binwidth=4)\n\n\nWhy Simulate?\nSimulate to assess result against noise\nRecall the relationship between mother’s heights and daughters heights in the Pearson/Fox dataset:\n\n\nShow code\n# recall this model\nexplore_md <-ggplot2::ggplot(data = md_df, aes(y = daughter_height, x = mother_height)) + \n  geom_jitter(alpha = .2) + \n  labs(title = \"The relationship between mothers height and daughter's height\") +\n       ylab(\"Daughter's height\") +\n       xlab(\"Mother's height\") + theme_classic()\nexplore_md\n\n\nWhat would a random relationship look like? Simulation can help us to address this question\n\nav_dh <-mean(md_df$daughter_height, na.rm=TRUE)\nsd_dh <-sd(md_df$daughter_height, na.rm=TRUE)\nav_mh <-mean(md_df$mother_height, na.rm=TRUE)\nsd_mh <-sd(md_df$mother_height, na.rm=TRUE)\n\n# number of obs\nN<- nrow(md_df)\n\n# fake data\nsim_dh = rnorm(N, av_dh, sd_dh)\nsim_mh = rnorm(N, av_mh, sd_mh)\nsim_df_md <- data.frame(sim_dh,sim_mh)\nfake_md <-ggplot2::ggplot(data = sim_df_md, aes(y = sim_dh, x = sim_mh)) + \n  geom_jitter(alpha = .2) + \n  labs(title = \"Fake data relationship between mothers height and daughter's height\") +\n       ylab(\"Daughter's height\") +\n       xlab(\"Mother's height\") + theme_classic() + \n  geom_smooth(method = lm)\n\n# real data\nexplore_md <-ggplot2::ggplot(data = md_df, aes(y = daughter_height, x = mother_height)) + \n  geom_jitter(alpha = .2) + \n  labs(title = \"The relationship between mothers height and daughter's height\") +\n       ylab(\"Daughter's height\") +\n       xlab(\"Mother's height\") + theme_classic() + \n  geom_smooth(method = lm)\n\n\nlibrary(patchwork)\nfake_md + explore_md  + plot_annotation(tag_levels = \"a\")\n\n\nUse fake data to explore a small sample\nCan appear to reveal relationships that are not there.\n\nset.seed(123)\n# no relationship between x and y\nx = rnorm(n = 10, mean = 0, sd = 1)\ny = rnorm(n = 10, mean = 0, sd = 1)\n\ndf<-data.frame(x,y)\nggplot2::ggplot(df,aes(y,x)) + geom_point() + geom_smooth(method=lm)\n\n\nSimulate a relationship between two variables\n\n### Simulate a relationship between two variables \nlibrary(ggplot2)\nN = 1000\nweight <-runif(N, min = 50, max =100)\nb <- rlnorm(N,.78,.1)\nb\n   [1] 2.403021 2.157979 2.034313 2.122099 2.438699 2.304824 2.468640\n   [8] 2.212028 2.272834 2.062986 2.317611 2.073767 1.892585 2.209573\n  [15] 2.650068 2.363377 2.451072 2.261177 2.052676 2.137797 2.122671\n  [22] 2.081586 2.340622 1.935303 2.378896 2.378370 1.935059 2.325533\n  [29] 2.781590 2.063242 2.373796 2.017340 2.437739 2.236657 2.573306\n  [36] 1.885330 2.170310 2.069501 2.138861 2.048365 2.006949 2.311444\n  [43] 1.956666 2.530465 1.937463 2.203634 2.300897 2.313296 2.116630\n  [50] 2.198885 2.401579 1.885802 2.017433 2.252499 2.086570 2.501775\n  [57] 2.333398 2.197272 1.876155 2.187173 2.113528 2.159260 1.938364\n  [64] 2.293011 1.966203 2.132677 2.266286 2.017075 2.312431 1.912381\n  [71] 1.647110 2.285299 2.372760 2.119999 2.294265 1.943340 2.153911\n  [78] 1.796514 2.454979 2.627390 2.428810 2.175515 2.174213 1.874596\n  [85] 2.360890 2.135982 2.042809 1.894202 2.117050 2.003897 2.096558\n  [92] 1.931390 2.582503 2.177984 2.429036 1.681742 2.084815 2.038984\n  [99] 1.930362 2.546350 1.893585 2.252046 2.374160 2.220692 1.998655\n [106] 2.396757 2.219005 1.961384 1.898749 2.687662 2.038368 1.812021\n [113] 2.300959 2.250209 1.905256 1.796256 2.156248 2.444742 2.324750\n [120] 2.076547 2.006880 2.241413 2.216070 2.323260 2.096816 2.386757\n [127] 2.007558 2.110544 2.349216 2.408483 1.797056 2.204981 2.318401\n [134] 1.886867 2.288880 2.008087 2.415787 2.302161 2.355859 2.207966\n [141] 2.378250 2.504406 2.655478 2.175287 1.742106 2.188360 2.226779\n [148] 2.147846 2.309033 2.413475 2.071352 2.118250 2.270010 2.064685\n [155] 2.201473 1.792891 1.950352 1.910232 2.002983 2.035354 2.266486\n [162] 2.406591 2.028429 1.974502 1.965666 2.092880 2.129947 2.289565\n [169] 2.112490 1.772075 2.161617 2.456454 2.457538 2.015976 1.868661\n [176] 2.789343 2.146327 2.160317 2.275176 1.856320 2.028260 1.870032\n [183] 2.035396 2.207554 1.903185 2.314048 2.245514 1.992875 2.231407\n [190] 2.350923 2.425674 2.135531 2.161141 2.162638 2.519716 2.441243\n [197] 2.371304 2.119682 2.264432 2.271247 1.965669 1.835230 2.326077\n [204] 1.872115 2.181840 2.236752 2.308012 2.223189 2.027320 2.407615\n [211] 2.595719 2.382423 1.796131 2.509184 2.169278 2.299040 2.321476\n [218] 2.160482 2.165115 2.415523 2.342363 2.408553 2.768464 2.331336\n [225] 2.227184 1.748812 2.855284 2.078678 2.766197 2.264750 2.544268\n [232] 2.157670 2.295951 2.228650 2.141246 2.155366 2.413996 2.137964\n [239] 1.779321 2.139155 2.302462 2.320182 2.320208 1.841886 2.263414\n [246] 2.403163 2.478511 2.132945 2.112370 2.531429 1.846343 2.088230\n [253] 2.283584 1.855627 2.243333 2.632112 2.180587 2.121566 2.287572\n [260] 2.121435 2.366330 2.387970 2.182060 1.939307 1.912054 2.055872\n [267] 2.362542 1.793519 1.806457 2.043414 2.269228 1.991013 2.383750\n [274] 2.255422 2.144564 2.367614 2.267860 2.086330 2.232476 2.327399\n [281] 2.260596 2.042550 2.376242 2.448054 2.242581 2.213136 2.165037\n [288] 2.707814 2.242590 2.147213 1.697588 1.865393 2.164594 2.226942\n [295] 2.242715 2.368249 2.139527 2.463194 1.989430 1.933160 1.929192\n [302] 2.349564 2.163458 2.360756 2.123848 2.056100 2.102579 1.812557\n [309] 1.940680 1.888526 2.424032 2.054982 2.360671 2.538692 2.140036\n [316] 2.244287 1.831057 2.009997 2.193770 2.247703 2.021946 2.853328\n [323] 2.083733 2.195532 2.327929 2.175804 2.045502 2.421847 2.563964\n [330] 2.175004 2.307643 2.160325 2.414870 1.943292 2.751334 2.053708\n [337] 1.885353 2.106248 2.213712 2.566036 2.389588 2.212772 1.898476\n [344] 2.000498 2.146141 2.815958 1.811178 2.442704 2.069437 2.576931\n [351] 1.946592 2.213029 1.954325 2.387750 2.530401 2.651358 2.362594\n [358] 2.623021 2.471048 2.152893 2.288059 1.979413 2.141443 2.464765\n [365] 2.302806 2.283560 1.966365 2.053506 2.020893 2.269432 1.975752\n [372] 2.307591 1.951032 2.619158 2.284299 2.033787 2.234695 2.105925\n [379] 2.263958 2.235250 1.971094 2.015503 2.247817 2.569998 2.431387\n [386] 2.049392 2.369295 2.170903 2.248203 2.239015 2.822280 1.937641\n [393] 2.203599 1.825771 2.314014 2.434304 2.520775 1.799458 2.273401\n [400] 2.558285 2.093000 2.135680 2.173516 2.262571 2.331510 2.488754\n [407] 2.160741 2.224713 2.797706 2.277572 2.223039 1.907466 2.182095\n [414] 2.133721 2.179064 2.059490 2.036675 2.029770 2.135177 2.501307\n [421] 2.422763 2.104341 1.843025 2.004795 2.083864 2.204198 2.041611\n [428] 2.666237 2.122879 1.932096 2.150873 1.972817 2.215805 2.233039\n [435] 2.260438 1.854869 2.230155 2.250258 1.892482 2.400163 2.359423\n [442] 2.745497 2.215926 2.191691 2.202644 2.196745 1.813308 1.845753\n [449] 2.164623 2.058326 2.193446 1.766286 1.877855 1.953947 2.407541\n [456] 1.954532 2.013851 2.198966 2.112190 2.213648 2.746991 1.949435\n [463] 2.115842 2.071606 2.537653 2.019907 2.163639 2.360122 1.962346\n [470] 2.574145 2.333983 1.959284 2.282926 2.135433 2.250884 2.161932\n [477] 2.427960 1.905777 2.070393 2.127784 1.974707 1.966007 2.177553\n [484] 2.152828 1.690570 2.420701 2.236635 2.777693 2.336186 2.086116\n [491] 2.885618 2.895688 1.931175 2.286228 2.135873 2.222661 2.231679\n [498] 1.922853 2.244671 2.598475 2.145969 2.146218 2.508932 2.386528\n [505] 1.849935 2.231905 2.573726 2.513127 2.275034 2.344617 1.935385\n [512] 2.247938 1.982889 2.083811 2.395424 1.947041 2.240484 2.276942\n [519] 2.193484 2.617498 1.969472 2.317787 2.162158 2.125308 2.285098\n [526] 1.969855 1.912966 2.076226 2.599127 2.193671 2.254985 2.140448\n [533] 2.286562 1.983439 2.449273 2.312827 2.012454 2.193405 2.343471\n [540] 2.306597 2.529934 2.051767 2.439062 2.419727 2.146313 1.978635\n [547] 1.956360 2.283658 2.166011 2.606244 2.301391 2.101824 1.968842\n [554] 2.058052 2.257570 2.085287 2.296584 2.109743 2.158566 2.027795\n [561] 2.639275 2.255253 2.232369 1.841930 2.330258 1.969220 1.995407\n [568] 2.391293 2.084919 1.831550 2.603848 1.719888 2.310078 2.415062\n [575] 2.048081 2.280578 2.279402 2.420713 2.289675 2.128700 2.390731\n [582] 2.363308 1.986438 1.896332 2.216718 2.122519 1.976734 2.199859\n [589] 1.911714 2.216928 2.049318 2.400595 2.779994 1.990532 2.424842\n [596] 2.369111 2.166213 2.084722 2.553668 1.785064 2.045578 1.889506\n [603] 2.508115 2.140265 2.069967 2.999377 2.170584 2.086786 2.247878\n [610] 1.864807 2.291096 2.160595 2.286112 1.977361 1.969346 2.035331\n [617] 2.020209 2.484087 2.554648 2.147514 2.104560 2.110861 2.196628\n [624] 2.202714 2.245669 2.024519 2.004331 2.458884 2.065015 2.248593\n [631] 2.169080 1.982215 2.314298 2.219563 2.509236 2.207247 2.110332\n [638] 2.243034 1.937582 2.006537 2.295676 2.110000 2.167130 2.156481\n [645] 2.044082 1.782704 2.258912 2.354113 1.917704 2.530053 2.267132\n [652] 2.494689 1.982350 2.218160 2.159763 2.355730 2.059399 2.179271\n [659] 1.826012 2.018264 2.208919 2.032706 2.171988 2.081747 2.317973\n [666] 2.451865 2.009227 2.115511 2.519288 1.750861 2.112805 2.681752\n [673] 2.716540 2.215902 2.000984 2.217867 2.043620 2.522579 2.012448\n [680] 2.264358 2.320970 2.022327 2.375368 2.024266 2.323382 2.434317\n [687] 1.976160 2.437077 2.077254 2.246639 2.225950 2.090243 2.240749\n [694] 1.928914 2.151976 2.369263 1.755208 1.879879 1.942171 1.860957\n [701] 2.274951 1.975274 1.756890 2.046729 2.097899 2.376618 1.953504\n [708] 2.450100 2.270128 2.261968 2.003203 2.652140 2.145930 1.817588\n [715] 2.137452 1.797785 2.114776 2.091282 2.335400 2.413190 2.028688\n [722] 2.364605 2.515401 2.016948 2.043695 2.328159 2.221771 2.304532\n [729] 2.510466 2.267569 2.423397 2.321679 2.278146 2.267343 2.482168\n [736] 1.973432 1.953225 2.314502 2.155518 2.197676 2.349324 2.352149\n [743] 2.124917 2.114346 2.197587 2.426141 2.276415 2.517587 2.179807\n [750] 2.441388 2.382857 2.319168 2.273840 2.121263 2.157815 2.232093\n [757] 2.192163 2.397250 2.157755 2.166174 2.078337 2.151502 2.166523\n [764] 1.730874 1.903162 2.165717 2.124362 1.934624 1.787550 2.105521\n [771] 2.328792 2.604717 2.173099 2.532782 2.199659 2.206821 2.253495\n [778] 1.999590 2.170219 2.388928 2.023902 2.112431 1.944924 2.260159\n [785] 2.276138 2.327593 1.930963 2.204991 1.984951 2.181388 2.494934\n [792] 2.074349 2.343553 2.023914 2.079541 2.279309 2.038244 1.839886\n [799] 2.475684 2.261517 2.057797 1.787096 2.638527 3.061904 2.227206\n [806] 2.374960 2.465651 2.033624 2.106189 1.835447 2.026565 2.321564\n [813] 2.118966 2.135240 2.157055 1.810555 2.372070 1.888250 2.136444\n [820] 2.087868 2.134304 2.524383 2.058122 2.017159 1.873924 2.012606\n [827] 1.941346 2.272305 2.001105 2.248817 2.149762 1.890127 2.015645\n [834] 2.383360 2.387645 2.665942 2.180691 1.878394 2.020123 2.272428\n [841] 2.637980 2.205603 2.444984 2.355630 1.940976 2.144462 2.485630\n [848] 2.381212 2.285031 2.288076 2.076865 1.911831 2.483186 1.892650\n [855] 1.985976 2.323086 1.922796 2.064345 1.938123 2.321158 2.281040\n [862] 2.275474 2.280162 2.306485 2.325501 1.791644 2.149249 2.206144\n [869] 2.345427 1.937215 2.075164 1.959395 2.424739 2.479129 2.360250\n [876] 1.930463 2.282326 2.447446 2.218417 2.061408 1.956952 2.041061\n [883] 2.343123 2.089310 2.231695 2.483067 2.311354 2.500441 1.840141\n [890] 2.121095 2.195713 2.311413 1.940759 2.364623 2.249570 2.239787\n [897] 2.295265 2.156236 2.393016 2.327566 2.148949 2.420654 2.246236\n [904] 2.332347 2.055630 2.554977 2.180602 2.374494 2.159741 2.121317\n [911] 2.359486 1.861790 2.288363 2.269040 1.666069 2.263331 1.756210\n [918] 2.330261 2.084666 2.035021 2.179979 2.502537 2.047189 2.306682\n [925] 2.257179 1.938759 1.832897 1.787363 2.305108 2.173907 2.624938\n [932] 2.310277 2.374934 2.492879 2.074931 2.295636 2.379474 2.501612\n [939] 2.354351 2.275306 2.000061 2.346573 2.293380 2.324314 2.275875\n [946] 2.137883 2.164768 2.336692 2.219236 2.007699 2.119084 1.911881\n [953] 1.980396 2.150153 1.822459 1.842541 2.435754 2.059242 1.812724\n [960] 2.156990 2.489562 2.330832 2.279915 2.455608 2.019500 2.346603\n [967] 2.057088 2.181639 2.722217 2.403542 2.355612 1.952611 2.016526\n [974] 2.741243 1.955547 2.228766 2.385138 2.415426 2.432480 2.146175\n [981] 2.009531 2.115464 1.993297 2.322646 2.440091 2.698568 2.262819\n [988] 1.998750 2.416808 2.388047 2.130113 1.866779 2.354035 2.442237\n [995] 2.118036 2.301645 2.122110 2.335513 2.156035 2.107563\nsigma <- runif(N, 0 , 10 )\nheight = weight * b\n\nheight\n   [1] 137.30863 152.62804 143.79785 145.24125 140.52331 131.23739\n   [7] 152.19589 162.13751 143.86728 191.63360 121.19151 149.53933\n  [13] 170.23088 123.94592 206.83088 142.57441 138.18304 198.22699\n  [19] 194.49570 146.91613 176.72458 113.95024 161.96749 123.31589\n  [25] 215.84202 172.25537 175.12906 210.73857 249.55622 148.53605\n  [31] 208.23832 164.33465 208.44895 111.90273 189.82207 115.01634\n  [37] 149.73151 166.88155 144.56540 113.80057 124.79404 192.78082\n  [43] 138.69305 226.24835 106.83797 158.09894 228.35905 218.95942\n  [49] 199.64788 129.19025 135.77275 155.87118 135.52274 186.59229\n  [55] 137.75259 148.56679 207.94006 120.14629 137.59521 165.29623\n  [61] 169.08104 143.89560 144.27366 224.08150 145.78437 201.57528\n  [67] 216.93327 162.24695 163.10610 109.68409 159.38263 148.68484\n  [73] 125.84179 206.45893 197.37521 110.99333 166.85104 175.52762\n  [79] 194.61639 184.50984 200.12097 143.56446 142.16312 114.32858\n  [85] 161.66061 211.91284 117.89073 103.33286 120.87372 169.33003\n  [91] 169.74326 182.65099 216.02623 189.16639 184.74468 139.57098\n  [97] 189.90639 182.11000 191.08862 183.26482 124.19097 158.71010\n [103] 119.95052 131.44828 184.14901 147.53978 137.47844 105.59023\n [109] 118.26581 232.76972 188.28951 135.67755 159.67608 140.23853\n [115] 105.84618 124.83927 169.47413 148.74945 167.93620 126.46072\n [121] 150.74673 151.73300 182.82413 159.69087 142.10601 183.02700\n [127] 174.69107 128.85955 165.94226 152.41925 146.45761 130.51599\n [133] 216.03376 164.77704 190.92516 162.45604 165.75177 176.09639\n [139] 220.82432 174.62256 218.77137 164.34513 226.81640 137.58882\n [145] 138.87577 162.07980 140.84741 168.02496 220.88075 229.50636\n [151] 131.96241 139.96157 225.37123 167.23879 213.24725 131.46664\n [157] 137.19096 158.47570 115.40654 160.06703 140.37775 236.12979\n [163] 162.41284 149.57144 137.84954 196.75652 145.27217 147.47537\n [169] 123.64884 103.85880 160.18043 153.89251 149.44963 168.77517\n [175]  97.88643 237.21312 145.07972 152.18827 207.14924 178.10067\n [181] 130.06502 183.36648 175.89838 186.13817 100.18787 161.43030\n [187] 165.92612 155.46951 189.47567 225.18125 196.27964 152.52190\n [193] 166.63265 114.45527 158.85006 170.53935 142.01086 194.15516\n [199] 130.53176 204.80045 152.02742 152.53672 136.27302 152.86339\n [205] 143.11447 192.87001 161.43853 218.91257 199.42728 207.86174\n [211] 163.16904 145.54076 143.06593 159.02223 166.06587 205.22280\n [217] 135.58127 151.70896 159.30663 225.62272 225.53534 226.64215\n [223] 231.74630 227.32475 168.87011 137.85177 190.78027 140.03273\n [229] 141.07941 170.17479 238.02180 108.56325 123.06952 129.73094\n [235] 189.53604 186.99788 238.00499 156.76329  95.58374 176.35391\n [241] 202.45471 131.91465 162.01833 112.81414 119.72990 167.72791\n [247] 131.97181 130.73745 111.38837 211.40998 119.80386 114.92801\n [253] 122.38916 174.46981 196.76807 239.07547 216.11144 117.06799\n [259] 125.70689 190.80522 211.14466 120.52441 194.10138 167.69099\n [265] 155.84499 152.22818 136.63014  90.41267 131.19018 152.46867\n [271] 157.66447 145.80842 204.20145 119.00755 145.27092 213.41820\n [277] 208.15650 129.11768 151.13711 216.08566 209.53047 132.34656\n [283] 136.28322 208.57327 123.76880 114.38900 216.43925 140.11241\n [289] 150.07316 205.60246 137.26993 119.97139 188.08127 204.21664\n [295] 147.37675 176.73843 181.57885 202.16201 163.52352 191.17531\n [301] 136.46482 131.50565 165.07508 144.60495 157.84560 140.86490\n [307] 208.50753 125.82035 119.27855 153.28197 137.75048 202.15570\n [313] 178.82932 147.63388 173.54644 222.85256 152.78077 142.60081\n [319] 145.15572 206.25542 115.63679 170.17473 197.61485 143.60091\n [325] 158.68337 194.07587 122.05293 123.24367 180.32460 161.29480\n [331] 164.05554 145.04518 225.36574 141.38497 210.99500 201.65803\n [337] 167.28657 127.30968 144.86387 252.92674 189.36291 194.81502\n [343] 130.30282 176.96346 165.00363 269.48656 107.33912 156.60398\n [349] 113.29778 155.96707 192.43021 143.43774 168.65660 213.18884\n [355] 139.85753 164.33048 150.08907 144.40497 138.12094 214.34596\n [361] 227.21044 112.53633 204.00557 194.26066 160.67241 165.53557\n [367] 167.78032 111.14631 135.33036 190.72165 130.09828 211.32556\n [373] 118.54197 196.16828 145.74394 121.21608 217.95214 139.17273\n [379] 167.35819 114.89104 152.50938 165.69853 179.40539 169.86883\n [385] 229.90150 166.64191 154.34828 150.68288 130.44920 208.51411\n [391] 275.60990 151.48908 146.48948 182.26827 142.86949 196.28668\n [397] 139.67341 133.79263 124.97406 148.52963 134.26520 169.13220\n [403] 188.19474 131.85370 217.58438 212.61102 190.18838 127.59663\n [409] 189.97220 190.55675 169.37588 128.73505 135.34782 112.89430\n [415] 134.73368 194.62980 184.50515 177.35284 123.29705 140.66624\n [421] 239.21464 151.10538 134.91098 116.80917 165.13963 140.05233\n [427] 125.56898 225.45803 136.16703 174.89317 117.64379 179.72655\n [433] 158.14510 196.04822 187.88600 133.97047 181.43942 112.56527\n [439] 115.18057 204.59855 143.35459 249.00743 144.89539 184.95052\n [445] 212.85045 122.55416 102.24388 154.87933 154.65673 188.78980\n [451] 216.21211  94.53950 137.06325 166.24039 130.84264 194.76369\n [457] 126.17771 115.39498 178.09194 197.78139 185.91720 133.18945\n [463] 136.16505 111.86391 173.25244 118.97392 166.17325 177.47513\n [469] 190.84164 172.63780 170.93085 106.04933 212.32428 149.01697\n [475] 195.36537 126.66151 176.60519 168.68080 110.00514 193.10504\n [481] 128.46877 134.14763 142.85979 111.66397 128.38230 203.21951\n [487] 212.84195 142.42992 232.34287 135.89875 279.78064 244.33922\n [493] 139.72411 207.64414 111.01190 193.26389 150.49546 175.86901\n [499] 172.33926 165.59371 193.23891 117.14539 229.83170 152.36252\n [505] 162.15699 219.19014 139.16994 233.01267 205.00769 162.38508\n [511] 128.47063 135.43135 155.59562 196.71746 183.21571 154.49366\n [517] 186.60263 174.17380 165.59082 132.98990 103.17175 223.59124\n [523] 191.27377 127.63332 188.55050 162.88403 133.45365 188.13774\n [529] 201.04554 206.76806 175.13501 203.98988 181.49142 141.16764\n [535] 238.75329 197.63544 142.21058 111.68427 183.57979 171.84876\n [541] 237.64611 185.97754 226.11327 165.49675 201.10419 113.90391\n [547] 125.38424 190.30926 214.15175 206.25033 175.66405 111.47887\n [553] 193.83636 115.27530 122.85280 196.09666 173.20503 141.08854\n [559] 204.45229 104.63135 163.26949 190.17312 136.82395 121.42872\n [565] 136.78429 177.37058 114.36494 217.93254 138.75112 125.84302\n [571] 212.18048  94.30437 118.04424 240.66633 162.20175 203.17928\n [577] 215.61957 212.40132 226.58668 111.13381 227.52598 220.43763\n [583] 176.33686 130.54502 115.50299 144.79945 125.89338 203.53834\n [589] 130.22614 144.59669 180.25140 221.43500 202.65069 172.14438\n [595] 133.86082 144.51381 211.63375 182.62553 232.25041 126.54114\n [601] 163.01607 169.94444 236.81727 148.23385 112.84656 243.81087\n [607] 189.45251 113.24763 146.17023 150.54234 200.80677 207.09921\n [613] 168.68654 154.93802 171.00034 189.02603 192.84376 131.20724\n [619] 191.97011 145.01568 194.20417 190.65671 122.71808 188.62797\n [625] 138.70024 108.81351 193.98378 142.26710 170.05987 131.93817\n [631] 110.63109 150.77858 125.69851 142.38525 178.21032 175.17573\n [637] 190.63628 134.80582 141.38775 145.28767 199.00615 180.90663\n [643] 198.42484 203.41722 199.56827 138.21451 171.94116 137.79779\n [649] 141.84021 158.61166 117.86598 203.91593 152.59742 126.44662\n [655] 138.62568 186.45965 119.96264 119.45684 130.43320 136.81425\n [661] 203.76546 128.07393 111.11290 193.85222 154.67693 200.04628\n [667] 155.35604 145.59417 149.37775 125.09382 172.27529 203.92612\n [673] 225.42139 191.60472 148.75545 153.52700 102.87924 126.59363\n [679] 200.73526 125.43261 164.63440 173.70624 206.95860 189.47115\n [685] 186.78459 213.75777 182.52224 196.52344 186.24908 114.89926\n [691] 157.69904 195.94800 184.56458 185.60768 126.12999 155.87535\n [697] 158.34228 186.95052 128.41492  93.15874 226.60448 113.41554\n [703]  92.18089 163.46752 138.60070 181.56519 175.36822 130.94960\n [709] 192.39571 220.38710 127.98157 165.68005 124.40336  97.38909\n [715] 166.44739 133.92037 158.00470 197.45588 121.05594 145.24314\n [721] 153.54952 146.17276 198.14264 149.48187 160.36393 133.18843\n [727] 127.27515 162.73256 211.25701 187.46308 232.15974 210.47332\n [733] 190.89612 204.91098 140.00226 123.41630 130.04053 162.85639\n [739] 176.25321 198.73718 147.86661 213.98292 107.89031 174.89879\n [745] 199.07055 177.27433 136.97387 128.24186 139.84908 233.79619\n [751] 230.38612 178.86947 138.63859 157.97342 194.32731 157.09810\n [757] 120.97580 153.49177 146.88552 136.38715 152.82863 146.93099\n [763] 121.46307  90.61070 120.16531 213.17594 158.10518 142.95151\n [769] 156.30277 175.56260 122.19352 220.76344 148.12513 238.60503\n [775] 195.25240 125.70095 145.91487 112.58521 172.52175 186.53654\n [781] 170.89032 138.49845 156.13722 225.00272 198.38871 125.20788\n [787] 140.10769 116.15198 132.94760 189.12115 125.25904 183.77795\n [793] 171.42803 174.14162 173.27835 179.16206 173.63935 152.45488\n [799] 159.60144 124.07127 201.91325 155.14981 212.76076 171.45576\n [805] 172.63734 149.94979 234.03480 102.61464 130.18725 103.70683\n [811] 134.38488 200.39725 211.02206 183.11105 162.25800 130.00215\n [817] 231.13750 105.75916 114.84940 197.20142 152.01069 131.56587\n [823] 169.53186 148.12194 151.59353 127.88251 112.33553 126.59626\n [829] 150.85159 174.06205 122.60591 110.55058 177.57684 182.01660\n [835] 222.16912 223.08000 110.45649 159.02476 191.07470 185.41310\n [841] 146.05108 211.85535 204.81173 135.28308 169.38331 208.29148\n [847] 176.57709 154.50769 143.89145 139.90229 162.58262 167.92092\n [853] 207.29684 146.35133 179.87615 204.33479  98.06919 142.53308\n [859] 101.83675 208.66087 219.40170 175.50714 211.18457 182.62317\n [865] 193.98444 135.38691 189.42948 209.95460 213.48953 103.77969\n [871] 104.16246 103.08363 226.29609 195.38575 155.04987 189.13377\n [877] 181.58116 187.40206 153.50747 136.00686 176.94626 106.33106\n [883] 159.77087 193.95033 189.46496 209.13466 155.78700 194.36954\n [889] 104.63441 189.30045 207.15043 139.15813 171.81766 188.73981\n [895] 220.19574 129.76074 175.12580 201.94783 223.71185 119.13513\n [901] 212.30435 180.36585 156.02021 165.31152 112.33254 148.41978\n [907] 153.23272 159.30650 152.82948 138.31542 184.07301 107.59974\n [913] 223.86821 118.44008 114.30549 222.10243 144.48583 123.65096\n [919] 146.96331 145.08737 164.38767 181.38390 166.15615 131.47836\n [925] 215.32882 152.13862 141.89194  99.80883 203.08326 160.69185\n [931] 233.87787 120.83056 216.10054 158.22410 133.09133 188.61190\n [937] 231.77488 125.95595 159.10912 161.43819 145.79211 200.83059\n [943] 220.14703 189.09582 216.45638 187.84823 123.16000 134.74009\n [949] 132.19017 143.87020 115.19547 116.98607 155.67385 150.52888\n [955] 142.64983 168.55788 199.98930 143.27166 154.94984 119.58612\n [961] 158.53816 185.75183 132.25616 227.56495 186.57544 173.40071\n [967] 182.43205 141.30477 145.04354 173.11909 172.20692 130.91613\n [973] 119.49333 206.55239  99.65380 197.65653 186.00085 198.24160\n [979] 178.59657 137.27190 146.81800 189.62369 158.02518 161.97654\n [985] 220.97924 177.91163 178.92646 145.45791 153.32269 185.69182\n [991] 131.13306 144.04374 170.38156 138.02158 171.74370 178.14670\n [997] 121.94114 182.11697 153.82077 152.55914\n# simulated height/ weight data\ndf<-data.frame(height,weight)\n\nm0<-lm(height ~ weight, data = df)\nplot(ggeffects::ggpredict(m0, terms = c(\"weight\")), add.data = TRUE, dot.alpha = .8) + labs(x = \"simulated weight\",\n                y = \"simulated height\",\n                title = \"simulated linear relationship\")\n\n\nSimulate a non-linear model\n\n\nShow code\na <-160\nb <- c(2, 0.75)\nx <- rnorm(100)\ny <- rnorm(100, mean = b[1] * exp(b[2] * x))\ndat1 <- data.frame(x, y)\nplot(y ~ x , data = dat1)\n\nShow code\n#Polynomial\nN<-1000\n\n# simulate weights\nweight <-runif(N, min = 60, max =120)\nweight_c <-scale(weight, scale=FALSE)\n\n# simulate coefficients\n\na = rnorm(N, mean = 180 , 10 )\nb1 = rnorm(N, mean = 2.2, .01)\nb2 = - rnorm(N, mean = .02, .001)\n\nheight <- a + b1 * weight_c  +  b2 * weight_c^2\n\n# simulated height/ weight data\n\ndf1<-data.frame(height,weight_c, weight)\n\nplot(height ~ weight)\n\nShow code\nm1 <- lm(height ~ weight_c, data = df1)\nplot(ggeffects::ggpredict(m1, terms = c(\"weight_c\")),\n     add.data = TRUE,\n     dot.alpha = .2)  + labs(title = \"simulated linear relationship\") +\n  xlab(\"simulated weight\") +  ylab(\"simulated height\")\n\n\nNon-linear model\n\n\nShow code\nm2 <-lm(height ~ weight_c + I(weight_c^2), data = df1)\n\nplot(ggeffects::ggpredict(m2, terms = c(\"weight_c\")),\n     add.data = TRUE,\n     dot.alpha = .2) + labs( title = \"simulated linear relationship\") + \n  xlab(\"simulated weight\") +  ylab(\"simulated height\")\n\nShow code\nm3 <-lm(height ~ bs(weight_c), data = df1)\n\nsummary(m3)\n\nCall:\nlm(formula = height ~ bs(weight_c), data = df1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.035  -6.296  -0.396   6.564  34.931 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     98.339      1.216   80.87   <2e-16 ***\nbs(weight_c)1   68.558      3.544   19.35   <2e-16 ***\nbs(weight_c)2  111.865      2.391   46.78   <2e-16 ***\nbs(weight_c)3  127.830      2.051   62.31   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10 on 996 degrees of freedom\nMultiple R-squared:  0.9333,    Adjusted R-squared:  0.9331 \nF-statistic:  4645 on 3 and 996 DF,  p-value: < 2.2e-16\n\nShow code\nplot(ggeffects::ggpredict(m3, terms = c(\"weight_c\")),\n     add.data = TRUE,\n     dot.alpha = .2)  + labs( title = \"simulated linear relationship\") + \n  xlab(\"simulated weight\") +  ylab(\"simulated height\")\n\n\nCheck linear model\n\n\nShow code\nperformance::check_model(m1)\n\n\nCheck quadratic model\n\n\nShow code\nperformance::check_model(m2)\n\n\nCheck splines model\n\n\nShow code\nperformance::compare_performance(m1, m2, m3)\n# Comparison of Model Performance Indices\n\nName | Model |      AIC |      BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n------------------------------------------------------------------------\nm1   |    lm | 7735.072 | 7749.795 | 0.911 |     0.911 | 11.537 | 11.549\nm2   |    lm | 7447.124 | 7466.755 | 0.933 |     0.933 |  9.980 |  9.995\nm3   |    lm | 7449.109 | 7473.648 | 0.933 |     0.933 |  9.980 | 10.000\n\nUse fake data to create a factor\nHere we simulate no relationship between a group and an outcome\n\n\nShow code\nN <- 200 # number of observations\n#group <- rep((0:1), length.out = 200) # 2 groups\ngroup <- rep(c(\"m\",\"n_m\"), each = N/2) #equivalent:\na <- rnorm(N, 150, 3) # intercept\nb1 <- rnorm(N, 20, 1) # coefficient of \"b\nsigma = rexp(N,1)# error term\noutcome <- rnorm(N, mean = a + b1 * (group == \"m\"), sigma)\n\ndf <-data.frame(outcome,group)\ndplyr::glimpse(df)\nRows: 200\nColumns: 2\n$ outcome <dbl> 169.2349, 163.9593, 172.4746, 170.9098, 171.0012, 17…\n$ group   <chr> \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m…\n\nShow code\n#model removing the intercept to show the difference\nms<-lm(outcome ~ -1 + group, data = df)\nms\n\nCall:\nlm(formula = outcome ~ -1 + group, data = df)\n\nCoefficients:\n  groupm  groupn_m  \n   170.4     150.0  \n\nShow code\n# no difference\nsjPlot::plot_model(ms)\n\n\nIs imbalance in my study causing a problem?\n\n\nShow code\n### Is imbalance wrecking my inference? \n\nN <- 120\ncells <-rep( letters[1:2], times = c(15, 105))\n\na <- rnorm(N, 2, 1)\nb1 <- rnorm(N, .2, .1)\nsigma <- rexp(N,1)\n\nout <- rnorm(N, mean = a + b1 * (cells == \"b\"), sigma)\ndfc<-data.frame(out,cells)\nsim_cells<-lm(out ~ cells, data = dfc)\n\nsummary(sim_cells)\n\nCall:\nlm(formula = out ~ cells, data = dfc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3571 -0.9177 -0.1441  0.8207  8.6009 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   1.5318     0.4666   3.283  0.00135 **\ncellsb        0.8271     0.4988   1.658  0.09996 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.807 on 118 degrees of freedom\nMultiple R-squared:  0.02277,   Adjusted R-squared:  0.01449 \nF-statistic: 2.749 on 1 and 118 DF,  p-value: 0.09996\n\nThis isn’t too convincing: we need to replicate the model many times\n\n\nShow code\n# Make a function for the simulation\nset.seed(12)\ntest_fun = function() {\n  N <- 120\n  cells <-rep( letters[1:2], times = c(110, 10))\n  a <- rnorm(N, 2, 1)\n  b1 <- rnorm(N, 1, .1)\n  sigma <- rexp(N, 1)\n  out <- rnorm(N, mean = a + b1 * (cells == \"b\"), sigma)\n  dfc <- data.frame(out, cells)\n  sim_cells <- lm(out ~ cells, data = dfc)\n  sim_cells\n}\n\nsim_lm = replicate(20, test_fun(), simplify = FALSE )\nlength(sim_lm)\n[1] 20\n\nWe can use the purrr package to generate many replicates of a model\n\n\nShow code\nlibrary(dplyr)\ntab_sim<-purrr::map_dfr(sim_lm, broom::tidy)\ntab_sim %>%\n  dplyr::mutate_if(is.numeric, round, 5)\n# A tibble: 40 x 5\n   term        estimate std.error statistic p.value\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)    1.89      0.143    13.2   0      \n 2 cellsb         1.07      0.497     2.15  0.0338 \n 3 (Intercept)    1.88      0.174    10.8   0      \n 4 cellsb         1.74      0.603     2.88  0.00472\n 5 (Intercept)    1.96      0.140    14.1   0      \n 6 cellsb         2.08      0.484     4.29  0.00004\n 7 (Intercept)    1.90      0.179    10.6   0      \n 8 cellsb         0.894     0.621     1.44  0.152  \n 9 (Intercept)    1.85      0.192     9.64  0      \n10 cellsb         0.162     0.664     0.244 0.808  \n# … with 30 more rows\n\nWhat percentage of simulations yield \"significant results?\n\n\nShow code\nsum(tab_sim$p.value <= .05) / length(tab_sim$p.value)\n[1] 0.725\n\nDoes balance fix the issue?\n\n\nShow code\n# Make a function for the simulation\nset.seed(12)\ntest_fun = function() {\n  N <- 120\n  cells <-rep( letters[1:2], times = c(60, 60))\n  a <- rnorm(N, 2, 1)\n  b1 <- rnorm(N, 1, .1)\n  sigma <- rexp(N, 1)\n  out <- rnorm(N, mean = a + b1 * (cells == \"b\"), sigma)\n  dfc <- data.frame(out, cells)\n  sim_cells <- lm(out ~ cells, data = dfc)\n  sim_cells\n}\n\nsim_lm = replicate(20, test_fun(), simplify = FALSE )\nlength(sim_lm)\n[1] 20\n\nWe can use the purrr package to generate many replicates of a model\n\n\nShow code\nlibrary(dplyr)\ntab_sim<-purrr::map_dfr(sim_lm, broom::tidy)\ntab_sim %>%\n  dplyr::mutate_if(is.numeric, round, 5)\n# A tibble: 40 x 5\n   term        estimate std.error statistic  p.value\n   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)    1.87      0.193      9.69 0       \n 2 cellsb         1.06      0.273      3.90 0.00016 \n 3 (Intercept)    2.10      0.237      8.87 0       \n 4 cellsb         0.665     0.335      1.98 0.0495  \n 5 (Intercept)    1.84      0.190      9.66 0       \n 6 cellsb         1.46      0.269      5.43 0       \n 7 (Intercept)    1.71      0.242      7.09 0       \n 8 cellsb         1.34      0.342      3.93 0.000140\n 9 (Intercept)    1.97      0.259      7.59 0       \n10 cellsb         0.625     0.366      1.71 0.0902  \n# … with 30 more rows\n\nWhat percentage of simulations yield \"significant results?\n\n\nShow code\nsum(tab_sim$p.value <= .05) / length(tab_sim$p.value)\n[1] 0.975\n\nAcknowledgments\nThe approach to simulation presented here owes to:\nAriel Muldoon: https://aosmith.rbind.io/\nAriel has a bunch of resources on her website, please check them out.\nRichard Mcelreath’s Statistical Rethinking (McElreath 2020)\nRegression and other stories (Gelman, Hill, and Vehtari 2020)\nAppendix 1: Conceptual Background\nSome preliminaries about science.\nScience begins with a question\nScience begins with a question about the world. The first step in science, then, is to clarify what you want to know.\nBecause science is a social practice, you will also need to clarify why your question is interesting: so what?\nIn short, know your question.\nScientific model (or theory)\nSometimes scientists are interested in specific features of the world: how did virus x originate? Such a question might have a forensic interest: what constellation of events gave rise to a novel infectious disease?\nMore typically, scientists seek generalisations. How do infectious diseases evolve? How do biological organisms evolve? Such questions have applied and fundamental interests. How can we better prevent infectious disease? How did life originate?\nA scientific model is a proposal for how nature is structured (and unstructured). For example, the theory of evolution by natural selection proposes that life emerges from variation, inheritance, and differential reproduction/survival.\nTo evaluate a scientific model, scientists must make generalisations beyond individual cases. This is where statistics shines.\nWhat is statistics?\nMathematics is a logic of certainty.\nStatistics is a logic of uncertainty.\nA statistical model uses the logic of probability to make better guesses.\nApplications of statistical models in science\nScientific models seek to explain how nature is structured. Where scientific models conflict, we can combine statistical models with data-collection to evaluate the credibility of of one theoretical model over others. To do this, a scientific model must make distinct, non-trivial predictions about the world.\nIf the predictions are not distinct, the observations will not enable a shift in credibility for one theory over another. Consider the theory that predicts any observation. Such a theory would be better classified as a conspiracy theory; it is compatible with any evidence whatsoever.\n\nToday we introduce a statistical method called regression. We will focus on how regression helps both to evaluate, and to make, informed predictions about the structures of the world.\nAppendix 2: How your computer sees your data\nUnder the hood\nUnder the hood, your computer sees your data as consisting of vectors and matrices. An algorithm searches to estimate (or in the case of Bayesian inference, to “solve”) an optimization problem to obtain a location for the unobserved parameter; in the case we examined above, this parameter is the relationship between daughter heights and mother heights.\n\\[\\begin{bmatrix}\n\\textbf{y}\\\\\n\\textit{daughter's height}\\\\\n51.5\\\\\n52.5 \\\\\n53.0 \\\\\n\\vdots\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\textbf{intercept} \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\vdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\textbf{x}\\\\\n\\textit{mother's height}\\\\\n59.5\\\\\n57.5 \\\\\n60.0 \\\\\n\\vdots\n\\end{bmatrix}\n\\begin{bmatrix}\n\\textbf{b}\\\\\n29.8 \\\\\n0.54\n\\end{bmatrix}\\]\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge university press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.\n\n\nPearson, Karl, and Alice Lee. 1903. “On the Laws of Inheritance in Man: I. Inheritance of Physical Characters.” Biometrika 2 (4): 357–462.\n\n\nThe relationship of probability distributions and data-generating processes is complex, intriguing, and both historically and philosophically rich \\(\\dots\\). Because our interests are applied, we will hardly touch up this richness in this course, alas.↩︎\nLater, we’ll prefer a different way of writing regression equations in math. (Note: writing math isn’t math - it’s just encoding the model that we’ve written).↩︎\n",
    "preview": "posts/5_1/distill-preview.png",
    "last_modified": "2021-03-23T17:33:34+13:00",
    "input_file": "lecture_5.utf8.md",
    "preview_width": 2500,
    "preview_height": 2500
  },
  {
    "path": "posts/4_1/",
    "title": "Consolidation of skills",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-16",
    "categories": [],
    "contents": "\n\nContents\nGet data\nPreamble\nData carpentry continued\nDifferent methods for selecting columns\nRe-leveling a factor\nCreating factors from numerical indicators\nUsing ifelse to create factors\nTransformations of indicators: scaling, centering, and logs\nPro tip 1:\nCreate and work with dates a date\nCreate a timeline\nSlice\nLags and leads using timeseries data\nPro tip 2\n\nData summary\nSummarise all your data\nThe skimr package\nTable1 & other canned table packages\nCreate a table using pipe functions\nBar graphs\nMissing data graphs\nBoxplots\nCorrelation graphs\nThe report package\nMeasures\nOrder of your Method section\n\nAppendix 1A Sampling Procedure – NZAVS Time 10 (2018; conducted from 18.06.2018-28.09.2019)\nAppendix 1B Sampling Procedure – NZAVS Time 11 (2019; conducted from 29.09.2019-17.10.2020)\nAppendix 2 Johannes’s mini-lecture on the papaja package\nAppendix 3 Style advice about research methods\n\n\n\n\nGet data\n\n\n#libraries\nif (!require(skimr)) install.packages('skimr')\nif (!require(lubridate)) install.packages('lubridate')\n\nif (!requireNamespace(\"devtools\")) {\n  install.packages(\"devtools\")\n}\nif (!require(easystats)) devtools::install_github(\"easystats/easystats\")\n\n\n# Attaching packages (red = needs update)\n✔ insight     0.13.1.1   ✔ bayestestR  0.8.3.1 \n✔ performance 0.7.0.1    ✔ parameters  0.12.0.1\n✔ see         0.6.2.1    ⚠ effectsize  0.4.3.1 \n✔ correlation 0.6.0.1    ✔ modelbased  0.5.9   \n✔ report      0.2.0      \nWarnings or errors in CRAN checks for package(s) 'bayestestR', 'parameters', 'effectsize', 'correlation'.\nRestart the R-Session and update packages in red with 'easystats::easystats_update()'.\n\nif (!require(ggthemes)) install.packages('ggthemes')\nif (!require(pmdplyr)) install.packages(\"pmdplyr\")\nif (!require(kableExtra)) install.packages(\"kableExtra\")\n# this should be part of easystats but in case not:\nif (!require(report)) install.packages('report')\nif (!require(brms)) install.packages('brms')\nif (!require(lme4)) install.packages('lme4')\nif (!require(table1)) install.packages('table1')\nif (!require(modelsummary)) install.packages(\"modelsummary\")\nif (!require(naniar)) install.packages(\"naniar\")\nif (!require(ggraph)) install.packages(\"ggraph\")\nif (!require(gtsummary)) install.packages(\"gtsummary\")\n\n\n# load tidyverse\nlibrary(\"tidyverse\")\n\n# theme set\ntheme_set(theme_classic())\n\n# uncomment below and run this code\n# easystats::install_easystats_latest()\n\n\n\n\n\nnz_0 <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"))\n\n# take all characters and make them factors\n# also get rid of duplicate rows\n# Note the convention of renaming dataframe when creating a new one:\n# ` nz <-nz_0 %>%... `\n\nf<-c(\"None Of The Time\",\"A Little Of The Time\",\"Some Of The Time\",  \"Most Of The Time\", \"All Of The Time\")\n\nnz <-nz_0 %>%\n  dplyr::mutate_if(is.character, factor) %>%\n  select(-c(SWB.Kessler01,SWB.Kessler02,SWB.Kessler03,SWB.Kessler04,SWB.Kessler05, SWB.Kessler06))%>%\n  dplyr::mutate(Wave = as.factor(Wave))%>%\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless,f))%>%\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed,f))%>%\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless,f))%>%\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort,f))%>%\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless,f))%>%\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous,f))\n\n# not used\n# nz <- haven::zap_formats(nz)\n# nz <- haven::zap_label(nz)\n# nz <- haven::zap_widths(nz)\n# nz <- haven::zap_labels(nz)\n\n\n\nPreamble\nOne of the advantages of R is that allows us to create highly effective workflows. Today, we’ll reinforce and extend the workflow skills that you’ve started to develop in previous weeks. Below we’ll be working with the nz dataset, which is a reduced, truncated, and jittered version of waves 10 and 11 of the New Zealand Attitudes and Values Study. This dataset is for teaching only, if you’d like to learn more about the study to which it belongs, go here or here.\nData carpentry continued\nDifferent methods for selecting columns\nSuppose we want to select all variables that start with Believe. We can do this in a number of ways.\nFirst there is explicit selection:\n\n\n# explicit selection \nnz %>%\n  select(\"Believe.God\", \"Believe.Spirit\")%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ Believe.God    <fct> Not Believe God, Not Believe God, Believe God…\n$ Believe.Spirit <fct> Not Believe Spirit, Not Believe Spirit, Belie…\n\nWe can select all instances of a column that start with a certain name. For this you by using starts_with\n\n\nnz %>%\n  select(starts_with(\"Believe\"))%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ Believe.God    <fct> Not Believe God, Not Believe God, Believe God…\n$ Believe.Spirit <fct> Not Believe Spirit, Not Believe Spirit, Belie…\n\nBy the same token, we can select all instances of a variable that ends with a certain string by using ends_with\n\n\nnz %>%\n  select(ends_with(\"conditions\"))%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ NZ.Social.Conditions   <dbl> 4, 6, 3, 0, 4, 3, 4, 5, 4, 6, 5, 5, 6…\n$ NZ.Business.Conditions <dbl> 8, 6, 4, 7, 5, 3, 4, 6, 6, 6, 5, 6, 5…\n\nWe can cast a broader net and select all instances of a variable within a string by using contains\n\n\nnz %>%\n  select(contains(\"Believe\"))%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 3\n$ Religion.Believe.Cats <dbl> 4, 4, 1, 1, 1, 1, 4, NA, 3, 3, 4, 4, 2…\n$ Believe.God           <fct> Not Believe God, Not Believe God, Beli…\n$ Believe.Spirit        <fct> Not Believe Spirit, Not Believe Spirit…\n\nAs we can see, the net that we cast using contains was too broad. We don’t want the Religion.Believe.Cats.\nIn R, you can programme your way out of this corner as follows:\n\n\nnz %>%\n  select(contains(\"Believe\") &  -  Religion.Believe.Cats)%>%\n   glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ Believe.God    <fct> Not Believe God, Not Believe God, Believe God…\n$ Believe.Spirit <fct> Not Believe Spirit, Not Believe Spirit, Belie…\n\nHowever, that’s inelegant; better to drop contains altogether and revert to another method.\nRe-leveling a factor\nDeath, taxes, and factors are consequence of living. Let’s look at the BigDoms variable in the nz, which is a factor identifying large religious denominations\n\n\nnz %>%\n  dplyr::select(BigDoms)%>%\n  table(useNA =\"ifany\")\n\n\n.\n Buddhist Christian    Muslim   Not_Rel TheOthers      <NA> \n       37      1176        13      2697       136        77 \n\nNote the use of ifany to print the NAs in this table. It’s almost never sensible to ignore missing values!\nSuppose we wanted to make “Not Rel” our base category for this factor. We could do so as follows:\n\n\n## suppose we want \"Not_Rel\" as the base category, and rearrange the other levels\nlibrary(forcats) # this is part of the tidyverse package. \nnz1<-nz %>%\n  dplyr::select(BigDoms, KESSLER6sum) %>%\n  dplyr::mutate(BigDoms =  \n                  forcats::fct_relevel(BigDoms, c(\"Not_Rel\",\"Christian\",\"Buddhist\",\"Muslim\",\"TheOthers\")))\n\n#inspect data\nnz1%>%\n  group_by(BigDoms)%>%\n  count()\n\n\n# A tibble: 6 x 2\n# Groups:   BigDoms [6]\n  BigDoms       n\n  <fct>     <int>\n1 Not_Rel    2697\n2 Christian  1176\n3 Buddhist     37\n4 Muslim       13\n5 TheOthers   136\n6 <NA>         77\n\nThe reordering makes for a more sensible model because the base category is now Not_Rel or not-religious. Hence comparisons are to this category.\n\n\nm0<- glm( KESSLER6sum ~ BigDoms, data = nz1 )\nparameters::model_parameters(m0)  %>%\n  print_html(caption = \"Model of Distress by Denomination with the base category is `No Religion'\")\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#hngmbjbciu .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#hngmbjbciu .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#hngmbjbciu .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#hngmbjbciu .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#hngmbjbciu .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#hngmbjbciu .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#hngmbjbciu .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#hngmbjbciu .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#hngmbjbciu .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#hngmbjbciu .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#hngmbjbciu .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#hngmbjbciu .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#hngmbjbciu .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#hngmbjbciu .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#hngmbjbciu .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#hngmbjbciu .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#hngmbjbciu .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#hngmbjbciu .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#hngmbjbciu .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#hngmbjbciu .gt_left {\n  text-align: left;\n}\n\n#hngmbjbciu .gt_center {\n  text-align: center;\n}\n\n#hngmbjbciu .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#hngmbjbciu .gt_font_normal {\n  font-weight: normal;\n}\n\n#hngmbjbciu .gt_font_bold {\n  font-weight: bold;\n}\n\n#hngmbjbciu .gt_font_italic {\n  font-style: italic;\n}\n\n#hngmbjbciu .gt_super {\n  font-size: 65%;\n}\n\n#hngmbjbciu .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nModel of Distress by Denomination with the base category is `No Religion'\n    \n    Parameter\n      Coefficient\n      SE\n      95% CI\n      t(4026)\n      p\n    (Intercept)\n      5.17\n      0.08\n      (5.02, 5.32)\n      67.55\n      < .001\n    BigDoms (Christian)\n      -0.25\n      0.14\n      (-0.53, 0.02)\n      -1.82\n      0.069 \n    BigDoms (Buddhist)\n      -0.32\n      0.66\n      (-1.60, 0.97)\n      -0.48\n      0.631 \n    BigDoms (Muslim)\n      -0.25\n      1.10\n      (-2.41, 1.91)\n      -0.23\n      0.820 \n    BigDoms (TheOthers)\n      0.82\n      0.35\n      (0.13, 1.51)\n      2.33\n      0.020 \n    \n\nWe can see the results better using a coefficient graph, which visualises the information presented in the table.\n\n\nplot(parameters::model_parameters(m0) ) + labs(title = \"Comparison of Religious groups to secular people\", \nsubtitle = \"Christians are a little more chilled out, \\n Other denoms are less chilled out\")\n\n\n\n\nThe base category is the comparison class. Should we infer that “The Others” denomination causes greater distress? We’ll return to this, and related questions, in the upcoming weeks. For now let’s just leave it at “probably not.”\nCreating factors from numerical indicators\nIt is almost never a good idea to transform continuous data into categorical data. However, occassionally, you will need to do so. For example, we might want to break the KESSLER6 distress indicator into its medically diagnostic components for “mild distress,” “moderate distress,” and “severe distress.” We may achieve this task using the cut function as follows:\n\n\nnz <-nz %>%\n  dplyr::mutate(k6cats = cut(\n    KESSLER6sum,\n    breaks = c(-Inf, 5, 13, Inf),   # create Kessler 6 diagnostic categories\n    labels = c(\"Low Distress\", \"Moderate Distress\", \"Serious Distress\"), \n    right = TRUE\n  ))\ntable(nz$k6cats, useNA = \"ifany\")\n\n\n\n     Low Distress Moderate Distress  Serious Distress \n             2560              1372               172 \n             <NA> \n               32 \n\nUsing ifelse to create factors\nI prefer to maintain control over how I am making the categories. For example, in the previous example, I didn’t remember whether cut includes a value to the left or to the right. I had to look this up. However, I can use ifelse function to explicitly create the relevant categories:\n\n\nnz %>%\n  dplyr::mutate(k6cats1 =  as.factor(ifelse(\n    KESSLER6sum <= 5,\n    \"Low Distress\",\n    ifelse(KESSLER6sum <= 13,  \"Moderate Distress\", \"Serious Distress\")\n  ))) %>%\n  group_by(k6cats1) %>%\n  count()\n\n\n# A tibble: 4 x 2\n# Groups:   k6cats1 [4]\n  k6cats1               n\n  <fct>             <int>\n1 Low Distress       2560\n2 Moderate Distress  1372\n3 Serious Distress    172\n4 <NA>                 32\n\n#check this is the same as the previous method\nnz %>%\n  group_by(k6cats) %>%\n  count()\n\n\n# A tibble: 4 x 2\n# Groups:   k6cats [4]\n  k6cats                n\n  <fct>             <int>\n1 Low Distress       2560\n2 Moderate Distress  1372\n3 Serious Distress    172\n4 <NA>                 32\n\nWe can see that this method returns the same values as the cut method above.\nTransformations of indicators: scaling, centering, and logs\nThroughout this course, we’ll be standardising and centering indicators. Occasionally, we’ll need to perform log transformations. You’ll need to know how to do this.\nSuppose we want to standardise the Relid indicator. This will transform the Relid indicator into standard deviation units. In later seminars, we’ll explain why this transformation is useful. For now, this is how you do it:\n\n\nnz1 <- nz %>%\n  select(Relid)%>%\n  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE))\nhead(nz1)\n\n\n# A tibble: 6 x 2\n  Relid religousid_s[,1]\n  <dbl>            <dbl>\n1     0           -0.624\n2     0           -0.624\n3     6            1.71 \n4     6            1.71 \n5     3            0.545\n6     3            0.545\n\nWhat happened? The variable name for our standardised variable looks weird: religious_s[ ,1]\nThis isn’t a worry. We use the variable as we would any other and all is fine.1\n\n\nsjPlot::tab_model(lm(religousid_s ~ 1 , data = nz1))\n\n\n\n \n\n\nreligousid_s\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n-0.00\n\n\n-0.03 – 0.03\n\n\n1.000\n\n\nObservations\n\n\n3993\n\n\nR2 / R2 adjusted\n\n\n0.000 / 0.000\n\n\nPro tip 1:\nTransform your data as the last step in your pipe workflow.\nThis is because if you filter cases, you’ll end up with a variable that isn’t measured standard deviations units\n\n\nnza <- nz %>%\n  select(Relid, BigDoms)%>%\n  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE)) \nnzb <- nz %>%\n  select(Relid, BigDoms)%>%\n  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE)) %>%\n  filter(BigDoms !=\"Not_Rel\")\n\n# compare\nsummary(nza$religousid_s)\n\n\n       V1         \n Min.   :-0.6237  \n 1st Qu.:-0.6237  \n Median :-0.6237  \n Mean   : 0.0000  \n 3rd Qu.: 0.5453  \n Max.   : 2.1040  \n NA's   :143      \n\n# with\nsummary(nzb$religousid_s)\n\n\n       V1        \n Min.   :-0.234  \n 1st Qu.: 0.935  \n Median : 1.325  \n Mean   : 1.298  \n 3rd Qu.: 2.104  \n Max.   : 2.104  \n NA's   :66      \n\nWhen we filter last, the mean value in the dataset is 1.3 – everything has changed!\n\n\nnz1 <- nz1 %>%\n  select(Relid)%>%\n  mutate(religousid_s = scale(Relid, scale = TRUE, center  = TRUE))\nhead(nz1)\n\n\n# A tibble: 6 x 2\n  Relid religousid_s[,1]\n  <dbl>            <dbl>\n1     0           -0.624\n2     0           -0.624\n3     6            1.71 \n4     6            1.71 \n5     3            0.545\n6     3            0.545\n\nor simply:\n\n\nnz1 <- nz1 %>%\n  select(Relid) %>%\n  mutate(religousid_s = scale(Relid))\n\nhead(nz1)\n\n\n\nTo center a variable we set scale = FALSE, center = TRUE\n\n\nnz1 <- nz %>%\n  mutate(religousid_c = scale(Relid, scale = FALSE, center  = TRUE))\n\n# inspect new indicator\nnz1%>%\n  select(Relid,religousid_c)%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ Relid        <dbl> 0, 0, 6, 6, 3, 3, NA, NA, 0, 0, 0, 0, 1, 1, 3, …\n$ religousid_c <dbl[,1]> <matrix[23 x 1]>\n\nWe use the log transformation for extreme values. We can create a new indicator by combining mutate and log as follows:\n\n\nnz1 <- nz %>%\n  mutate(charitydonate_log = log(CharityDonate + 1))\n\n# inspect new indicator\nnz1 %>%\n  select(CharityDonate,charitydonate_log)%>%\n    glimpse()\n\n\nRows: 4,136\nColumns: 2\n$ CharityDonate     <dbl> 1000, 0, 500, 500, 50, 20, 20, 100, 400, 3…\n$ charitydonate_log <dbl> 6.908755, 0.000000, 6.216606, 6.216606, 3.…\n\nNote that we have to add \\[+1\\] to the log transformation, as you will recall that the log of zero is undefined. You cannot obtain zero by raising it to the power of another number.\nCreate and work with dates a date\n\n\nnz <- nz %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE)  # first data of data collection in this study\n\n\n\nWe can analyze dates, for example, for how many minutes were data collected?\n\n\nnz %>%\n  select(date)%>%\n  summary()\n\n\n      date           \n Min.   :2018-01-24  \n 1st Qu.:2018-08-08  \n Median :2019-10-03  \n Mean   :2019-05-15  \n 3rd Qu.:2019-12-05  \n Max.   :2020-10-10  \n\nint<-lubridate::interval(ymd(\"2018-01-02\"), ymd(\"2020-10-06\"))\n\n#time in years\ntime_length(int, \"year\")\n\n\n[1] 2.759563\n\n#time in minutes\ntime_length(int, \"minutes\")\n\n\n[1] 1451520\n\nFun! So much so you have some homework that will work with dates.\nCreate a timeline\nHere we’re going to graph the number of responses each day for the years of data collection.\n\n\nlibrary(lubridate)\nlibrary(ggplot2)\n\n\ndatrep <- nz %>%\n  count(day = floor_date(date, \"day\"))%>%\n  dplyr::mutate(Year = factor(ifelse(\n    day < \"2018-01-01\",\n    2017,\n    ifelse(day < \"2019-01-01\", 2018,\n           ifelse(day < \"2020-01-01\", 2019, 2020))\n  ))) %>%\n  arrange(day)\n\n# create the graph\nggplot(datrep, aes(day, n)) +\n  geom_col(aes(fill = Year)) +\n  scale_x_date(date_labels = \"%b/%Y\")  +\n  xlab(\"Days\") + ylab(\"Count of Responses\") +\n  ggtitle(\"Our Dataset's Daily Counts\")  +\n  theme_classic()  +\n  scale_fill_viridis_d()\n\n\n\n\nNote that we can use the datrep dataframe that we created to explore aspects of data collection. For example we can arrange the dataset by day in descending order of participants sampled:\n\n\ndatrep%>%\n  arrange(desc(n))\n\n\n# A tibble: 623 x 3\n   day            n Year \n   <date>     <int> <fct>\n 1 2018-06-21   121 2018 \n 2 2018-06-22   104 2018 \n 3 2018-06-20    88 2018 \n 4 2018-06-24    80 2018 \n 5 2019-12-03    67 2019 \n 6 2018-06-23    65 2018 \n 7 2018-06-25    62 2018 \n 8 2019-12-05    46 2019 \n 9 2018-10-20    45 2018 \n10 2018-06-26    44 2018 \n# … with 613 more rows\n\nTake not of that code, you might need it for your workbook.\nWhat might we do with dates? Well we might ask, were there any inherently stressful days?\nTo see this, we can take average stress levels by day, and then see where the high average stress days fall.\n\n\ntn<-nz %>%\n  select(date,KESSLER6sum,Id) %>%\n  group_by(date)%>%\n  summarise(\n   av_distress =  mean(KESSLER6sum, na.rm = TRUE),\n   n = n_distinct(Id)\n  ) %>%\n  arrange(desc(av_distress))%>%\n  glimpse()\n\n\nRows: 623\nColumns: 3\n$ date        <date> 2020-05-10, 2020-05-23, 2020-04-22, 2020-04-26,…\n$ av_distress <dbl> 19.00000, 19.00000, 17.00000, 15.00000, 15.00000…\n$ n           <int> 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 2, 3, 1, 1, 1, …\n\nGraphing the densities reveals the following\n\n\ntn%>%\n  ggplot(., aes(date, av_distress)) + \n  geom_col(aes(fill =(n))) + scale_x_date(date_labels = \"%b/%Y\")  + theme_classic() + scale_fill_viridis_c()\n\n\n\n\nClearly the “stressful days” are an artifact of days with low numbers of participant respondents.\nLet’s see whether there are any stressful days of the week. We do this by creating a weekday variable using the wday function in the lubridate package. Let’s graph our results using a pipe %>% workflow:\n\n\nnz %>%\n  select(Id, date, KESSLER6sum) %>%\n  mutate(weekdays = wday(date, label = TRUE)) %>%\n  group_by(weekdays) %>%\n  summarise(\n    mn_k6 =  mean(KESSLER6sum, na.rm = TRUE),\n    sd_k6 =  sd(KESSLER6sum, na.rm = TRUE),\n    n_k6w = n()\n  ) %>%\n  mutate(\n    se_k6 = sd_k6 / sqrt(n_k6w),\n    lw_ci = mn_k6 - qt(1 - (0.05 / 2), n_k6w - 1) * se_k6,\n    up_ci = mn_k6 + qt(1 - (0.05 / 2), n_k6w - 1) * se_k6\n  ) %>%\n  ggplot(., aes(x = weekdays, y = mn_k6, colour = mn_k6)) +\n  geom_errorbar(aes(ymin = lw_ci, ymax = up_ci), width = .1) +\n  geom_point(size = 3)  +\n  scale_y_continuous(limits = c(0,7)) + \n  theme_classic() + scale_fill_viridis_d()\n\n\n\n\nDespite the variability over the two years of data collection, the bars of the graph overlap: we don’t find differences in distress by days.\n“Ok Boomer,” you ask, “what if we were to calculate distress by generational cohorts?”\nMy reply, I’m not a boomer, I’m a GenX-er. I’m keen to check it out:\n\n\nnz$hour\n\n\nNULL\n\nnz %>%\n  select(GenCohort, KESSLER6sum) %>%\n  group_by(GenCohort) %>%\n  summarise(\n    mn_k6 =  mean(KESSLER6sum, na.rm = TRUE),\n    sd_k6 =  sd(KESSLER6sum, na.rm = TRUE),\n    n_k6w = n()\n  ) %>%\n  mutate(\n    se_k6 = sd_k6 / sqrt(n_k6w),\n    lw_ci = mn_k6 - qt(1 - (0.05 / 2), n_k6w - 1) * se_k6,\n    up_ci = mn_k6 + qt(1 - (0.05 / 2), n_k6w - 1) * se_k6\n  ) %>%\n  ggplot(., aes(x = GenCohort, y = mn_k6, colour = GenCohort)) +\n  geom_errorbar(aes(ymin = lw_ci, ymax = up_ci), width = .1) +\n  geom_point(size = 3)  +\n  scale_y_continuous(limits = c(0, 7)) +\n  theme_classic() +\n  geom_hline(yintercept = 5,\n             colour = \"red\",\n             linetype = \"dashed\") +\n  scale_y_continuous(limits = c(0, 10)) +\n  theme(\n    legend.text = element_text(size = 6),\n    legend.title = element_text(size = 8),\n    axis.text.x = element_blank()\n  ) +\n  xlab(\"Birth Generation Cohort\") +\n  ylab(\"Kessler 6 Distress\") +\n  labs(title = \"Average Distress by Birth Cohort\",\n       subtitle = \"Red line indicates clinically moderate distress threshold\") +\n  scale_colour_viridis_d() \n\n\n\n\nLater, we’ll ask why you’re so stressed out.\nSlice\nDplyr’s slice function can be handy. Say we only want the first four rows\n\n\ndatrep%>%\n  arrange(desc(n)) %>%\n  slice(1:4)\n\n\n# A tibble: 4 x 3\n  day            n Year \n  <date>     <int> <fct>\n1 2018-06-21   121 2018 \n2 2018-06-22   104 2018 \n3 2018-06-20    88 2018 \n4 2018-06-24    80 2018 \n\nSay we only want the 1st row, the 3rd row, and the 20th row\n\n\ndatrep%>%\n  dplyr::arrange(desc(n)) %>%\n  dplyr::slice(c(1,3,20))\n\n\n# A tibble: 3 x 3\n  day            n Year \n  <date>     <int> <fct>\n1 2018-06-21   121 2018 \n2 2018-06-20    88 2018 \n3 2019-12-12    36 2019 \n\nLags and leads using timeseries data\nCreate a difference variable for change in Kessler 6\n\n\nlibrary(\"pmdplyr\")\ndf <-nz %>%\n  dplyr::filter(!is.na(KESSLER6sum))%>%\n  mutate(wave = as.numeric(Wave))%>%\n   mutate(lag_k6 = tlag(KESSLER6sum,\n    .i = Id, # id variable\n    .t = wave # time series variable, needs to be numeric\n  ))%>%\n  mutate(diff_k6 = lag_k6 - KESSLER6sum) %>%\n  select(Id,Wave,KESSLER6sum,diff_k6,Emp.JobSecure,Employed)%>%\n  arrange(desc(diff_k6)) \n\n\n\nWhat to do with this new variable. Well, we might explore whether employment security relates to distress change:\n\n\ndf %>%\n  filter(Wave == 2019) %>%\n  mutate(employed_employsecurity = as.factor(ifelse(Employed ==1, Emp.JobSecure,0)))%>%\n  ggplot(data = ., aes(x = diff_k6, fill = employed_employsecurity) )+\n   geom_histogram() + \n  xlab(\"Difference in K6 eleveation (cases above 5)\") + \n  ylab(\"Counts of cases\") + \n  labs(subtitle =\"No clear relationship between unemployment insecurity and distress change\")+\n  scale_fill_discrete(name=\"Employment Security 1-7\") + \n  scale_fill_viridis_d() + theme_classic() + \n  theme(legend.position = \"bottom\")\n\n\n\n\nAnd remarkably we don’t see much evidence in the cross-sectional analysis.\n\n\n# create data frame with new variable Zero is for the unemeployed. \ndfnew <- df %>%\n  filter(Wave == 2019) %>%\n  mutate(employed_employsecurity = as.numeric(ifelse(Employed == 1, Emp.JobSecure, 0)))%>%\nfilter(!is.na(employed_employsecurity))\n\nhead(dfnew)\n\n\n# A tibble: 6 x 7\n     Id Wave  KESSLER6sum diff_k6 Emp.JobSecure Employed\n  <dbl> <fct>       <dbl>   <dbl>         <dbl>    <dbl>\n1  1713 2019            1      20             4        1\n2   564 2019            4      13            NA        0\n3  1001 2019            0      12             4        1\n4   517 2019            4      11            NA        0\n5   610 2019           14       9             7        1\n6   922 2019            5       9             7        1\n# … with 1 more variable: employed_employsecurity <dbl>\n\n# Graph\nggplot(dfnew, aes(y = diff_k6, employed_employsecurity)) +\n  geom_jitter(alpha = .2) +\n  geom_smooth(method = lm) +\n  xlab(\"employed_employsecurity\") +\n  ylab(\"Kessler 6 distress jumps over 5\") +\n  ggtitle(\"Jumps in distress change not related to employement insecurity\") +\n  scale_fill_viridis_d() + theme_classic()\n\n\n\n\nHowever, perhaps our indicator is misleading us. We can formally model the relationship between employment security and Kessler6 distress across two years\n\n\n# create dataframe with the variables we need\ndfnew2 <- df %>%\n  mutate(employed_employsecurity = as.numeric(ifelse(Employed == 1, Emp.JobSecure, 0))) %>%\n  filter(!is.na(employed_employsecurity)) %>%\n  dplyr::mutate(employsecurity_s = scale(employed_employsecurity))\n\n# multi-level model \n\nm00a<-lme4::lmer(KESSLER6sum ~  employsecurity_s * Wave + (1|Id), data = dfnew2)\nplot(ggeffects::ggpredict(m00a, terms=c(\"employsecurity_s\", \"Wave\")),\n     add.data = TRUE, jitter = 0.2, dot.alpha =.05) + geom_hline(yintercept = 5,\n             colour = \"red\",\n             linetype = \"dashed\") + \n  labs(title = \"There is a relationship between employment security and Kessler6 distress\")\n\n\n\n\nThis suggests a stable negative relationship between employment security and (low) distress. So is there are causal relationship? Not necessarily. Again, we return to casual inference in the upcoming weeks. For now, we want to alert you to an important lesson:\nPro tip 2\nDo not read too much into your descriptive analysis!\nThis is especially true when creating new variables. Just because you can make a variable doesn’t mean you should use it, or interpret it!\nPut differently, our workflow will require much more than descriptive statistics.\nData summary\nSummarise all your data\nThe skimr package\nThe skimmer package can be helpful in detecting problems. A drawback note that it is interpreting all factors as numbers).\nFor example. ( I won’t run the following code, you will do so for your homework).\n\n\nlibrary(\"skimr\")\nnz %>%\n  select(-date) %>% #not useful\n  dplyr::group_by(Wave) %>%\n  skim()\n\n\n\nHowever, I want to point out that skimr works with individual columns, and it accepts a tidy workflow.\n\n\nnz %>%\n  dplyr::group_by(Wave) %>%\n  select(KESSLER6sum,HLTH.SleepHours)  %>%\n  skim() \n\n\n\nTable 1: Data summary\n\n\n\n\n\n\nName\n\n\nPiped data\n\n\nNumber of rows\n\n\n4136\n\n\nNumber of columns\n\n\n3\n\n\n_______________________\n\n\n\n\nColumn type frequency:\n\n\n\n\nnumeric\n\n\n2\n\n\n________________________\n\n\n\n\nGroup variables\n\n\nWave\n\nVariable type: numeric\n\nskim_variable\n\n\nWave\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\nKESSLER6sum\n\n\n2018\n\n\n15\n\n\n0.99\n\n\n5.07\n\n\n3.93\n\n\n0\n\n\n2\n\n\n4\n\n\n7\n\n\n23\n\n\n▇▅▂▁▁\n\n\nKESSLER6sum\n\n\n2019\n\n\n17\n\n\n0.99\n\n\n5.20\n\n\n4.01\n\n\n0\n\n\n2\n\n\n4\n\n\n7\n\n\n22\n\n\n▇▅▂▁▁\n\n\nHLTH.SleepHours\n\n\n2018\n\n\n97\n\n\n0.95\n\n\n6.93\n\n\n1.13\n\n\n3\n\n\n6\n\n\n7\n\n\n8\n\n\n12\n\n\n▁▅▇▁▁\n\n\nHLTH.SleepHours\n\n\n2019\n\n\n69\n\n\n0.97\n\n\n6.96\n\n\n1.14\n\n\n2\n\n\n6\n\n\n7\n\n\n8\n\n\n12\n\n\n▁▃▇▁▁\n\n\nTable1 & other canned table packages\nIn earlier seminars, we encountered the table1 package, which makes really great html tables:\n\n\nlibrary(table1)\n\ntable1::table1(~Age  +\n                 GenCohort +\n                 Male + \n                 Edu +\n                 Pol.Orient + \n                 Relid + \n                 BigDoms   | Wave, data = nz,\n               overall = FALSE)\n\n\n\n2018(N=2068)\n2019(N=2068)\nAge\n\n\nMean (SD)\n50.1 (13.5)\n51.8 (13.4)\nMedian [Min, Max]\n52.0 [18.0, 90.0]\n54.1 [19.6, 91.1]\nGenCohort\n\n\nGen Boombers: born >= 1946 & b.< 1961\n693 (33.5%)\n693 (33.5%)\nGen_Silent: born< 1946\n50 (2.4%)\n50 (2.4%)\nGenX: born >=1961 & b.< 1980\n886 (42.8%)\n886 (42.8%)\nGenY: born >=1980 & b.< 1996\n387 (18.7%)\n387 (18.7%)\nGenZ: born >= 1996\n52 (2.5%)\n52 (2.5%)\nMale\n\n\nMale\n772 (37.3%)\n772 (37.3%)\nNot_Male\n1290 (62.4%)\n1289 (62.3%)\nMissing\n6 (0.3%)\n7 (0.3%)\nEdu\n\n\nMean (SD)\n5.39 (2.78)\n5.64 (2.72)\nMedian [Min, Max]\n6.00 [0, 10.0]\n7.00 [0, 10.0]\nMissing\n80 (3.9%)\n65 (3.1%)\nPol.Orient\n\n\nMean (SD)\n3.53 (1.39)\n3.57 (1.37)\nMedian [Min, Max]\n4.00 [1.00, 7.00]\n4.00 [1.00, 7.00]\nMissing\n116 (5.6%)\n63 (3.0%)\nRelid\n\n\nMean (SD)\n1.67 (2.57)\n1.53 (2.56)\nMedian [Min, Max]\n0 [0, 7.00]\n0 [0, 7.00]\nMissing\n44 (2.1%)\n99 (4.8%)\nBigDoms\n\n\nBuddhist\n18 (0.9%)\n19 (0.9%)\nChristian\n619 (29.9%)\n557 (26.9%)\nMuslim\n7 (0.3%)\n6 (0.3%)\nNot_Rel\n1322 (63.9%)\n1375 (66.5%)\nTheOthers\n70 (3.4%)\n66 (3.2%)\nMissing\n32 (1.5%)\n45 (2.2%)\n\n\nUnfortunately, the table1 package only prints html tables.\nFor publications, I might use the modelsummary package\n\n\nlibrary(\"modelsummary\")\nnnz<-nz %>%\n  dplyr::select(Age, \n                Male,\n                BigDoms,\n                Edu,\n                GenCohort,\n                Relid,\n                Pol.Orient,\n                Wave)\nmodelsummary::datasummary_balance( ~ Wave, data=nnz, dinm=FALSE, output = 'table.tex')\n\n\n\nI’ll put the \\(LaTeX\\) output into my document because I generally prefer to write in \\(LaTeX\\)\nHowever if you want to print inline, you can simply use:\n\n\nlibrary(\"modelsummary\")\nnnz<-nz %>%\n  dplyr::select(Age, \n                Male,\n                BigDoms,\n                Edu,\n                GenCohort,\n                Relid,\n                Pol.Orient,\n                Wave)\nmodelsummary::datasummary_balance( ~ Wave, data=nnz, dinm=FALSE)\n\n\n\n\n\n\n2018 (N=2068)\n\n\n\n\n2019 (N=2068)\n\n\n\n\n\n\n\nMean\n\n\nStd. Dev.\n\n\nMean\n\n\nStd. Dev.\n\n\nAge\n\n\n\n\n50.1\n\n\n13.5\n\n\n51.8\n\n\n13.4\n\n\nEdu\n\n\n\n\n5.4\n\n\n2.8\n\n\n5.6\n\n\n2.7\n\n\nRelid\n\n\n\n\n1.7\n\n\n2.6\n\n\n1.5\n\n\n2.6\n\n\nPol.Orient\n\n\n\n\n3.5\n\n\n1.4\n\n\n3.6\n\n\n1.4\n\n\n\n\n\n\nN\n\n\n%\n\n\nN\n\n\n%\n\n\nMale\n\n\nMale\n\n\n772\n\n\n37.3\n\n\n772\n\n\n37.3\n\n\n\n\nNot_Male\n\n\n1290\n\n\n62.4\n\n\n1289\n\n\n62.3\n\n\nBigDoms\n\n\nBuddhist\n\n\n18\n\n\n0.9\n\n\n19\n\n\n0.9\n\n\n\n\nChristian\n\n\n619\n\n\n29.9\n\n\n557\n\n\n26.9\n\n\n\n\nMuslim\n\n\n7\n\n\n0.3\n\n\n6\n\n\n0.3\n\n\n\n\nNot_Rel\n\n\n1322\n\n\n63.9\n\n\n1375\n\n\n66.5\n\n\n\n\nTheOthers\n\n\n70\n\n\n3.4\n\n\n66\n\n\n3.2\n\n\nGenCohort\n\n\nGen Boombers: born >= 1946 & b.< 1961\n\n\n693\n\n\n33.5\n\n\n693\n\n\n33.5\n\n\n\n\nGen_Silent: born< 1946\n\n\n50\n\n\n2.4\n\n\n50\n\n\n2.4\n\n\n\n\nGenX: born >=1961 & b.< 1980\n\n\n886\n\n\n42.8\n\n\n886\n\n\n42.8\n\n\n\n\nGenY: born >=1980 & b.< 1996\n\n\n387\n\n\n18.7\n\n\n387\n\n\n18.7\n\n\n\n\nGenZ: born >= 1996\n\n\n52\n\n\n2.5\n\n\n52\n\n\n2.5\n\n\nCreate a table using pipe functions\nAbove we saw how to create a clunky table using table(x). However, R has lots of functionality to enable better.\n\n\nlibrary(kableExtra)\nnz %>%\n  select(k6cats, Wave) %>%\n  filter(!is.na(k6cats))%>%\n  group_by( Wave, k6cats) %>%\n  summarise(n = n())%>%\n  kbl(caption = \"Distress by Year\") %>%\n   kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)%>%\n  collapse_rows()\n\n\n\nTable 2: Distress by Year\n\n\nWave\n\n\nk6cats\n\n\nn\n\n\n2018\n\n\nLow Distress\n\n\n1296\n\n\nModerate Distress\n\n\n673\n\n\nSerious Distress\n\n\n84\n\n\n2019\n\n\nLow Distress\n\n\n1264\n\n\nModerate Distress\n\n\n699\n\n\nSerious Distress\n\n\n88\n\n\nNote that we can use the pivot_wider function to spread the dataframe to enable a table that is easier to interpret.\nCredit where credit is due: I just learned about pivot_wider from Johannes and Thorven. I’m keen to get pivot_longer and pivot_wider into my vocabulary, and to do more things, like this:\n\n\nnz %>%\n  select(k6cats, Wave) %>%\n  filter(!is.na(k6cats))%>%\n  group_by( Wave, k6cats) %>%\n  summarise(n = n())%>%\n pivot_wider(names_from = Wave, values_from = n) %>%\n   kbl(caption = \"Distress counts by year\") %>%\n   kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\n\n\n\nTable 3: Distress counts by year\n\n\nk6cats\n\n\n2018\n\n\n2019\n\n\nLow Distress\n\n\n1296\n\n\n1264\n\n\nModerate Distress\n\n\n673\n\n\n699\n\n\nSerious Distress\n\n\n84\n\n\n88\n\n\nNice!\nBar graphs\nFor categorical data, in place of tables we can use bar graphs\nHere’s the table:\n\n\ntable(nz$BigDoms)\n\n\n\n Buddhist Christian    Muslim   Not_Rel TheOthers \n       37      1176        13      2697       136 \n\nHere’s the bar graph:\n\n\nggplot(nz) + \n  geom_bar(mapping = aes(x = BigDoms))\n\n\n\n\nNote that we can re-order the factor levels to produce a nicer output, using fct_infreq\n\n\nggplot(nz) + \n  geom_bar(mapping = aes(x = fct_infreq(BigDoms))  )\n\n\n\n\nMissing data graphs\nWhat do you notice about the patterns of missingness in this graph?\n\n\nlibrary(\"naniar\")\nnaniar::vis_miss(nz) #What do you notice? \n\n\n\n\nHere, we find all the problem cases:\n\n\ngg_miss_upset(nz)\n\n\n\n\nWhat explains these feature of missingess?\nBoxplots\nA box plot provides visual information for the following statistics:\nMinimum – (0p) min outlier\nMaximum – (100p) max outlier\nMedian – (50th p)\nFirst Quartile (Q1 or 25p)\nThird Quartile (Q3 or 75p)\nInterquartile range (IQR), whcih is the distance between Q1 and Q3\nOptional: the notch displays a confidence interval around the median. This is +/- 1.58 X IQR/sqrt(n). We use notches to compare differences between groups; overlap implies uncertainty about whether the medians differ.\nThere’s a simple explanation here\nWe can use base R to investigate differences in distress among big denominations:\n\n\n# using base R\nboxplot(KESSLER6sum ~ BigDoms, data = nz, notch = TRUE, col = c(\"cadetblue1\",\"orange\",\"red\",\"darkblue\",\"brown\"))\n\n\n\n\nHere’s a ggplot boxplot:\n\n\nggplot(data = nz, aes(x = KESSLER6sum, y = BigDoms, fill = BigDoms)) + \n  geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + \n  ggtitle(\"If the notches don't overlap, there's likely a difference\") + \n  geom_jitter(alpha = .05)\n\n\n\n\nHere’s a ggplot2 boxplot with points overlaid, and jittered. This allows us to se the differences in sample sizes\n\n\nggplot(data = nz, aes(x = KESSLER6sum, y = BigDoms, fill = BigDoms)) + \n  geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + \n  ggtitle(\"If the notches don't overlap, there's likely a difference\") + \n  geom_jitter(alpha = .07)\n\n\n\n\nWe could look at differences by wave:\n\n\nggplot(data = nz, aes(x = KESSLER6sum, y = BigDoms, fill = BigDoms)) + \n  geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + \n   geom_jitter(alpha = .07) + \n  facet_grid(Wave ~ .) +\n  ggtitle(\"If the notches don't overlap, there's likely a difference\") \n\n\n\n\nCorrelation graphs\nJohannes will describe a method for making a correlation plot. Here is another method.\n\n\nbzsec<-nz%>%\n  select(\n    Your.Future.Security,\n    Standard.Living,\n    NZ.Economic.Situation,\n    NZ.Business.Conditions,\n    Emp.JobSecure,\n    CharityDonate,\n    Id\n  ) %>%\n  mutate_all(., as.numeric) %>% #make numeric \n  mutate(Id = as.factor(Id),\n         CharityDonate = log(CharityDonate + 1))# make Id a factor for the \n\n# make a correlation plot using the \"correlation\" package from easystates\n\nlibrary(correlation)\np1<-bzsec %>%\n  correlation(partial = FALSE, multilevel = TRUE ) %>%\n  plot()\n\n\n\nPrint summary\n\n\nbzsec %>%\n  correlation(partial = FALSE, multilevel = TRUE ) %>%\n  summary()\n\n\nParameter              | CharityDonate | Emp.JobSecure | NZ.Business.Conditions | NZ.Economic.Situation | Standard.Living\n-------------------------------------------------------------------------------------------------------------------------\nYour.Future.Security   |       0.10*** |       0.22*** |                0.47*** |               0.32*** |         0.38***\nStandard.Living        |       0.09*** |       0.16*** |                0.26*** |               0.30*** |                \nNZ.Economic.Situation  |       0.07*** |       0.10*** |                0.43*** |                       |                \nNZ.Business.Conditions |          0.04 |       0.13*** |                        |                       |                \nEmp.JobSecure          |          0.03 |               |                        |                       |                \n\nLet’s set multilevel to FALSE.\n\n\nlibrary(correlation)\np2<-bzsec %>%\n  select(-Id)%>% # get rid of grouping variable\n  correlation(partial = FALSE, multilevel = FALSE ) %>%\n  plot()\n#print summary\nbzsec %>%\n  select(-Id)%>% # get rid of grouping variable\n  correlation(partial = FALSE, multilevel = FALSE ) %>%\n  summary()\n\n\n# Correlation Matrix (pearson-method)\n\nParameter              | CharityDonate | Emp.JobSecure | NZ.Business.Conditions | NZ.Economic.Situation | Standard.Living\n-------------------------------------------------------------------------------------------------------------------------\nYour.Future.Security   |       0.22*** |       0.31*** |                0.53*** |               0.42*** |         0.53***\nStandard.Living        |       0.25*** |       0.26*** |                0.32*** |               0.36*** |                \nNZ.Economic.Situation  |       0.12*** |       0.14*** |                0.52*** |                       |                \nNZ.Business.Conditions |       0.10*** |       0.20*** |                        |                       |                \nEmp.JobSecure          |       0.10*** |               |                        |                       |                \n\np-value adjustment method: Holm (1979)\n\n\n\nlibrary(patchwork)\n# create a two panel plot\np1 / p2 + \n  plot_annotation(title = \"Plot of multilevel (a) and single-level (b) correlation\", tag_levels = 'a')\n\n\n\n\nWe can see an even greater correlations between the variables. This is because the model does not adjust for the repeated measures, which create dependencies in the data.\nThe report package\nThe reports package from the easystats group is powerful tool for saving tame. Before extolling its virtues, I’d like to point out two major limitations.\nFirst, the package is in development. Currently, it has lots of bugs.\nSecond, the package uses terminology that won’t work for all contexts and purposes. For example, it uses the term “significant” to describe p values that are below the traditional p = .05 threshold.\nIf you learn nothing else from this course, you should learn never to use “significant” to describe a p value. You may, if you like, use “statistically signficant” however it would be better altogether if you simply dropped p-values from data analysis. We’ll show you how. With those provisos in mind, consider some useful functionality from the report package.\n\n\n# create a demographic dataframe\nnz_demagraphics <- nz %>%\n  select(Age, GenCohort, Male, Edu, Pol.Orient, Relid, BigDoms, Wave)\n\n# now a nice way to save you time when reporting\npaste(\n  report::report_participants(\n    nz_demagraphics, \n    group = \"Wave\", \n    age = \"Age\",\n    sex = \"Male\",\n    education = \"Edu\",\n    spell_n = TRUE),\n  \"were recruited in the study by through enticement by lollipops. Those who did not volunteer were coerced.\"\n  )\n\n\n[1] \"For the 'Wave - 2018' group: Two Thousand, Sixty Eight participants (Mean age = 50.1, SD = 13.5, range: [18, 90]; 0.0% females; Mean education = 5.4, SD = 2.8, range: [0, 10]) and for the 'Wave - 2019' group: Two Thousand, Sixty Eight participants (Mean age = 51.8, SD = 13.4, range: [19.6, 91.08]; 0.0% females; Mean education = 5.6, SD = 2.7, range: [0, 10]) were recruited in the study by through enticement by lollipops. Those who did not volunteer were coerced.\"\n\nThe table function of report isn’t great yet. However it has some nice features. For example you should always report your session information, and doing so in tabluar form clarifies the elements\nTry running the following code on your own:\n\n\nr <- report_table(sessionInfo())\nr\n\n\n\nHere is another method, which you can try on your own\n\n\ncite_packages()\n\n\n\nHere’s a demographic table (try on your own)\n\n\nreport_table(nz_demagraphics)\n\n\n\nHere’s a data summary\n\n\nlibrary(\"report\")\nnz %>%\n  group_by(Wave)%>%\n  select(\n    \"Wave\", \n    \"Age\",\n    \"Male\",\n    \"Edu\",\n    \"Relid\",\n    \"Pol.Orient\",\n    \"KESSLER6sum\",\n    \"FeelHopeless\",\n    \"FeelDepressed\",\n    \"FeelRestless\",\n    \"EverythingIsEffort\",\n    \"FeelWorthless\",\n    \"FeelNervous\"\n    )%>%\n  report() %>% \n  summary()\n\n\nThe data contains 4136 observations, grouped by Wave, of the following variables:\n\n- 2018 (n = 2068):\n  - Age: Mean = 50.07, SD = 13.49, range: [18, 90]\n  - Male: 2 levels, namely Male (n = 772), Not_Male (n = 1290) and missing (n = 6)\n  - Edu: Mean = 5.39, SD = 2.78, range: [0, 10], 3.87% missing\n  - Relid: Mean = 1.67, SD = 2.57, range: [0, 7], 2.13% missing\n  - Pol.Orient: Mean = 3.53, SD = 1.39, range: [1, 7], 5.61% missing\n  - KESSLER6sum: Mean = 5.07, SD = 3.93, range: [0, 23], 0.73% missing\n  - FeelHopeless: 5 levels, namely None Of The Time (n = 1046), A Little Of The Time (n = 600), Some Of The Time (n = 338), Most Of The Time (n = 54), All Of The Time (n = 4) and missing (n = 26)\n  - FeelDepressed: 5 levels, namely None Of The Time (n = 1469), A Little Of The Time (n = 361), Some Of The Time (n = 170), Most Of The Time (n = 43), All Of The Time (n = 5) and missing (n = 20)\n  - FeelRestless: 5 levels, namely None Of The Time (n = 535), A Little Of The Time (n = 763), Some Of The Time (n = 594), Most Of The Time (n = 138), All Of The Time (n = 16) and missing (n = 22)\n  - EverythingIsEffort: 5 levels, namely None Of The Time (n = 551), A Little Of The Time (n = 795), Some Of The Time (n = 501), Most Of The Time (n = 158), All Of The Time (n = 40) and missing (n = 23)\n  - FeelWorthless: 5 levels, namely None Of The Time (n = 1433), A Little Of The Time (n = 356), Some Of The Time (n = 186), Most Of The Time (n = 67), All Of The Time (n = 7) and missing (n = 19)\n  - FeelNervous: 5 levels, namely None Of The Time (n = 620), A Little Of The Time (n = 789), Some Of The Time (n = 482), Most Of The Time (n = 146), All Of The Time (n = 13) and missing (n = 18)\n\n- 2019 (n = 2068):\n  - Age: Mean = 51.83, SD = 13.45, range: [19.55, 91.08]\n  - Male: 2 levels, namely Male (n = 772), Not_Male (n = 1289) and missing (n = 7)\n  - Edu: Mean = 5.64, SD = 2.72, range: [0, 10], 3.14% missing\n  - Relid: Mean = 1.53, SD = 2.56, range: [0, 7], 4.79% missing\n  - Pol.Orient: Mean = 3.57, SD = 1.37, range: [1, 7], 3.05% missing\n  - KESSLER6sum: Mean = 5.20, SD = 4.01, range: [0, 22], 0.82% missing\n  - FeelHopeless: 5 levels, namely None Of The Time (n = 977), A Little Of The Time (n = 639), Some Of The Time (n = 349), Most Of The Time (n = 79), All Of The Time (n = 4) and missing (n = 20)\n  - FeelDepressed: 5 levels, namely None Of The Time (n = 1434), A Little Of The Time (n = 377), Some Of The Time (n = 193), Most Of The Time (n = 35), All Of The Time (n = 5) and missing (n = 24)\n  - FeelRestless: 5 levels, namely None Of The Time (n = 517), A Little Of The Time (n = 757), Some Of The Time (n = 608), Most Of The Time (n = 146), All Of The Time (n = 15) and missing (n = 25)\n  - EverythingIsEffort: 5 levels, namely None Of The Time (n = 544), A Little Of The Time (n = 804), Some Of The Time (n = 502), Most Of The Time (n = 156), All Of The Time (n = 35) and missing (n = 27)\n  - FeelWorthless: 5 levels, namely None Of The Time (n = 1407), A Little Of The Time (n = 375), Some Of The Time (n = 195), Most Of The Time (n = 60), All Of The Time (n = 9) and missing (n = 22)\n  - FeelNervous: 5 levels, namely None Of The Time (n = 568), A Little Of The Time (n = 850), Some Of The Time (n = 468), Most Of The Time (n = 133), All Of The Time (n = 24) and missing (n = 25)\n\nNotes:\nMore about the report package: here\nThis package is brought to you by easystats\nMeasures\nWhen reporting your study, it is extremely important to include information about your measure. For example:\nWe measure psychological distress using the Kessler-6 scale (R. C. Kessler et al. 2002), which exhibits strong diagnostic concordance for moderate and severe psychological distress in large, cross-cultural samples Prochaska et al. (2012). Participants rated during the past 30 days, how often did… (a) “\\(\\dots\\) you feel hopeless”; (b) “\\(\\dots\\) you feel so depressed that nothing could cheer you up”; (c) \\(\\dots\\) you feel restless or fidgety”; (d)“\\(\\dots\\) you feel that everything was an effort”; (e) “\\(\\dots\\) you feel worthless”; (f) “\\(\\dots\\) you feel nervous?” Ordinal response alternatives for the Kessler-6 are: “None of the time”; “A little of the time”; “Some of the time”; “Most of the time”; “All of the time.”\nWe report sample descriptive statistics for indicators of personal Kessler-6 distress below in Table1.\nTable 1\n\n\nlibrary(gtsummary)\ntb1 <-nz %>%\n  dplyr::select(\n    KESSLER6sum,\n    FeelHopeless,\n    FeelDepressed,\n    FeelRestless,\n    EverythingIsEffort,\n    FeelWorthless,\n    FeelNervous,\n    Wave,\n  ) %>%\n  gtsummary::tbl_summary(\n    by = Wave,\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} / {N} ({p}%)\"\n    ),\n    digits = all_continuous() ~ 2,\n    missing_text = \"(Missing)\"\n  )%>%\n  bold_labels() \ntb1\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#myqyvvhdwo .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#myqyvvhdwo .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#myqyvvhdwo .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#myqyvvhdwo .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#myqyvvhdwo .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#myqyvvhdwo .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#myqyvvhdwo .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#myqyvvhdwo .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#myqyvvhdwo .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#myqyvvhdwo .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#myqyvvhdwo .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#myqyvvhdwo .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#myqyvvhdwo .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#myqyvvhdwo .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#myqyvvhdwo .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#myqyvvhdwo .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#myqyvvhdwo .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#myqyvvhdwo .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#myqyvvhdwo .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#myqyvvhdwo .gt_left {\n  text-align: left;\n}\n\n#myqyvvhdwo .gt_center {\n  text-align: center;\n}\n\n#myqyvvhdwo .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#myqyvvhdwo .gt_font_normal {\n  font-weight: normal;\n}\n\n#myqyvvhdwo .gt_font_bold {\n  font-weight: bold;\n}\n\n#myqyvvhdwo .gt_font_italic {\n  font-style: italic;\n}\n\n#myqyvvhdwo .gt_super {\n  font-size: 65%;\n}\n\n#myqyvvhdwo .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nCharacteristic\n      2018, N = 2,0681\n      2019, N = 2,0681\n    KESSLER6sum\n      5.07 (3.93)\n      5.20 (4.01)\n    (Missing)\n      15\n      17\n    FeelHopeless\n      \n      \n    None Of The Time\n      1,046 / 2,042 (51%)\n      977 / 2,048 (48%)\n    A Little Of The Time\n      600 / 2,042 (29%)\n      639 / 2,048 (31%)\n    Some Of The Time\n      338 / 2,042 (17%)\n      349 / 2,048 (17%)\n    Most Of The Time\n      54 / 2,042 (2.6%)\n      79 / 2,048 (3.9%)\n    All Of The Time\n      4 / 2,042 (0.2%)\n      4 / 2,048 (0.2%)\n    (Missing)\n      26\n      20\n    FeelDepressed\n      \n      \n    None Of The Time\n      1,469 / 2,048 (72%)\n      1,434 / 2,044 (70%)\n    A Little Of The Time\n      361 / 2,048 (18%)\n      377 / 2,044 (18%)\n    Some Of The Time\n      170 / 2,048 (8.3%)\n      193 / 2,044 (9.4%)\n    Most Of The Time\n      43 / 2,048 (2.1%)\n      35 / 2,044 (1.7%)\n    All Of The Time\n      5 / 2,048 (0.2%)\n      5 / 2,044 (0.2%)\n    (Missing)\n      20\n      24\n    FeelRestless\n      \n      \n    None Of The Time\n      535 / 2,046 (26%)\n      517 / 2,043 (25%)\n    A Little Of The Time\n      763 / 2,046 (37%)\n      757 / 2,043 (37%)\n    Some Of The Time\n      594 / 2,046 (29%)\n      608 / 2,043 (30%)\n    Most Of The Time\n      138 / 2,046 (6.7%)\n      146 / 2,043 (7.1%)\n    All Of The Time\n      16 / 2,046 (0.8%)\n      15 / 2,043 (0.7%)\n    (Missing)\n      22\n      25\n    EverythingIsEffort\n      \n      \n    None Of The Time\n      551 / 2,045 (27%)\n      544 / 2,041 (27%)\n    A Little Of The Time\n      795 / 2,045 (39%)\n      804 / 2,041 (39%)\n    Some Of The Time\n      501 / 2,045 (24%)\n      502 / 2,041 (25%)\n    Most Of The Time\n      158 / 2,045 (7.7%)\n      156 / 2,041 (7.6%)\n    All Of The Time\n      40 / 2,045 (2.0%)\n      35 / 2,041 (1.7%)\n    (Missing)\n      23\n      27\n    FeelWorthless\n      \n      \n    None Of The Time\n      1,433 / 2,049 (70%)\n      1,407 / 2,046 (69%)\n    A Little Of The Time\n      356 / 2,049 (17%)\n      375 / 2,046 (18%)\n    Some Of The Time\n      186 / 2,049 (9.1%)\n      195 / 2,046 (9.5%)\n    Most Of The Time\n      67 / 2,049 (3.3%)\n      60 / 2,046 (2.9%)\n    All Of The Time\n      7 / 2,049 (0.3%)\n      9 / 2,046 (0.4%)\n    (Missing)\n      19\n      22\n    FeelNervous\n      \n      \n    None Of The Time\n      620 / 2,050 (30%)\n      568 / 2,043 (28%)\n    A Little Of The Time\n      789 / 2,050 (38%)\n      850 / 2,043 (42%)\n    Some Of The Time\n      482 / 2,050 (24%)\n      468 / 2,043 (23%)\n    Most Of The Time\n      146 / 2,050 (7.1%)\n      133 / 2,043 (6.5%)\n    All Of The Time\n      13 / 2,050 (0.6%)\n      24 / 2,043 (1.2%)\n    (Missing)\n      18\n      25\n    \n        \n          1\n          \n           \n          Mean (SD); n / N (%)\n          \n      \n    \n\nNote that you can use the gtsummary package to create in-line referencing. For example: Average Kessler-6 distress in 2018 was 5.07 (3.93) and in 2019 was 5.20 (4.01).\nOrder of your Method section\nThe following is a brief guide to describing your method. We’ll be returning to report writing in future weeks. For now, I just want to put this on the table for you. The advice is just a guide.\nHeading\nInclude\nParticipants\nParticipant or subject characteristics\nSampling procedures\nSample size and power\n\nMaterials\nPrimary and secondary measures\nQuality of measurements\n\nProcedure\nData collection methods\nResearch design (e.g., experimental, correlational, or descriptive)\nData processing and diagnostics (e.g., outlier removal)\nData analysis strategy (e.g., comparison or regression tests)\n\nBelow are the sampling procedures from the New Zealand Attitudes and Values Study, from where the nz teaching dataset was drawn.\nAppendix 1A Sampling Procedure – NZAVS Time 10 (2018; conducted from 18.06.2018-28.09.2019)\nThe Time 10 (2018) NZAVS contained responses from 47,951 participants (18,010 retained from one or more previous wave. The sample retained 2,964 participants from the Time 1 (2009) sample (a retention rate of 45.5%). The sample retained 14,049 participants from Time 9 (2017; a retention rate of 82.3% from the previous year). Participants who provided an email address were first emailed and invited to complete an online version if they preferred. Participants who did not complete the online version (or did not provide an email) were then posted a copy of the questionnaire, with a second postal follow-up two months later. We staggered the time of contact, so that participants who had completed the previous wave were contacted approximately one year after they last completed the questionnaire. We offered a prize draw for participation (five draws each for $1000 grocery vouchers, $5000 total prize pool). All participants were posted a Season’s Greetings card from the NZAVS research team and informed that they had been automatically entered into a bonus seasonal grocery voucher prize draw. Participants were also emailed an eight-page newsletter about the study.\nTo boost sample size and increase sample diversity for subsequent waves, a booster sample was conducted by selecting people from the New Zealand electoral roll. As with previous booster samples, sampling was conducted without replacement (i.e., people included in previous sample frames were identified and removed from the 2018 roll). The sample frame consisted of 325,000 people aged from 18-65 randomly selected from the 2018 New Zealand Electoral Roll, who were currently residing in New Zealand (one can be registered to vote in New Zealand but living overseas). The electoral roll contained ~3,250,000 registered voters. The New Zealand Electoral Roll contains participants’ date of birth (within a one-year window), and we limited our frame to people who 65 or younger, due to our aim of retaining participants longitudinally. We concurrently advertised the survey on Facebook via a $5000 paid promotion of a link to a YouTube video describing the NZAVS and the large booster sample we were conducting. The advertisement targeted men and women aged 18-65+ who lived in New Zealand and ran for 14 days. This paid promotion reached 147,296 people, with 4,721 link clicks (i.e., clicking to watch the video), according to Facebook. The goal of the paid promotion was twofold: (a) to increase name recognition of the NZAVS during the period in which questionnaires were being posted, and (b) to help improve retention by potentially reaching previous participants who happened to see the advertisement. A total of 29,293 participants who were contained in our sample frame completed the questionnaire (response rate = 9.2% when adjusting for the 98.2% accuracy of the 2018 electoral roll). A further 648 participants completed the questionnaire, but were unable to be matched to our sample frame (for example, due to a lack of contact information) or were unsolicited opt-ins. Informal analysis indicates that unsolicited opt-ins were often the partners of existing participants.\nAppendix 1B Sampling Procedure – NZAVS Time 11 (2019; conducted from 29.09.2019-17.10.2020)\nThe Time 11 wave was conducted during COVID-19 pandemic. Procedures thus differed in that there was an increased focus on online deliver using email reminders and extensive Facebook advertising, no Christmas card, and incomplete phoning of non-respondents.\nThe Time 11 (2019) NZAVS contained responses from 42,684 participants (36,522 retained from one or more previous wave. The sample retained 2,506 participants from the Time 1 (2009) sample (a retention rate of 38.4%). The sample retained 34,782 participants from Time 10 (2018; a retention rate of 72.5% from the previous year). Participants who provided an email address were first emailed and invited to complete an online version if they preferred. Participants who did not complete the online version (or did not provide an email) were then posted a copy of the questionnaire, with a second postal follow-up two months later. We staggered the time of contact, so that participants who had completed the previous wave were contacted approximately one year after they last completed the questionnaire. A second reminder email was sent approximately four months following the first email attempt. We offered a prize draw for participation (five draws each for $1000 grocery vouchers, $5000 total prize pool). Participants were also emailed an eight-page newsletter about the study. As in past years, three attempts were made to phone non-respondents using each available cell and landline number. However, due to the university closure during COVID-19 lockdowns, phoning attempts were made for only 54% of the phoning pool (11,687 from a total of 21,636 non-respondents who provided at least one phone number).\nTwo additional forms of recruitment were also introduced during Time 11. The first was a large information box in the questionnaire (taking a full page on the paper version), which asked people: ‘Do you have a partner who would also like to join the NZAVS?’ with additional details for how partners might join the study (see questionnaire for the full text). The second was a Facebook advertisement. The advertisement targeted men and women aged 18-65+ who lived in New Zealand and ran from and 4th April 2020 – 4th July 2020 (overlapping with New Zeeland’s first lockdown period and recovery), and again from 18th August 2020 – 4th September (during the second Auckland lockdown). Given the unprecedented nature of the COVID-19 lockdowns, we thought it important to maximise sampling during these periods. The goal of the Facebook advertisement was threefold: (a) to increase name recognition of the NZAVS and remind people to complete the paper/online version already posted/emailed to them, (b) to help improve retention by potentially reaching previous lost participants who happened to see the advertisement, and (c) to recruit new participants (and also the partners of existing participants) while people were at home with some possibly having more free time during lockdown. This last goal was indirect and not explicitly stated it in the advertisement.\nThe Facebook advertisement read as follows: “Participate in the New Zealand Attitudes and Values Study. Complete the 2020 Questionnaire online” with the body of text: “If you are part of the NZAVS, but have not heard from us in the last year, then please consider completing the 2020 questionnaire online. The study is more important than ever as we aim to understand the impact of COVID-19 on mental health, wellbeing and resilience in our communities. We wish you all the best at this time and hope you keep well and stay safe.” This paid promotion reached 883,969 people, with 37,850 link clicks (i.e., clicking the link for the Qualtrics survey) according to Facebook. A total of 6106 people continued complete the questionnaire and provide full contact details, and were thus included in the dataset (4734 were new participants opting in to the study, and 1372 were previously ‘lost’ participants).\nAppendix 2 Johannes’s mini-lecture on the papaja package\nLecture\nPapaja R markdown template\nAppendix 3 Style advice about research methods\nAPA style advice here\n\n\n\nKessler, R C, G Andrews, L J Colpe, E Hiripi, D K Mroczek, S L T Normand, E E Walters, and A M Zaslavsky. 2002. “Short Screening Scales to Monitor Population Prevalences and Trends in Non-Specific Psychological Distress.” Psychol. Med. 32 (6): 959–76.\n\n\nKessler, Ronald C, Jennifer Greif Green, Michael J Gruber, Nancy A Sampson, Evelyn Bromet, Marius Cuitan, Toshi A Furukawa, et al. 2010. “Screening for Serious Mental Illness in the General Population with the K6 Screening Scale: Results from the WHO World Mental Health (WMH) Survey Initiative.” Int. J. Methods Psychiatr. Res. 19 Suppl 1 (June): 4–22.\n\n\nProchaska, Judith J, Hai-Yen Sung, Wendy Max, Yanling Shi, and Michael Ong. 2012. “Validity Study of the K6 Scale as a Measure of Moderate Mental Distress Based on Mental Health Treatment Need and Utilization.” Int. J. Methods Psychiatr. Res. 21 (2): 88–97.\n\n\nNotice, the intercept here is zero. This because we centered the new indicator at zero, and we wrote a model that is estimating the population average for this outcome (an intercept-only model). Don’t worry if you don’t know what an intercept is, we’ll get to regression in a few weeks.↩︎\n",
    "preview": "posts/4_1/op.png",
    "last_modified": "2021-03-17T22:42:30+13:00",
    "input_file": {},
    "preview_width": 10241,
    "preview_height": 8450
  },
  {
    "path": "posts/3_1/",
    "title": "Visualisation",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\n\nContents\nData visualisation with ggplot2\nIntroduction\nCreating a graph\nUsing ggplot 2 to highlight elements of interest.\nFacets\nUnderstanding your data through graphs\nTransforming data\nRevisiting logical operators\nCommand filter: keeps rows matching criteria\nTask\nCommand select: picks columns by column name\nCommand arrange reorders rows\nCommand mutate add new variable name\nCommand summarise reduce variables to values\nMultiple pipe operators\nOther functions\n\n\n\nData visualisation with ggplot2\n\n\n\nIntroduction\nIn this lecture we’ll first introduce you to the ggplot2 package, and vocabulary, for creating graphs in R. We’ll mostly follow the approach described in the book “R for data science,” which can be found here.\nWe’ll then turn to data-wrangling using the dplyr package.\nBoth ggplot2 and dplyr can be found in library(tidyverse)\nCreating a graph\nStep 1, load tidyverse:\n\n\nlibrary(\"tidyverse\")\n\n\n\nStep 2, Make sure your dataset is loaded. We’ll start with the mpg dataset\n\n\n#inspect the mpg dataset\nhead(mpg)\n\n\n# A tibble: 6 x 11\n  manufacturer model displ  year   cyl trans   drv     cty   hwy fl   \n  <chr>        <chr> <dbl> <int> <int> <chr>   <chr> <int> <int> <chr>\n1 audi         a4      1.8  1999     4 auto(l… f        18    29 p    \n2 audi         a4      1.8  1999     4 manual… f        21    29 p    \n3 audi         a4      2    2008     4 manual… f        20    31 p    \n4 audi         a4      2    2008     4 auto(a… f        21    30 p    \n5 audi         a4      2.8  1999     6 auto(l… f        16    26 p    \n6 audi         a4      2.8  1999     6 manual… f        18    26 p    \n# … with 1 more variable: class <chr>\n\nStep 3. Inspect the\"Negative relationship between highway fuel efficiency and a cars engine size (which is given by the variable displ).\n\n\n# Create graph\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Negative relationship between engine displacement and fuel efficiency.\")\n\n\n\n\nA basic problem with this graph is that we don’t know what it is representing. To avoid this problem, it is useful to get into the habit of adding titles to your graphs, and also of using informative axis labels. We do this by adding additional layers.\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Negative relationship between engine displacement and fuel efficiency.\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\")\n\n\n\n\nLet’s walk through the logic of the ggplot2 “grammar”:\nFirst we call the data\n\n\n# here we are calling up the data\nggplot(data = mpg)\n\n\n\nNext, we add a layer of points, by calling the relevant columns and rows of this dataset\n\n\n# Here, we add a layer of points, by calling the relevant columns and rows of this dataset\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\nThen we add the title\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title =  \"Negative relationship between engine displacement and fuel efficiency.\")\n\n\n\n\nThen we add the labels\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\")   + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\nWe can change the axis starting positions:\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\")   + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") + expand_limits(x = 0, y = 0)\n\n\n\n\nThe generic method for adding layers is as follows:\n\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\nUsing ggplot 2 to highlight elements of interest.\nHere we can use the “color =” option.1\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) \n\n\n\n\nHere’s a shape command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape = class)) \n\n\n\n\nHere’s a size command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, size = cty)) \n\n\n\n\nHere’s the fill command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, fill = cty)) \n\n\n\n\nHere’s the alpha command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, alpha  = .1)) \n\n\n\n\nHere’s the alpha command combined with the fill command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, alpha = cty, size = cty)) \n\n\n\n\nFacets\nWe can create multiple graphs using facets\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) + \n   facet_wrap(~ class, nrow = 2)\n\n\n\n\nWe use facet_grid for graphing the Negative relationship between two variables.\nNote the difference betwen these two graphs:\nHere the focus is on the negative relationship between class and the x variable, displacement\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_grid(class ~ .)  + theme(legend.position = \"none\") \n\n\n\n\nHere the focus is on the relationship betwen class and the y variable, highway milage.\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_grid(. ~ class) + theme(legend.position = \"none\") \n\n\n\n\nWe can focus on Negative relationship between class and the x and y variables simultaneously. Here we add the ’year` indicator and we do not see much of an improvement in highway milage for the different classes, adjusting for displacement:\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_grid(class ~ year) + theme(legend.position = \"bottom\") +\n  labs(title =\"Negative relationship between engine displacement and fuel efficiency by class.\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\nUnderstanding your data through graphs\nWe can create a graph of relationships:\n\n\n# set better theme\ntheme_set(theme_classic())\nggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy)) + \n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\") +\n  xlab(\"Engine displacement in (units)\") +\n  ylab(\"Highway miles per liter\") \n\n\n\n\nAdd points as a layer\n\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  geom_smooth(mapping = aes(x = displ, y = hwy)) +\n  theme(legend.position = \"bottom\") +\n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\") +\n  xlab(\"Engine displacement in (units)\") +\n  ylab(\"Highway miles per liter\") \n\n\n\n\nWe can write this more compactly, by including the mapping with the data layer\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\nThen we can include mappings for specific layers\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n\n\n\n\nWe can add a grouping factor e.g. for “drv”, thus creating multiple lines\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, group = drv)) + \n  geom_point(aes(color = class)) + \n  geom_smooth()\n\n\n\n\nWe can replace the smooths with linear models\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, linetype = drv)) + \n  geom_point(aes(color = class)) + \n  geom_smooth(method = \"lm\")\n\n\n\n\nTransforming data\nFirst we’ll get the flights data\n\n\nlibrary(nycflights13)\nhead(flights)\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\nNext we’ll create some data frames to help us illustrate points\n\n\ndf <- data.frame(\ncolour = c(\"blue\", \"black\", \"blue\", \"blue\", \"black\"), value = 1:5)\nhead(df)\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n5  black     5\n\nRevisiting logical operators\nRecall our logical operators. These will be essential for data wrangling\n\n\nknitr::include_graphics(\"logic.png\")\n\n\n\n\nCommand filter: keeps rows matching criteria\nKeep only blue rows:\n\n\ndf%>%\nfilter(colour == \"blue\")\n\n\n  colour value\n1   blue     1\n2   blue     3\n3   blue     4\n\nKeep only values 1 and 4\n\n\ndf%>%\n  filter (value %in% c(1,4))\n\n\n  colour value\n1   blue     1\n2   blue     4\n\nKeep values 1 through 4\n\n\ndf %>%\n  filter (value %in% c(1:4))\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n\nAnother way to do the same\n\n\ndf %>%\n  filter (value != 5)\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n\nTask\nHow can we find all flights that left in January?\n\n\nhead(flights)\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\n\nflights%>%\n  dplyr::filter(month ==1)\n\n\n# A tibble: 27,004 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 26,994 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nFlights delayed by more than 15 mintutes that arrived on time\n\n\nflights%>%\n  dplyr::filter (dep_delay >15 & arr_delay <=0)\n\n\n# A tibble: 4,314 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1     1025            951        34     1258\n 2  2013     1     1     1033           1017        16     1130\n 3  2013     1     1     2052           2029        23     2349\n 4  2013     1     1     2107           2040        27     2354\n 5  2013     1     2      727            645        42     1024\n 6  2013     1     2     1004            945        19     1251\n 7  2013     1     2     1031           1015        16     1135\n 8  2013     1     2     1500           1430        30     1741\n 9  2013     1     2     1737           1720        17     1908\n10  2013     1     2     1831           1815        16     2130\n# … with 4,304 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nCommand select: picks columns by column name\nSelect the colour column\n\n\ndf%>%\n  dplyr::select ( colour )\n\n\n  colour\n1   blue\n2  black\n3   blue\n4   blue\n5  black\n\nAnother way?\n\n\ndf%>%\n  dplyr::select ( !value )\n\n\n  colour\n1   blue\n2  black\n3   blue\n4   blue\n5  black\n\nor\n\n\ndf%>%\n  dplyr::select ( -c(value ))\n\n\n  colour\n1   blue\n2  black\n3   blue\n4   blue\n5  black\n\nCommand arrange reorders rows\n\n\ndf %>%\n  arrange(value)\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n5  black     5\n\n\n\ndf %>%\n  arrange(desc(value))\n\n\n  colour value\n1  black     5\n2   blue     4\n3   blue     3\n4  black     2\n5   blue     1\n\nTask: how would we order flights by departure data and time ?\n\n\nflights %>%\n  arrange(month, day, dep_time)\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nTask which flights have the greated difference between departure delay and arrival delay?\n\n\nflights%>%\n  arrange(desc(dep_delay - arr_delay))\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     6    13     1907           1512       235     2134\n 2  2013     2    26     1000            900        60     1513\n 3  2013     2    23     1226            900       206     1746\n 4  2013     5    13     1917           1900        17     2149\n 5  2013     2    27      924            900        24     1448\n 6  2013     7    14     1917           1829        48     2109\n 7  2013     7    17     2004           1930        34     2224\n 8  2013    12    27     1719           1648        31     1956\n 9  2013     5     2     1947           1949        -2     2209\n10  2013    11    13     2024           2015         9     2251\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nNot this could be written briefly as this:\n\n\narrange(flights, desc(dep_delay - arr_delay))\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     6    13     1907           1512       235     2134\n 2  2013     2    26     1000            900        60     1513\n 3  2013     2    23     1226            900       206     1746\n 4  2013     5    13     1917           1900        17     2149\n 5  2013     2    27      924            900        24     1448\n 6  2013     7    14     1917           1829        48     2109\n 7  2013     7    17     2004           1930        34     2224\n 8  2013    12    27     1719           1648        31     1956\n 9  2013     5     2     1947           1949        -2     2209\n10  2013    11    13     2024           2015         9     2251\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nCommand mutate add new variable name\n\n\ndf %>%\n  mutate(double_value = 2 * value)\n\n\n  colour value double_value\n1   blue     1            2\n2  black     2            4\n3   blue     3            6\n4   blue     4            8\n5  black     5           10\n\nOrder flights by greatest difference between departure delay and arrival delay?\n\n\nflights %>%\n  mutate(diff_dep_arr = dep_delay - arr_delay)%>%\n  select(flight,diff_dep_arr)%>%\n  arrange(desc(diff_dep_arr))\n\n\n# A tibble: 336,776 x 2\n   flight diff_dep_arr\n    <int>        <dbl>\n 1   4377          109\n 2     51           87\n 3     51           80\n 4   1465           79\n 5     51           76\n 6    673           74\n 7   1532           74\n 8   1284           73\n 9    612           73\n10    427           72\n# … with 336,766 more rows\n\nCommand summarise reduce variables to values\nSum all values in the df dataset\n\n\ndf %>%\n  summarise (total = sum(value))\n\n\n  total\n1    15\n\nSummaries the values by colour groups, and give the number of items per colour group\n\n\ndf %>%\n  group_by(colour) %>%\n  summarise(total = sum(value),\n            n = n())\n\n\n# A tibble: 2 x 3\n  colour total     n\n  <chr>  <int> <int>\n1 black      7     2\n2 blue       8     3\n\nUseful summary functions are:\nmin(x)\nmax(x)\nmean(x)\nn\nn_distinct\nsum(x)\nsum(x > 10)\nmean(x > 10)\nsd(x)\nvar(x)\nTask, how many flights flew on Christmas?\n\n\nhead(flights)\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\n\nflights %>%\n  filter( month == 12, day == 25)%>%\n  summarise (n = n())\n\n\n# A tibble: 1 x 1\n      n\n  <int>\n1   719\n\nCalculate average delay:\n\n\nflights %>%\n  summarise(delay = mean(dep_delay, na.rm = TRUE))\n\n\n# A tibble: 1 x 1\n  delay\n  <dbl>\n1  12.6\n\n\n\nsummarise(flights, delay = mean(dep_delay, na.rm = TRUE))\n\n\n# A tibble: 1 x 1\n  delay\n  <dbl>\n1  12.6\n\nMultiple pipe operators\nHere we:\nGroup flights by destination.\nSummarise to compute distance, average delay, and number of flights.\nRemove Honolulu airport, because it is so far away\n\n\ndelays <- flights %>% \n  group_by(dest) %>% \n  summarise(\n    count = n(),\n    dist = mean(distance, na.rm = TRUE),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) %>% \n  filter(dest != \"HNL\")\nhead(delays)\n\n\n# A tibble: 6 x 4\n  dest  count  dist delay\n  <chr> <int> <dbl> <dbl>\n1 ABQ     254 1826   4.38\n2 ACK     265  199   4.85\n3 ALB     439  143  14.4 \n4 ANC       8 3370  -2.5 \n5 ATL   17215  757. 11.3 \n6 AUS    2439 1514.  6.02\n\n\n\nflights %>% \n  filter(!is.na(dep_delay), !is.na(arr_delay)) %>% # not cancelled\n   group_by(tailnum) %>% # group by unique aircraft\n  summarise(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  ) %>%\n  ggplot(mapping = aes(x = n, y = delay)) + \n  geom_point(alpha = 1/10)  + \n  labs(title = \"Variation in average delay by tailnumber \") \n\n\n\n\nOther functions\nSuppose you only wanted to keep your mutated variables, in this case you can use transmute\n\n\nnew_flights <-transmute(flights,\n  gain = dep_delay - arr_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n)\nhead(new_flights)\n\n\n# A tibble: 6 x 3\n   gain hours gain_per_hour\n  <dbl> <dbl>         <dbl>\n1    -9  3.78         -2.38\n2   -16  3.78         -4.23\n3   -31  2.67        -11.6 \n4    17  3.05          5.57\n5    19  1.93          9.83\n6   -16  2.5          -6.4 \n\nTo learn more, go to https://dplyr.tidyverse.org/\n\nRemoving the axis and labels here just to keep the code compact↩︎\n",
    "preview": "posts/3_1/op.jpg",
    "last_modified": "2021-03-17T01:36:04+13:00",
    "input_file": {}
  },
  {
    "path": "posts/2_1/",
    "title": "Coding basics",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-03-02",
    "categories": [],
    "contents": "\n\nContents\nTo do:\nFundamentals of R\nHow to use R as calculator\nInspecting data\ncheck head of dataset sing head\ncheck data types using str\nnames\nview rownames\ntable (and use of $)\nchange column names\n\nBasic data structures\nvectors of numbers ( and use of c and i:x )\nvectors of characters\ndataframes (2 dimensional square arrays of vectors)\nrename columns of a data frame\nmatrix\nlists\n\nClasses in R\nnumeric and integer\nfactors\n\nIndexing in R\ncolumns\nrows\nrows and columns\nselection by negation\n\nBasic data wrangling in R\nuse of $ and [i:x]\n\nIndexing for logical operations\ndefinitions\nevaluation using logical operators\n\nThe basic structure of R commands\nmean\nsd\nsummary\nCoding\nInstalling package\n\nRolling your own code\nCoding etiquette\n\nusing R!\ndata summary\nmodel\nresults\ngraph predicted effects\nWhat is the advantage of this graph?\ntry another model\nAppendix: # symbol is for commenting code\nRounding\n\n\nTo do:\nA good, and free introduction to R for Data Science Read chapters: 2-8 (they are short chapters.)\nFundamentals of R\nA console runs all your code in R\nA source page runs all your code in R.\nA working directory is where R will look for raw data and other material.\nIn R-studio, (for starters) you can can use the file tab to import and save your material.\nSimilarly, in R-studio, (for starters) you can can use install packages by clicking the Install tab in the package pane (generally lower right pane) to install packagees.\nHow to use R as calculator\nWe can use R as a calculator. You can run any mathematical operation you would normally use by entering it into the console:\n\n\n## Addition\nx <- 3 + 2\nx\n\n\n[1] 5\n\n## Subtraction\nx <-  3 - 2\nx\n\n\n[1] 1\n\n## Multiplication\nx <-  3 * 2\nx\n\n\n[1] 6\n\n## Division\nx <-  3 / 2\nx\n\n\n[1] 1.5\n\n## Modulus (Remainder from division)\nx <-  3 %% 2\nx\n\n\n[1] 1\n\n## Exponentiation\nx = 3 ^ 2\nx\n\n\n[1] 9\n\n## Integer Division (Number of times denominator fits in numerator)\nx = 3 %/% 2\nx\n\n\n[1] 1\n\nInspecting data\nIn a moment, we’ll teach you how to import data into R. For now, let’s work with a dataset that is already present in your R environment, the iris dataset.\nHere are some useful commands for inspecting data\ncheck head of dataset sing head\n\n\n# the top rows and columns of the dataset\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ncheck data types using str\n\n\nstr(iris)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nnames\n\n\n#names of the columns\nnames(iris)\n\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n[5] \"Species\"     \n\nview rownames\n\n\n# view rownames\nrownames(iris)\n\n\n  [1] \"1\"   \"2\"   \"3\"   \"4\"   \"5\"   \"6\"   \"7\"   \"8\"   \"9\"   \"10\" \n [11] \"11\"  \"12\"  \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\"  \"19\"  \"20\" \n [21] \"21\"  \"22\"  \"23\"  \"24\"  \"25\"  \"26\"  \"27\"  \"28\"  \"29\"  \"30\" \n [31] \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\"  \"37\"  \"38\"  \"39\"  \"40\" \n [41] \"41\"  \"42\"  \"43\"  \"44\"  \"45\"  \"46\"  \"47\"  \"48\"  \"49\"  \"50\" \n [51] \"51\"  \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"60\" \n [61] \"61\"  \"62\"  \"63\"  \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"70\" \n [71] \"71\"  \"72\"  \"73\"  \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"80\" \n [81] \"81\"  \"82\"  \"83\"  \"84\"  \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"90\" \n [91] \"91\"  \"92\"  \"93\"  \"94\"  \"95\"  \"96\"  \"97\"  \"98\"  \"99\"  \"100\"\n[101] \"101\" \"102\" \"103\" \"104\" \"105\" \"106\" \"107\" \"108\" \"109\" \"110\"\n[111] \"111\" \"112\" \"113\" \"114\" \"115\" \"116\" \"117\" \"118\" \"119\" \"120\"\n[121] \"121\" \"122\" \"123\" \"124\" \"125\" \"126\" \"127\" \"128\" \"129\" \"130\"\n[131] \"131\" \"132\" \"133\" \"134\" \"135\" \"136\" \"137\" \"138\" \"139\" \"140\"\n[141] \"141\" \"142\" \"143\" \"144\" \"145\" \"146\" \"147\" \"148\" \"149\" \"150\"\n\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntable (and use of $)\n\n\n# create a table\ntable(iris$Species)\n\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\nchange column names\n\n\n# change column names\n# create new dataset for \nirisdat2 <- iris\n# chage names of columns\nnames(irisdat2)[] <- c(\"s_l\", \"s_w\", \"p_l\", \"p_w\", \"sp\")\n#inspect\nhead(irisdat2)\n\n\n  s_l s_w p_l p_w     sp\n1 5.1 3.5 1.4 0.2 setosa\n2 4.9 3.0 1.4 0.2 setosa\n3 4.7 3.2 1.3 0.2 setosa\n4 4.6 3.1 1.5 0.2 setosa\n5 5.0 3.6 1.4 0.2 setosa\n6 5.4 3.9 1.7 0.4 setosa\n\nBasic data structures\nvectors of numbers ( and use of c and i:x )\n\n\ngo_vector <- c(1:5)\ngo_vector\n\n\n[1] 1 2 3 4 5\n\nvectors of characters\n\n\ngo_vector2 <- c(\"hello\", \"world\")\ngo_vector2\n\n\n[1] \"hello\" \"world\"\n\n\n\nas.vector(irisdat2$s_l)\n\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7\n [17] 5.4 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4\n [33] 5.2 5.5 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6\n [49] 5.3 5.0 7.0 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1\n [65] 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7\n [81] 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7\n [97] 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4\n[113] 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1\n[129] 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8\n[145] 6.7 6.7 6.3 6.5 6.2 5.9\n\ndataframes (2 dimensional square arrays of vectors)\n2 x dimensional “square” array with equal column and row lengths. Can contain data with multiple formats characters, facotors, integers, etc.\n\n\nyuk <- data.frame(c(\"the\", \"enumeration\", \"of\", \"the\", \"constitution\"), 6:10)\nyuk\n\n\n  c..the....enumeration....of....the....constitution.. X6.10\n1                                                  the     6\n2                                          enumeration     7\n3                                                   of     8\n4                                                  the     9\n5                                         constitution    10\n\nrename columns of a data frame\n\n\nnames(yuk)[] <- c(\"short\", \"best\")\nyuk\n\n\n         short best\n1          the    6\n2  enumeration    7\n3           of    8\n4          the    9\n5 constitution   10\n\nmatrix\nSame as a dataframe but can only contain one format (e.g. numbers or characters)\n\n\nyok <- as.matrix(yuk)\nyok\n\n\n     short          best\n[1,] \"the\"          \" 6\"\n[2,] \"enumeration\"  \" 7\"\n[3,] \"of\"           \" 8\"\n[4,] \"the\"          \" 9\"\n[5,] \"constitution\" \"10\"\n\nlists\nArrays with constraints on “squareness” or data types.\n\n\nlok <- list(yok, yuk)\nlok\n\n\n[[1]]\n     short          best\n[1,] \"the\"          \" 6\"\n[2,] \"enumeration\"  \" 7\"\n[3,] \"of\"           \" 8\"\n[4,] \"the\"          \" 9\"\n[5,] \"constitution\" \"10\"\n\n[[2]]\n         short best\n1          the    6\n2  enumeration    7\n3           of    8\n4          the    9\n5 constitution   10\n\nClasses in R\nnumeric and integer\nnumeric means number\n\n\nis.numeric(4.2)\n\n\n[1] TRUE\n\ninteger means a number that is not a fraction\n\n\nis.integer(4.2)\n\n\n[1] FALSE\n\nNote the default here:\n\n\nis.integer(4)\n\n\n[1] FALSE\n\n\n\nis.integer(as.integer(4))\n\n\n[1] TRUE\n\nWe’ll need to ensure that certain numbers are integers later on, when we are estimating poisson models and/or doing bayesian data analysis.\ncharacters\nCharacters are strings:\n\n\n# this is a character\nis.character(\"chapeau\")\n\n\n[1] TRUE\n\n# this is not\nis.character(4)\n\n\n[1] FALSE\n\nfactors\nA factor is a category. It can be ordered (e.g. an ordinal scale) or unordered (say a participant in a study, or a wave in a longitidunal study)\n\n\nstr(iris)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nIt’s really important to check that ordered categories are really ordered categories in your dataset.\nThere was is a famous retraction recently where a group found that religion was associated with immorality, however the countries in the the study had been coded as numbers, not as factors. The study’s effect entirely disappeared once this error was corrected!\nIndexing in R\ncolumns\n\n\n# select second column of \"yuk\"\nyuk[, 2]\n\n\n[1]  6  7  8  9 10\n\nrows\n\n\n# select second row of yuk\nyuk[2, ]\n\n\n        short best\n2 enumeration    7\n\nrows and columns\n\n\n#select first row and first column of yuk\nyuk[1, 2]\n\n\n[1] 6\n\nselection by negation\n\n\n# negate the first column of yuk\nyuk[, -1]\n\n\n[1]  6  7  8  9 10\n\n\n\n# negate the second column of yuk\nyuk[,-2]\n\n\n[1] \"the\"          \"enumeration\"  \"of\"           \"the\"         \n[5] \"constitution\"\n\nBasic data wrangling in R\nc\n\n\n# select only the first and second cols of iris\niris_short2 <- iris[ ,c( 1, 2 ) ]\nhead(iris_short2)\n\n\n  Sepal.Length Sepal.Width\n1          5.1         3.5\n2          4.9         3.0\n3          4.7         3.2\n4          4.6         3.1\n5          5.0         3.6\n6          5.4         3.9\n\n-c\n\n\n# select all columns but the first and second of iris\niris_short <- iris[ ,-c( 1, 2 ) ]\nhead(iris_short)\n\n\n  Petal.Length Petal.Width Species\n1          1.4         0.2  setosa\n2          1.4         0.2  setosa\n3          1.3         0.2  setosa\n4          1.5         0.2  setosa\n5          1.4         0.2  setosa\n6          1.7         0.4  setosa\n\ncbind\n\n\n# for use with dataframes and matrices -- note that srings a are c\nyokyuk<-cbind(yok,yuk)\nyokyuk\n\n\n         short best        short best\n1          the    6          the    6\n2  enumeration    7  enumeration    7\n3           of    8           of    8\n4          the    9          the    9\n5 constitution   10 constitution   10\n\nstr(yokyuk)\n\n\n'data.frame':   5 obs. of  4 variables:\n $ short: chr  \"the\" \"enumeration\" \"of\" \"the\" ...\n $ best : chr  \" 6\" \" 7\" \" 8\" \" 9\" ...\n $ short: chr  \"the\" \"enumeration\" \"of\" \"the\" ...\n $ best : int  6 7 8 9 10\n\nrbind\n\n\nrbind(yuk[,],yok[2:3])\n\n\n         short best\n1          the    6\n2  enumeration    7\n3           of    8\n4          the    9\n5 constitution   10\n6  enumeration   of\n\nuse of $ and []\n\n\n#select the fifth row of the column\niris_short$Petal.Width[5]\n\n\n[1] 0.2\n\nuse of $ and [i:x]\n\n\n#select the 5th-25th row of the column\niris_short$Petal.Width[5:25]\n\n\n [1] 0.2 0.4 0.3 0.2 0.2 0.1 0.2 0.2 0.1 0.1 0.2 0.4 0.4 0.3 0.3 0.3\n[17] 0.2 0.4 0.2 0.5 0.2\n\nIndexing for logical operations\ndefinitions\n== means “equals to”\n!= means “not equals to”\n> means “greater than”\n< means “less than”\n>=means “greater than or equal”\n<= means “less than or equal”\n! means “not”\n& means “and”\n| means “or”!\nis.na means “is missing” (missing values are coded in R as NA)\n> -9999 == 666 > TRUE !!! :)\nevaluation using logical operators\ncreate dataframe\n\n\n# create data frame\ndf<-data.frame( x = c(1:10),y = c(11:20) )\n\n\n\nevaluate cases\n\n\n#evaluate cases in y that greater  than 15\ndf[,\"y\"] > 15\n\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nsum instances\n\n\n# count these cases\nsum(df[,\"y\"] > 15)\n\n\n[1] 5\n\nsum instances with a different operator\n\n\n# count cases greater than or equal to 15\nsum(df[,\"y\"] >= 15)\n\n\n[1] 6\n\nanother methods\n\n\n# another approach\nsum(df$y >= 15)\n\n\n[1] 6\n\nusing the or function\n\n\n# combine operators\nsum(df$y >= 15 | df$y <=11)\n\n\n[1] 7\n\ngo meta\n\n\n# go \"meta\"\nsum(df$y >= 15 | df$y <= 11) == sum(df$x >=5 | df$x <=1 )\n\n\n[1] TRUE\n\ngo meta-meta!\n\n\n# go meta-meta\nsum(sum(df$y >= 15 | df$y <= 11) == sum(df$x >=5 | df$x <=1 ))\n\n\n[1] 1\n\nuse operators to modify data\n\n\n# using assignment to modify data\ndf$x[df$x >=5 ] <- NA\ndf\n\n\n    x  y\n1   1 11\n2   2 12\n3   3 13\n4   4 14\n5  NA 15\n6  NA 16\n7  NA 17\n8  NA 18\n9  NA 19\n10 NA 20\n\nusing is.na and !is.na\n\n\nsum(is.na(df$x))\n\n\n[1] 6\n\n\n\nsum(!is.na(df$x))\n\n\n[1] 4\n\n\n\nsum(is.na(df$x)) + sum(!is.na(df$x)) \n\n\n[1] 10\n\nThe basic structure of R commands\nThe four main elements of every R code are:\nobjects,\nfunctions,\narguments\noperators.\nFigure 1 provides a simple example, that produces a new object which contains the mean of variable x.\n\n\n\nFigure 1: The Basic Syntax of R\n\n\n\nmean\nThe function mean generates the arithmetic mean of an input object:\n\n\n# a function to assess the mean of a Sepal.Length\nmean(iris$Sepal.Length)\n\n\n[1] 5.843333\n\nsd\nThe function sd gives us the standard deviation:\n\n\n# standard deviation of Sepal.Length\nsd(iris$Sepal.Length)\n\n\n[1] 0.8280661\n\nsummary\n\n\n# summary of the \"Sepal Length\" column\nsummary(iris$Sepal.Length)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.300   5.100   5.800   5.843   6.400   7.900 \n\n\n\n# summary of the Iris data set\nsummary(iris)\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\nCoding\nThe object needs to be specified inside of the function brackets as the x argument, in this case we define x = df .\nLast, we assign the result of this function for later us via the <- operator to an object which we decided to call object.\nIn other words, we create a new object that can be further manipulated and contains information about the mean of a previously created object “x”. This structure represents the foundation of most operations in R.\nInstead calling the mean function as above one could manually add all values of x dividing it by the number of x values.\nNevertheless, this would be very cumbersome.\nFor this reason, functions (pre-assembled lines of code) exist to reduce the amount of coding necessary.\nThese functions can be bundled into packages. R’s capacity for creating packages is main appeal of R as a statistical tool because community developed functions are available from a central repository called CRAN in the form of packages.\nThese packages can be installed in R with the command install.packages(“package name”).\nIt is important that you only need to install a package once on your machine, expect if you want to upgrade the package. Generally speaking you regularly want to upgrade your packages, but keep a permanent note in your code which version of a package you used when it was initially written.\nInstalling package\ninstall these packages (we’ll be using them later)\n\n\ninstall.packages(\"devtools\") # installing packages\n\n\n\n\n\ninstall.packages(\"remotes\") # installing packages\n\n\n\n\n\ninstall.packages(\"tidyverse\") ## data wrangling and visualisation\n\n\n\n\n\ninstall.packages(\"lme4\") # multilevel modelling\n\n\n\n\n\ninstall.packages(\"patchwork\") # arranging multiple graphs\n\n\n\n\n\nlibrary(devtools)\ndevtools::install_github(\"strengejacke/sjPlot\") # plots and tables\n\n\n\n\n\ninstall.packages(\"papaja\")  # writing APA documents\n\n\n\n\n\ninstall.packages(\"table1\") # summary tables\n\n\n\nextra credit\n\n\ndevtools::install_github(\"easystats/easystats\")\n\n\n\n\n\ndevtools::install_github(\"strengejacke/ggeffects\")\n\n\n\nsuper extra credit\n\n\nif (!requireNamespace(\"remotes\")) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"paul-buerkner/brms\")\n\n\n\n\n\ndevtools::install_github(\"stan-dev/cmdstanr\")\n\n\n\nRolling your own code\nLet’s use R to write a function. Recall that a factorial for a number \\(n\\) is the product of all positive inters less than or equal to \\(n\\). Thus the factorial for 5 = \\[1 \\times 2 \\times 3 \\times 4 \\times 5\\]\nIn R we can write a function:\n\n\n# create a function to perform the factorial operation \ngo_factorial <- function(x) {\n  y <- 1\n  for (i in 1:x) {\n    y <- y * ((1:x)[i])\n  }\n  print(y)\n}\n\n\n\nLet’s try it out\n\n\n# test of the `go_factorial` function\ngo_factorial(5)\n\n\n[1] 120\n\nLet’s see if this is the number that R’s factorial function produces:\n\n\n# R's native factorial function\nfactorial(5)\n\n\n[1] 120\n\nWe can use R’s == relational operator to evaluate whether the two functions are the same\n\n\n# are the two functions equivalent for factorial five\ngo_factorial(5) == factorial(5)\n\n\n[1] 120\n[1] TRUE\n\nFor more information about relational operators type the following into your console:\n\n\n?`==`\n\n\n\nWe can make more complicated functions:\n\n\n# function for factorial that throws warnings when the data that are entered are not appropriate. \ngo_bayes_factorial <- function (x) {\n  # check is the number is negative, positive or zero\n  if (x  < 0) {\n    print(\"not even Ashley Bloomfield could make a factorial for a negative number\")\n  } else if (x == 0) {\n    print(\"the factorial of zero is defined as 1\")\n  } else {\n    for (i in 1:x)\n      y <- 1\n    for (i in 1:x) {\n      y <- y * ((1:x)[i])\n    }\n    print(y)\n  }\n}\n\n\n\nWe’ll come back to functions later. It’s useful to look at an example of a function so that you can see that R is much more than a calcultor. It is a tool to empower you for doing data anlysis in new and creative ways.\nCoding etiquette\nKeep your code legible and annotate\nWhy is this bad code?\n\n\ndf1<-data.frame(a=rnorm(10,1,1),b=rnorm(10,4,8),c=rnorm(10,8,1),d=rnorm(10,7,2))\n\n\n\nWhy is this better code?\n\n\n# Create a data frame with four columns of randomly generated numbers specifying different means and standard deviations \ndf1 <- data.frame(\n  a = rnorm( 10, mean = 1, sd = 1 ),\n  b = rnorm( 10, mean = 4, sd = 8 ),\n  c = rnorm( 10, mean = 8, sd = 1 ),\n  d = rnorm( 10, mean = 7, sd = 2 )\n)\n\n\n\nusing R!\ndata summary\n\n\n# basic summary\nsummary(iris)\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\n\ntable1::table1(~ Sepal.Length   + Sepal.Width   + Petal.Length  + Petal.Width |Species, data = iris  )\n\n\n\nsetosa(N=50)\nversicolor(N=50)\nvirginica(N=50)\nOverall(N=150)\nSepal.Length\n\n\n\n\nMean (SD)\n5.01 (0.352)\n5.94 (0.516)\n6.59 (0.636)\n5.84 (0.828)\nMedian [Min, Max]\n5.00 [4.30, 5.80]\n5.90 [4.90, 7.00]\n6.50 [4.90, 7.90]\n5.80 [4.30, 7.90]\nSepal.Width\n\n\n\n\nMean (SD)\n3.43 (0.379)\n2.77 (0.314)\n2.97 (0.322)\n3.06 (0.436)\nMedian [Min, Max]\n3.40 [2.30, 4.40]\n2.80 [2.00, 3.40]\n3.00 [2.20, 3.80]\n3.00 [2.00, 4.40]\nPetal.Length\n\n\n\n\nMean (SD)\n1.46 (0.174)\n4.26 (0.470)\n5.55 (0.552)\n3.76 (1.77)\nMedian [Min, Max]\n1.50 [1.00, 1.90]\n4.35 [3.00, 5.10]\n5.55 [4.50, 6.90]\n4.35 [1.00, 6.90]\nPetal.Width\n\n\n\n\nMean (SD)\n0.246 (0.105)\n1.33 (0.198)\n2.03 (0.275)\n1.20 (0.762)\nMedian [Min, Max]\n0.200 [0.100, 0.600]\n1.30 [1.00, 1.80]\n2.00 [1.40, 2.50]\n1.30 [0.100, 2.50]\n\n\n\n\n# plot relationship (what is happening here? )\nplot( Sepal.Length   ~ Sepal.Width , data = iris )\n\n\n\n\nmodel\n\n\nlibrary(\"tidyverse\")  # plotting\nlibrary(\"ggeffects\")  # plotting\nlibrary(\"ggplot2\")  # plotting\nlibrary(\"patchwork\") # arrange multiple plots\nlibrary(\"sjPlot\")  # tables and plots\n\n# basic model\nm1<- lm(Sepal.Length ~ Sepal.Width, data = iris)\nsummary(m1)\n\n\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5561 -0.6333 -0.1120  0.5579  2.2226 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   6.5262     0.4789   13.63   <2e-16 ***\nSepal.Width  -0.2234     0.1551   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8251 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\nresults\n\n\n# better summary method\nsjPlot::tab_model(m1)\n\n\n\n \n\n\nSepal.Length\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n6.53\n\n\n5.58 – 7.47\n\n\n<0.001\n\n\nSepal.Width\n\n\n-0.22\n\n\n-0.53 – 0.08\n\n\n0.152\n\n\nObservations\n\n\n150\n\n\nR2 / R2 adjusted\n\n\n0.014 / 0.007\n\n\n\n\n# plot the coefficients\nsjPlot::plot_model(m1)\n\n\n\n\ngraph predicted effects\n\n\n# plot the predicted relationship of Sepal Width on Sepal Length\np1 <- ggeffects::ggpredict(m1, terms = \"Sepal.Width\")\nplot(p1)\n\n\n\n\nWhat is the advantage of this graph?\n\n\npp1 <- plot(p1,\n            add.data = TRUE,\n            dot.alpha = .8,\n            jitter = .2)\npp1\n\n\n\n\ntry another model\n\n\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nsummary (m2 <- lm(Petal.Length ~ Petal.Width, data = iris)) \n\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.33542 -0.30347 -0.02955  0.25776  1.39453 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.08356    0.07297   14.85   <2e-16 ***\nPetal.Width  2.22994    0.05140   43.39   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4782 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\n\npp2<-plot(\n  p2 <- ggeffects::ggpredict(m2, terms = \"Petal.Width\"),\n  add.data = TRUE,\n  dot.alpha = .8,\n  jitter = .2\n) \npp2\n\n\n\n\n\n\n## arange plots\nlibrary(patchwork)\npp1 / pp2 + plot_annotation(title = \"two plots\", tag_levels = \"i\")\n\n\n\n\nAppendix: # symbol is for commenting code\nIn case you haven’t figured it out yet, the hash symbol # is for commenting:\n\n\nr_comments <- 200 # here I am creating the variable for the number of time Jack says R is great\n\njill_roll <- 199 # here I'm creating a variable for the number of times Jill rolls here eyes\n\noutcome <- log(r_comments) * sqrt(jill_roll) * pi # here I am illustrating some functions in r using the variables I just created\n\noutcome # print outcome\n\n\n[1] 234.8088\n\nRounding\nIt is often useful to round numbers:\n\n\nround(outcome, digits = 2) # illustrate the useful `round` function.\n\n\n[1] 234.81\n\n\n\n\n## to be continued\n```{.r .distill-force-highlighting-css}\n\n\n",
    "preview": "posts/2_1/syntax.png",
    "last_modified": "2021-03-18T17:32:58+13:00",
    "input_file": "week_2_visualising_coding.utf8.md",
    "preview_width": 1285,
    "preview_height": 478
  },
  {
    "path": "posts/1_1/",
    "title": "Course basics",
    "description": "Goals: (1) Download Rstudio (2) Get Git (3) Teach you the essentials of Rmarkdown (4) Integrate (1)-(3).",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-02-23",
    "categories": [],
    "contents": "\n\nContents\nTo do:\nDownload R\nDownload R-Studio desktop\nRead about Rmarkdown\nRecommended reading\nWho is this course for?\nHow will you benefit from this course?\n\nOur approach to teaching and learning\nWhat is R?\nIn a nutshell\nHistory\nPurpose\n\nWhat is R Studio?\nThe IDE\nA quick walk through R Studio\n\nGitHub\nWhat is Git/GitHub?\nInstall Git\nCreate a repository\nNext copy the location\nThen open a new project in Rstudio as a GitHub project\n\nThen open an Rmarkdown document, write and save it\nThen commit your Rmarkdown document by including a message, and “pushing” to GitHub\nNext commit!\nExtra information\n\nRMarkdown\nWhat is Rmarkdown?\nWhy is Rmarkdown useful?\nExtra information\nTips and tricks (JB)\nFinal thought: why quantitative psychology has to change (Why can’t I use SPSS, JK)\n\nFinal thought\nLecture slides\n\nTo do:\nDownload R\nR is freely available for download at: https://www.r-project.org/ Please make sure that you have a current version of R-studio desktop on your machine\n\n\n\nDownload R-Studio desktop\nRstudio desktop is also freely available for download at:\nhttps://rstudio.com/products/rstudio/download/#download\nPlease make sure that you have the current version of R-studio desktop on your machine\n\n\n\nRead about Rmarkdown\nRead Danielle Navarro’s brief account of Rmarkdown here\nRecommended reading\nThis is a thorough rmarkdown workshop. You might quickly feel lost. Don’t worry about. We only expect you to acquire the basics this week.\nRmarkdown workshop\n\n\n\n\n\n\n\n\n\nWho is this course for?\nFor those of you who always wanted to learn R but never thought they could, this course is for you.\nHow will you benefit from this course?\nYou’ll learn how to use R, and Github, and we’ll teach you the fundamentals of statistics with a focus thinking not on rules.\nYou’ll learn how to learn – that is, how to obtain the resources you need to address a problem at hand.\nBy the end of the course, you’ll know how to:\nData skills:\nperform your data analysis in R\ndocument your analysis and collaborate in GitHub\ncreate a publication-ready article, with tables and graphs\ncreate a free personal website on github.\nStatistical skills:\nlearn the importance of knowing your question\nlearn how to collect data that bears on your question\nlearn how to explore your data visually\nlearn how to avoid common modelling pitfalls\nlearn how to improve your inference using multi-level models\nOur approach to teaching and learning\n\n\nThis course is designed to provide you with basic understanding, useful tips, and some guide rails for learning. Our main task is to give you the confidence, and the inspiration, for independent learning.\nWhat is R?\nIn a nutshell\nR is a free programming language and software environment for statistical computing (for download links see here: Windows, Mac).\nHistory\nR is the brainchild of Ross Ihaka and [Robert Gentleman](https://en.wikipedia.org/wiki/Robert_Gentleman_(statistician). It was created at the University of Auckland, where Ross Ihaka remains a professor of statistics.\nPurpose\nR was conceived to be a flexible language for data analysis usable by researchers. Since the initial beta release of R in 2000 the language has gained substantial popularity inside and outside of academia (have a look at this blog post for an interesting analysis). New versions of R are released periodically and can be downloaded and installed to replace the older R version.\nWhat is R Studio?\nThe IDE\nThe are many ways for using R on your computer. For the purposes of getting started, we will be using the Integrated Development Environment (IDE): R Studio.\nR Studio provides an interface with a number of user-friendly options, including a separate console and editor that has various help and syntax-auto-complete functions, and various tools for plotting, history, data visualization, debugging and work space management. It is important to remember that R and R Studio are not the same thing.\n\nA quick walk through R Studio\n\n\nGitHub\nWhat is Git/GitHub?\nGithub is a version control system. It is similar to Google docs, though for code. It is useful for collaboration because code easily breaks. It is only rarely possible to simultaneously work in real time on the same code because it will eventually break. Where and how is not easy to assess.\nA second function of GitHub is that it allows us to reconstruct histories of analysis. This is critical for open and reproducible science. This is the main function that we will be examining here.\nA third function, which pertains to single users, is that when writing code you can rewind and recover from your mistakes. This will save you a whole lot of time in the end.\nNote that GitHub has an interface with Rstudio. You will be using GitHub with Rstudio throughout this course.\nInstall Git\nWe suggested installing the educational version because this will allow you to have private repositories.\nIf you haven’t done that, but want to get started you can open a free account and retrospectively add your educational account later.\nPRO TIP Pick a user name that will be OK for professional purposes. If in doubt use your name.\nDirections for installing Git can be found http://github.com\nCreate a repository\nFirst create a repository on GitHub\n\n\n\nNext copy the location\n\n\n\nThen open a new project in Rstudio as a GitHub project\n\n\n\nThen open an Rmarkdown document, write and save it\nFirst, make sure that Rmarkdown is installed:\n\n\n# run this code\nif (!requireNamespace(\"devtools\"))\n  install.packages('devtools')\ndevtools::install_github('rstudio/rmarkdown')\n\n\n\nNext, create a document\n\n\n\nMake sure you save your document\nPress ⌘ + S  is the command for “save”\nThen commit your Rmarkdown document by including a message, and “pushing” to GitHub\nNote that we don’t want want to push .Rproj files to GitHub (this will mess up your collaborations), so I edited my .gitignore file.\nTypically you won’t want to be pushing large html files back and forth to GitHub (that can cause GitHub to freeze).\nYou can edit your gitingore file by adding a * like so:\n/*.html\nsee: https://git-scm.com/docs/gitig\nNext commit!\n\n\n\nVoila!\nExtra information\nJB’s recommendations for using Git and Rstudio\nThis is a very good tutorial on github and Rstudio: link\nVideo link\nA very brief setup video for Mac Users Link\nJK’s recommendations for using Git and Rstuio\nsetup\nRMarkdown\nWhat is Rmarkdown?\nIn the example above we breezed through Rmarkdown without exampling it. What is Rmarkdown?\nRmarkdown is a format for combining data-analysis with ordinary writing using a simple markup language.\nThe Rmarkdown code we used to write the opening paragraph looks like this.\n\ndiv.blue{ background-color:#e6f0ff}\n\n## To do\nRead Danielle Navarro's brief account of \nRmarkdown [here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThe:\n\n##\n\nmakes a heading. Then we write as we ordinarily would write:\n\nRead Danielle Navarro’s brief account of Rmarkdown\n\nand we include a link by typing\n\n[here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThink of rmarkdown as writing in word but without having to use your mouse all the time, and you can write up your analysis while writing in a one0stop shop.\nRmarkdown is just an efficient method for composing text without having to reach for your mouse, and a way of documenting and reporting your analysis\nWhy is Rmarkdown useful?\nRmarkdown merges two very powerful ideas: 1. R as a coding based tool to make your analysis repeatable; 2. markdown an approach to writing text that allows for the direct embedding of code output.\nThis is an immensely powerful approach that can be used for everything, from writing research papers, to writing complex technical documentation. This website is written Rmarkdown.\nYou will be creating a website similar to Johannes Karl’swebsite and Joseph Bulbulia’s websiteand you will do this by written using Rmarkdown).\nYou might think that writing in R markdown is only a nice technical trick for people really into coding, but in reality it addresses a central problem of statistical analysis.\nThe majority of errors in quantitative research papers (some meta-researchers indicate values as high as 80%) are human errors in transcribing values from the statistical software they are using to the final document (for a marginally entertaining story around this issue see this post).\nExtra information\ncoding tips Rmarkdown website\nJB’s recommendation for a very short introduction to Rmarkdown: https://rpubs.com/bpbond/626346\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips and tricks (JB)\nOne day, someone might ask you to collaborate in \\(\\LaTeX\\) (pronounced “Lay-Tek”). \\(\\LaTeX\\) is a document preparation system developed by Leslie Lamport in the 1980s that uses \\(\\TeX\\), a typesetting system that Donald Knuth developed in the 1970s to create mathematical documents. Writing in LaTeX is only a little more complicated than writing in markdown. For example, instead of writing # Heading, ## Subheading, ## Subsubheading, you would write \\section{Heading} \\subsection{Subheading}, \\subsubsection{Subsubheading}. However, the principles of mouseless composition that make Rmarkdown so nice, also make LaTeX nice. Rmarkdown shares features for bibliographic referencing with LaTeX that we’ll cover in later weeks. For now, since we are teaching you about Rmarkdown, we thought it’d be useful to teach about LaTeX too. Stay tuned for more.\nFinal thought: why quantitative psychology has to change (Why can’t I use SPSS, JK)\nQuantitative psychology has long struggled with replicability of it’s results both in substantive and also statistical areas. Concerns around these topics have already been raised on works by authors such as Joseph Banks Rhine the founder of modern parapsychology in the 1930s. Numerous authors, even at the time criticized both methods of the experiment and of the analysis [@gulliksenExtraSensoryPerceptionWhat1938]. In modern times, Deryl Bem’s article “Feeling the Future” that reported evidence in favor of Extra Sensory Perception revived this debate and led to an increased uptake of Open Science methods. Importantly, this is not only an issue in psychology, but instead affects all quantitative fields such as biology, chemistry, and physics. Out of the many issues that are addressed as part of the open science movement (if you are interested in getting active in it have a look at ANZORN) we will focus mostly on aspects of reproducability in analysis.\nUntil recently IBMs SPSS (Statistical Package for the Social Sciences), which originally launched in 1968 dominated the research space in psychology. If you never had the fortune of working with SPSS this is what it looked like:\n\n\n\nSPSS presented the user with a GUI (Graphical User Interface) through which they could run tests on their data. The big issue was that each statistical test has many different options researchers can choose (you will often hear people talk about researcher degrees of freedom) and a GUI makes it very difficult to accurately record every small setting a researcher has chosen. As a work around researchers could either store their output of the analysis which recorded some settings, but even for moderately complex analysis this output could stretch in the hundredth of pages. Alternatively, researchers could save the underlying code that SPSS used, but this was also very clunky and extremely arcane to understand. To give you a sense of scope below you see a snippet from a widely used analysis in SPSS aimed at examining the similarity of factor structures across groups. This code has a total of 130 lines that researchers would have needed to largely enter by hand and double check for any potential coding errors.\n\n\n\nAdditionally, some changes made by researchers were extremely difficult to account for. For example, when a researcher re-coded a variable say reversing its direction there was no way of knowing that this had taken place if you later looked at the data set. Together with the rise in complex analysis in psychology this has led to a steady decline in the use of SPSS and most psychology departments, as well as private, and governmental stake holder now require a certain fluency in R or similar coding based languages.\nFinal thought\nR, Rstudio, GitHub, R-markdown \\(\\dots\\) these are just tools that fit our task\nOur main task in this course is to develop statistical intuition and workflows that will enable you to do better science.\nLecture slides\nClick here to go to the lecture slides.\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\n\n\n\n",
    "preview": "posts/1_1/Rlogo.png",
    "last_modified": "2021-03-18T17:32:19+13:00",
    "input_file": "intro.utf8.md",
    "preview_width": 724,
    "preview_height": 561
  },
  {
    "path": "posts/1_2/",
    "title": "R basics",
    "description": "Some fundamentals",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-02-23",
    "categories": [],
    "contents": "\n\nContents\nFundamentals of RUsing R as calculator\nThe basic structure of R commands\nRecommended reading\n\n\nFundamentals of R\nA consoleruns all your code in R\nA source page runs all your code in R.\nA working directory is where R will look for raw data and other material.\nIn R-studio, (for starters) you can can use the file tab to import and save your material.\nSimilarly, in R-studio, (for starters) you can can use install packages by clicking the Install tab in the package pane (generally lower right pane) to install packagees.\nUsing R as calculator\nWe can use R as a calculator. You can run any mathematical operation you would normally use by entering it into the console:\n\n\n## Addition\nx = 3 + 2\nx\n\n\n[1] 5\n\n## Subtraction\nx = 3 - 2\nx\n\n\n[1] 1\n\n## Multiplication\nx = 3 * 2\nx\n\n\n[1] 6\n\n## Division\nx = 3 / 2\nx\n\n\n[1] 1.5\n\n## Modulus (Remainder from division)\nx = 3 %% 2\nx\n\n\n[1] 1\n\n## Exponentiation\nx = 3 ^ 2\nx\n\n\n[1] 9\n\n## Integer Division (Number of times denominator fits in numerator)\nx = 3 %/% 2\nx\n\n\n[1] 1\n\nThe basic structure of R commands\nWhile using R as calculator might be interesting, it does not get us very far in analysing our data.\nTo really unlock the full potential of R we first need to understand the basic structure of most R code and learn some terms.\nThe four main elements of every R code are:\nobjects,\nfunctions,\narguments\noperators.\nFigure 1 provides a simple example, that produces a new object which contains the mean of variable x.\n\n\n\nFigure 1: The Basic Syntax of R\n\n\n\nThe function mean generates the arithmetic mean of an input object.\nThe object needs to be specified inside of the function brackets as the x argument, in this case we define x = df .\nLast, we assign the result of this function for later us via the <- operator to an object which we decided to call object.\nIn other words, we create a new object that can be further manipulated and contains information about the mean of a previously created object “x”. This structure represents the foundation of most operations in R.\nInstead calling the mean function as above one could manually add all values of x dividing it by the number of x values.\nNevertheless, this would be very cumbersome.\nFor this reason, functions (pre-assembled lines of code) exist to reduce the amount of coding necessary.\nThese functions can be bundled into packages. R’s capacity for creating packages is main appeal of R as a statistical tool because community developed functions are available from a central repository called CRAN in the form of packages.\nThese packages can be installed in R with the command install.packages(“package name”).\nIt is important that you only need to install a package once on your machine, expect if you want to upgrade the package. Generally speaking you regularly want to upgrade your packages, but keep a permanent note in your code which version of a package you used when it was initially written.\nRecommended reading\nAn introduction to R and Rstudio\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-19T18:35:55+13:00",
    "input_file": {}
  },
  {
    "path": "posts/1_3/",
    "title": "Set up",
    "description": "The purpose of this week is to get you set up to write in R markdown",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-02-23",
    "categories": [],
    "contents": "\n\nContents\nTO DO\nMarkdown is a format for writing\nRmarkdown and You\nHow to keep track of everything\nSome useful sites\n\nTO DO\nRead Daniell Navarro’s brief account of Rmarkdown here\nMarkdown is a format for writing\nRmarkdown is a format for combing code with ordinary writing.\nThe Rmarkdown code we used to write the opening paragraph looks like this.\n\ndiv.blue{ background-color:#e6f0ff}\n\n## TO DO\nRead Danielle Navarro's brief account of \nRmarkdown [here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThe:\n\n##\n\nmakes a heading. We write as ordinary,\n\nRead Danielle Navarro’s brief account of Rmarkdown\n\nand we include a link by typing\n\n[here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThink of rmarkdown as writing in word but without having to use your mouse all the time. The really great thing about Rmarkdown is that you can write document and do the analysis in a single stop shop. Figure 1 shows Rmarkdown in the rconsole (upper left).\n\n\n\nFigure 1: Screenshot of Rmarkdown document (upper left)\n\n\n\nRmarkdown and You\nRmarkdown merges two very powerful ideas: 1. R as a coding based tool to make your analysis repeatable; 2. markdown an approach to writing text that allows for the direct embedding of code output.\nThis is an immensely powerful approach that can be used for everything, from writing research papers, to writing complex technical documentation. This website is written Rmarkdown.\nYou will be creating a website similar to Johannes Karl’swebsite and Joseph Bulbulia’s websiteand you will do this by written using Rmarkdown).\nYou might think that writing in R markdown is only a nice technical trick for people really into coding, but in reality it addresses a central problem of statistical analysis.\nThe majority of errors in quantitative research papers (some meta-researchers indiate values as high as 80%) are human errors in transcribing values from the statistical software they are using to the final document (for a marginally entertaining story around this issue see this post).\nHow to keep track of everything\nNow that we have our repeatable code, our repeatable document, the last thing we need is a transparent way to document what we are doing and share with others. For that we come to our last tool that in a similar confusing way to R and Rstudio is split in to parts; git and Github.\nSome useful sites\ncoding tips Rmarkdown website\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-19T18:36:56+13:00",
    "input_file": {}
  }
]
