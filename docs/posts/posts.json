[
  {
    "path": "posts/12_1/",
    "title": "Bayesian inference and consolidation of statistics",
    "description": "\"With an excursion into the whens and when nots of mediation\"",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-25",
    "categories": [],
    "contents": "\n\nContents\n\n\n\n\n\nUncertainty\nFuture Horizons\n\n\n\n",
    "preview": "posts/12_1/distill-preview.png",
    "last_modified": "2021-02-23T15:07:02+13:00",
    "input_file": {},
    "preview_width": 1023,
    "preview_height": 876
  },
  {
    "path": "posts/11_1/",
    "title": "Missing data and measurement error",
    "description": "\"With an excursion into the whens and when nots of mediation\"",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-18",
    "categories": [],
    "contents": "\n\nContents\n\n\n\n\n\nRemember: parameters are not observed\nMeasurement error\nMissing eata\n\n\n\n",
    "preview": "posts/11_1/distill-preview.png",
    "last_modified": "2021-02-23T15:06:35+13:00",
    "input_file": {},
    "preview_width": 1106,
    "preview_height": 553
  },
  {
    "path": "posts/10_1/",
    "title": "Longitudinal data: within and between-individual effects",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-11",
    "categories": [],
    "contents": "\n\nContents\nLongitudional data and the multi-level model\nMediation from the vantage point of causal inference (when and when not)\n\nLongitudional data and the multi-level model\n\n\n\nMediation from the vantage point of causal inference (when and when not)\n\n\n\n",
    "preview": "posts/10_1/op2.png",
    "last_modified": "2021-02-23T15:06:10+13:00",
    "input_file": {},
    "preview_width": 1800,
    "preview_height": 600
  },
  {
    "path": "posts/9_1/",
    "title": "Introduction to multilevel models: group-level slopes",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-05-04",
    "categories": [],
    "contents": "\n\nContents\nCovariance\n\nCovariance\n\n\n\n\n\n\n",
    "preview": "posts/9_1/op.png",
    "last_modified": "2021-02-23T15:05:58+13:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1536
  },
  {
    "path": "posts/8_1/",
    "title": "Introduction to multilevel models: group-level intercepts",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-27",
    "categories": [],
    "contents": "\n\nContents\nPartial pooling\nRandom intercepts\n\nPartial pooling\nRandom intercepts\n\n\n\n\n\n\n",
    "preview": "posts/8_1/op2.png",
    "last_modified": "2021-02-23T15:05:36+13:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1536
  },
  {
    "path": "posts/7_1/",
    "title": "Modelling binary, count data, and ordinal data",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\n\nContents\nBinary Data\nCount data\nZero infalted data\nOrdinal data\n\nBinary Data\nCount data\nZero infalted data\nOrdinal data\n\n\n\n\n\n\n",
    "preview": "posts/7_1/op.png",
    "last_modified": "2021-02-23T15:05:19+13:00",
    "input_file": {},
    "preview_width": 1000,
    "preview_height": 1000
  },
  {
    "path": "posts/6_1/",
    "title": "An introduction to directed acyclic graphs ",
    "description": "\"Control is killing your model\"",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-30",
    "categories": [],
    "contents": "\n\nContents\nWhat is causal confounding?\nThe difference between prediction and explanation?\n\nWhat is causal confounding?\n\n\n\nThe difference between prediction and explanation?\n\n\n\n",
    "preview": "posts/6_1/op2.png",
    "last_modified": "2021-02-23T15:05:04+13:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 800
  },
  {
    "path": "posts/4_1/",
    "title": "Consolidation of skills",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-03-16",
    "categories": [],
    "contents": "\n\nContents\nPreamble\nAssigned reading: workflow Advice: read this\nSomething useful\n\nData Carpentry\nDifferent ways to select variables\nRe-leveling a factor\nMutate by cutting\nPreferable: use ifelse to do the same\nUsing logical operators to create factors\nCreate a table.\nCreate and work with dates a date\nCreate a graph showing the number of responses each day for the years of data collection?\nSlice\n\nData Summaries\nboxplots\nR has loads of canned solutions\nWhat is the variation in my indicators?\nWhich values are most common (and most rare?)\nshortcuts\n\n\n\n\n\n\nspec_tbl_df [4,126 × 64] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Id                         : num [1:4126] 1 1 2 2 3 3 4 4 5 5 ...\n $ Wave                       : num [1:4126] 2019 2018 2019 2018 2019 ...\n $ years                      : num [1:4126] 10.43 9.47 10.61 9.9 10.17 ...\n $ Age                        : num [1:4126] 48 47 48.2 47 54.5 ...\n $ Male                       : chr [1:4126] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ Gender                     : num [1:4126] 1 1 1 1 1 1 0 0 0 0 ...\n $ Edu                        : num [1:4126] 3 3 7 7 4 4 8 7 7 7 ...\n $ Partner                    : num [1:4126] 1 1 1 1 0 0 1 NA 0 0 ...\n $ BornNZ                     : num [1:4126] 1 1 1 1 1 1 1 1 1 1 ...\n $ BigDoms                    : chr [1:4126] \"Not_Rel\" \"Not_Rel\" \"Not_Rel\" \"Not_Rel\" ...\n $ TSCORE                     : num [1:4126] 3869 3520 3936 3677 3774 ...\n $ GenCohort                  : chr [1:4126] \"GenX: born >=1961 & b.< 1980\" \"GenX: born >=1961 & b.< 1980\" \"GenX: born >=1961 & b.< 1980\" \"GenX: born >=1961 & b.< 1980\" ...\n $ Religion.Church            : num [1:4126] 0 0 0 0 0 0 2 NA 0 0 ...\n $ Religion.Believe.Cats      : num [1:4126] 4 4 1 1 1 1 1 NA 3 1 ...\n $ Relid                      : num [1:4126] 0 0 0 0 0 0 7 7 2 2 ...\n $ HLTH.Fatigue               : num [1:4126] 2 2 1 2 2 2 1 2 NA 1 ...\n $ HLTH.SleepHours            : num [1:4126] 6 6 6 4 7 7 6 4 7 8 ...\n $ HomeOwner                  : num [1:4126] NA 1 NA 0 NA 1 NA NA NA 0 ...\n $ Pol.Orient                 : num [1:4126] 3 3 5 3 4 4 3 NA 4 4 ...\n $ PATRIOT                    : num [1:4126] 4.5 5 6.5 7 4 4 5.5 4 6.5 6 ...\n $ Env.SatNZEnvironment       : num [1:4126] 7 4 7 7 7 7 4 3 7 8 ...\n $ Env.MotorwaySpend          : num [1:4126] 5 5 3 5 4 4 4 6 5 6 ...\n $ Env.PubTransSubs           : num [1:4126] 5 6 5 5 4 4 7 6 4 6 ...\n $ Env.ClimateChgConcern      : num [1:4126] 6 6 7 7 4 4 6 NA 4 2 ...\n $ LIFEMEANING                : num [1:4126] 5 6.5 5 4.5 5.5 5.5 7 7 5 6 ...\n $ Hours.Internet             : num [1:4126] 10 5 14 15 2 2 4 4 10 14 ...\n $ Issue.GovtSurveillance     : num [1:4126] 3 1 3 3 4 4 1 2 4 3 ...\n $ Issue.RegulateAI           : num [1:4126] NA 1 NA 4 NA 4 NA 3 NA 4 ...\n $ Issue.IncomeRedistribution : num [1:4126] 3 2 4 4 2 4 7 6 4 3 ...\n $ Hours.Exercise             : num [1:4126] 14 24 7 10 5 6 1 0 1 7 ...\n $ Hours.Work                 : num [1:4126] 14 0 35 65 60 50 41 42 0 0 ...\n $ Hours.News                 : num [1:4126] 4 4 5 4 1 2 0 0 8 7 ...\n $ CONSCIENTIOUSNESS          : num [1:4126] 4.75 5.25 5.5 5.5 5 4.25 4.75 4.75 NA 5.25 ...\n $ EXTRAVERSION               : num [1:4126] 3.25 2.75 4.75 4 3.75 4.5 5.75 4.25 NA 4 ...\n $ AGREEABLENESS              : num [1:4126] 4.5 5 5 6 5.75 5.25 5 5.25 NA 5 ...\n $ OPENNESS                   : num [1:4126] 6.5 7 4.25 4.25 6 6 5.75 6.25 NA 5 ...\n $ Religious                  : chr [1:4126] \"Not_Religious\" \"Not_Religious\" \"Not_Religious\" \"Not_Religious\" ...\n $ Spiritual.Identification   : num [1:4126] NA 1 NA 5 NA 4 NA NA NA 2 ...\n $ Believe.God                : chr [1:4126] \"Not Believe God\" \"Not Believe God\" \"Believe God\" \"Believe God\" ...\n $ Believe.Spirit             : chr [1:4126] \"Not Believe Spirit\" \"Not Believe Spirit\" \"Believe Spirit\" \"Believe Spirit\" ...\n $ HoursCharity               : num [1:4126] 2 0 0 2 0 0 0 4 0 0 ...\n $ CharityDonate              : num [1:4126] 180 80 300 100 4200 3500 400 350 50 100 ...\n $ Your.Personal.Relationships: num [1:4126] 7 6 2 2 8 8 10 10 9 9 ...\n $ Your.Future.Security       : num [1:4126] 8 10 8 6 8 7 8 7 9 9 ...\n $ Standard.Living            : num [1:4126] 7 8 8 6 8 8 10 10 9 9 ...\n $ NZ.Economic.Situation      : num [1:4126] 7 4 2 6 5 6 7 5 7 8 ...\n $ NZ.Social.Conditions       : num [1:4126] 7 7 2 6 5 5 2 0 9 7 ...\n $ NZ.Business.Conditions     : num [1:4126] 7 8 2 6 5 5 6 5 9 7 ...\n $ Emp.JobSecure              : num [1:4126] 7 NA 6 6 5 4 6 NA NA NA ...\n $ Issue.Food.GMO             : num [1:4126] 1 2 5 5 4 4 7 7 1 4 ...\n $ Env.SacMade                : logi [1:4126] NA NA NA NA NA NA ...\n $ KESSLER6sum                : num [1:4126] 5 3 7 7 3 3 0 4 NA 2 ...\n $ SWB.Kessler01              : num [1:4126] 0 0 1 0 0 0 0 1 NA 0 ...\n $ SWB.Kessler02              : num [1:4126] 0 0 0 0 0 0 0 0 NA 0 ...\n $ SWB.Kessler03              : num [1:4126] 2 1 3 3 1 1 0 0 NA 1 ...\n $ SWB.Kessler04              : num [1:4126] 1 1 1 2 1 1 0 2 NA 1 ...\n $ SWB.Kessler05              : num [1:4126] 0 0 0 0 0 0 0 0 NA 0 ...\n $ SWB.Kessler06              : num [1:4126] 2 1 2 2 1 1 0 1 NA 0 ...\n $ FeelHopeless               : chr [1:4126] \"None Of The Time\" \"None Of The Time\" \"A Little Of The Time\" \"None Of The Time\" ...\n $ FeelDepressed              : chr [1:4126] \"None Of The Time\" \"None Of The Time\" \"None Of The Time\" \"None Of The Time\" ...\n $ FeelRestless               : chr [1:4126] \"Some Of The Time\" \"A Little Of The Time\" \"Most Of The Time\" \"Most Of The Time\" ...\n $ EverythingIsEffort         : chr [1:4126] \"A Little Of The Time\" \"A Little Of The Time\" \"A Little Of The Time\" \"Some Of The Time\" ...\n $ FeelWorthless              : chr [1:4126] \"None Of The Time\" \"None Of The Time\" \"None Of The Time\" \"None Of The Time\" ...\n $ FeelNervous                : chr [1:4126] \"Some Of The Time\" \"A Little Of The Time\" \"Some Of The Time\" \"Some Of The Time\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Id = col_double(),\n  ..   Wave = col_double(),\n  ..   years = col_double(),\n  ..   Age = col_double(),\n  ..   Male = col_character(),\n  ..   Gender = col_double(),\n  ..   Edu = col_double(),\n  ..   Partner = col_double(),\n  ..   BornNZ = col_double(),\n  ..   BigDoms = col_character(),\n  ..   TSCORE = col_double(),\n  ..   GenCohort = col_character(),\n  ..   Religion.Church = col_double(),\n  ..   Religion.Believe.Cats = col_double(),\n  ..   Relid = col_double(),\n  ..   HLTH.Fatigue = col_double(),\n  ..   HLTH.SleepHours = col_double(),\n  ..   HomeOwner = col_double(),\n  ..   Pol.Orient = col_double(),\n  ..   PATRIOT = col_double(),\n  ..   Env.SatNZEnvironment = col_double(),\n  ..   Env.MotorwaySpend = col_double(),\n  ..   Env.PubTransSubs = col_double(),\n  ..   Env.ClimateChgConcern = col_double(),\n  ..   LIFEMEANING = col_double(),\n  ..   Hours.Internet = col_double(),\n  ..   Issue.GovtSurveillance = col_double(),\n  ..   Issue.RegulateAI = col_double(),\n  ..   Issue.IncomeRedistribution = col_double(),\n  ..   Hours.Exercise = col_double(),\n  ..   Hours.Work = col_double(),\n  ..   Hours.News = col_double(),\n  ..   CONSCIENTIOUSNESS = col_double(),\n  ..   EXTRAVERSION = col_double(),\n  ..   AGREEABLENESS = col_double(),\n  ..   OPENNESS = col_double(),\n  ..   Religious = col_character(),\n  ..   Spiritual.Identification = col_double(),\n  ..   Believe.God = col_character(),\n  ..   Believe.Spirit = col_character(),\n  ..   HoursCharity = col_double(),\n  ..   CharityDonate = col_double(),\n  ..   Your.Personal.Relationships = col_double(),\n  ..   Your.Future.Security = col_double(),\n  ..   Standard.Living = col_double(),\n  ..   NZ.Economic.Situation = col_double(),\n  ..   NZ.Social.Conditions = col_double(),\n  ..   NZ.Business.Conditions = col_double(),\n  ..   Emp.JobSecure = col_double(),\n  ..   Issue.Food.GMO = col_double(),\n  ..   Env.SacMade = col_logical(),\n  ..   KESSLER6sum = col_double(),\n  ..   SWB.Kessler01 = col_double(),\n  ..   SWB.Kessler02 = col_double(),\n  ..   SWB.Kessler03 = col_double(),\n  ..   SWB.Kessler04 = col_double(),\n  ..   SWB.Kessler05 = col_double(),\n  ..   SWB.Kessler06 = col_double(),\n  ..   FeelHopeless = col_character(),\n  ..   FeelDepressed = col_character(),\n  ..   FeelRestless = col_character(),\n  ..   EverythingIsEffort = col_character(),\n  ..   FeelWorthless = col_character(),\n  ..   FeelNervous = col_character()\n  .. )\n\nPreamble\nAssigned reading: workflow Advice: read this\nAdvice on how to name your files (by Danielle Navarro) here\nSomething useful\nThe hash symbol # is for commenting\n\n\nr_comments <- 200 # here I am creating the variable for the number of time Jack says R is great\n\njill_roll <- 199 # here I'm creating a variable for the number of times Jill rolls here eyes\n\noutcome <- log(r_comments) * sqrt(jill_roll) * pi # here I am illustrating some functions in r using the variables I just created\n\noutcome # print outcome\n\n\n[1] 234.8088\n\nround(outcome, digits = 2) # illustrate the useful `round` function.\n\n\n[1] 234.81\n\nData Carpentry\nDifferent ways to select variables\nTask: 3 different ways to select the variables that start with Believe.\nExplicit selection\n\n\nnames(nz)\n\n\n [1] \"Id\"                          \"Wave\"                       \n [3] \"years\"                       \"Age\"                        \n [5] \"Male\"                        \"Gender\"                     \n [7] \"Edu\"                         \"Partner\"                    \n [9] \"BornNZ\"                      \"BigDoms\"                    \n[11] \"TSCORE\"                      \"GenCohort\"                  \n[13] \"Religion.Church\"             \"Religion.Believe.Cats\"      \n[15] \"Relid\"                       \"HLTH.Fatigue\"               \n[17] \"HLTH.SleepHours\"             \"HomeOwner\"                  \n[19] \"Pol.Orient\"                  \"PATRIOT\"                    \n[21] \"Env.SatNZEnvironment\"        \"Env.MotorwaySpend\"          \n[23] \"Env.PubTransSubs\"            \"Env.ClimateChgConcern\"      \n[25] \"LIFEMEANING\"                 \"Hours.Internet\"             \n[27] \"Issue.GovtSurveillance\"      \"Issue.RegulateAI\"           \n[29] \"Issue.IncomeRedistribution\"  \"Hours.Exercise\"             \n[31] \"Hours.Work\"                  \"Hours.News\"                 \n[33] \"CONSCIENTIOUSNESS\"           \"EXTRAVERSION\"               \n[35] \"AGREEABLENESS\"               \"OPENNESS\"                   \n[37] \"Religious\"                   \"Spiritual.Identification\"   \n[39] \"Believe.God\"                 \"Believe.Spirit\"             \n[41] \"HoursCharity\"                \"CharityDonate\"              \n[43] \"Your.Personal.Relationships\" \"Your.Future.Security\"       \n[45] \"Standard.Living\"             \"NZ.Economic.Situation\"      \n[47] \"NZ.Social.Conditions\"        \"NZ.Business.Conditions\"     \n[49] \"Emp.JobSecure\"               \"Issue.Food.GMO\"             \n[51] \"Env.SacMade\"                 \"KESSLER6sum\"                \n[53] \"SWB.Kessler01\"               \"SWB.Kessler02\"              \n[55] \"SWB.Kessler03\"               \"SWB.Kessler04\"              \n[57] \"SWB.Kessler05\"               \"SWB.Kessler06\"              \n[59] \"FeelHopeless\"                \"FeelDepressed\"              \n[61] \"FeelRestless\"                \"EverythingIsEffort\"         \n[63] \"FeelWorthless\"               \"FeelNervous\"                \n\nnz %>%\n  select(\"Believe.God\", \"Believe.Spirit\")%>%\n  as_tibble()\n\n\n# A tibble: 4,126 x 2\n   Believe.God     Believe.Spirit    \n   <fct>           <fct>             \n 1 Not Believe God Not Believe Spirit\n 2 Not Believe God Not Believe Spirit\n 3 Believe God     Believe Spirit    \n 4 Believe God     Believe Spirit    \n 5 Believe God     Believe Spirit    \n 6 Believe God     Believe Spirit    \n 7 Believe God     Believe Spirit    \n 8 <NA>            <NA>              \n 9 Not Believe God Believe Spirit    \n10 Believe God     Believe Spirit    \n# … with 4,116 more rows\n\nstarts_with\n\n\nnz %>%\n  select(starts_with(\"Believe\"))%>%\n  as_tibble()\n\n\n# A tibble: 4,126 x 2\n   Believe.God     Believe.Spirit    \n   <fct>           <fct>             \n 1 Not Believe God Not Believe Spirit\n 2 Not Believe God Not Believe Spirit\n 3 Believe God     Believe Spirit    \n 4 Believe God     Believe Spirit    \n 5 Believe God     Believe Spirit    \n 6 Believe God     Believe Spirit    \n 7 Believe God     Believe Spirit    \n 8 <NA>            <NA>              \n 9 Not Believe God Believe Spirit    \n10 Believe God     Believe Spirit    \n# … with 4,116 more rows\n\ncontains\n\n\nnz %>%\n  select(contains(\"Believe\"))%>%\n  as_tibble()\n\n\n# A tibble: 4,126 x 3\n   Religion.Believe.Cats Believe.God     Believe.Spirit    \n                   <dbl> <fct>           <fct>             \n 1                     4 Not Believe God Not Believe Spirit\n 2                     4 Not Believe God Not Believe Spirit\n 3                     1 Believe God     Believe Spirit    \n 4                     1 Believe God     Believe Spirit    \n 5                     1 Believe God     Believe Spirit    \n 6                     1 Believe God     Believe Spirit    \n 7                     1 Believe God     Believe Spirit    \n 8                    NA <NA>            <NA>              \n 9                     3 Not Believe God Believe Spirit    \n10                     1 Believe God     Believe Spirit    \n# … with 4,116 more rows\n\nwhoops, so we try something else\nThis is a hack.\n\n\nnz %>%\n  select(contains(\"Believe\") &  -  Religion.Believe.Cats)%>%\n  as_tibble()\n\n\n# A tibble: 4,126 x 2\n   Believe.God     Believe.Spirit    \n   <fct>           <fct>             \n 1 Not Believe God Not Believe Spirit\n 2 Not Believe God Not Believe Spirit\n 3 Believe God     Believe Spirit    \n 4 Believe God     Believe Spirit    \n 5 Believe God     Believe Spirit    \n 6 Believe God     Believe Spirit    \n 7 Believe God     Believe Spirit    \n 8 <NA>            <NA>              \n 9 Not Believe God Believe Spirit    \n10 Believe God     Believe Spirit    \n# … with 4,116 more rows\n\nLesson: think about your interests and purposes.\nRe-leveling a factor\n\n\nnz %>%\n  dplyr::select(BigDoms)%>%\n  table() \n\n\n.\n Buddhist Christian    Muslim   Not_Rel TheOthers \n       36      1204         9      2655       128 \n\n## suppose we want \"Not_Rel\" as the base category, and rearrange the other levels\nlibrary(forcats)\nnz %>%\n  dplyr::select(BigDoms) %>%\n  dplyr::mutate(BigDoms =  \n                  forcats::fct_relevel(BigDoms, c(\"Not_Rel\",\"Christian\",\"Buddhist\",\"Muslim\",\"TheOthers\")))%>%\n  table()\n\n\n.\n  Not_Rel Christian  Buddhist    Muslim TheOthers \n     2655      1204        36         9       128 \n\nMutate by cutting\n\n\nnz <-nz %>%\n  dplyr::mutate(k6cats = cut(\n    KESSLER6sum,\n    breaks = c(-Inf, 5, 13, Inf),   # create Kessler 6 diagnostic categories\n    labels = c(\"Low Distress\", \"Moderate Distress\", \"Serious Distress\"), \n    right = TRUE\n  ))\n\n\n\nPreferable: use ifelse to do the same\nPersonally I find the following method better, because it gives me explicit control of how I am making the categories.\n\n\nnz %>%\n  dplyr::mutate(k6cats1 =  as.factor(ifelse(\n    KESSLER6sum <= 5,\n    \"Low Distress\",\n    ifelse(KESSLER6sum <= 13,  \"Moderate Distress\", \"Serious Distress\")\n  ))) %>%\n  group_by(k6cats1) %>%\n  count()\n\n\n# A tibble: 4 x 2\n# Groups:   k6cats1 [4]\n  k6cats1               n\n  <fct>             <int>\n1 Low Distress       2510\n2 Moderate Distress  1400\n3 Serious Distress    177\n4 <NA>                 39\n\n#check this is the same as the previous method\nnz %>%\n  group_by(k6cats)%>%\n  count()\n\n\n# A tibble: 4 x 2\n# Groups:   k6cats [4]\n  k6cats                n\n  <fct>             <int>\n1 Low Distress       2510\n2 Moderate Distress  1400\n3 Serious Distress    177\n4 <NA>                 39\n\nUsing logical operators to create factors\nCreate a table.\n\n\nlibrary(kableExtra)\nnz %>%\n  select(k6cats, Wave) %>%\n  filter(!is.na(k6cats))%>%\n  group_by( Wave, k6cats) %>%\n  summarise(n = n())%>%\n  kbl(caption = \"Distress by Year\") %>%\n   kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)%>%\n  collapse_rows()\n\n\n\nTable 1: Distress by Year\n\n\nWave\n\n\nk6cats\n\n\nn\n\n\n2018\n\n\nLow Distress\n\n\n1272\n\n\nModerate Distress\n\n\n675\n\n\nSerious Distress\n\n\n88\n\n\n2019\n\n\nLow Distress\n\n\n1238\n\n\nModerate Distress\n\n\n725\n\n\nSerious Distress\n\n\n89\n\n\nCreate and work with dates a date\n\n\nnz <- nz %>%\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE)  # first data of data collection in this study\n\n\n\nWe can analyze dates, for example, for how many minutes were data collected?\n\n\nnz %>%\n  select(date)%>%\n  summary()\n\n\n      date           \n Min.   :2018-01-02  \n 1st Qu.:2018-08-09  \n Median :2019-10-03  \n Mean   :2019-05-18  \n 3rd Qu.:2019-12-07  \n Max.   :2020-10-06  \n\nint<-lubridate::interval(ymd(\"2018-01-02\"), ymd(\"2020-10-06\"))\n\n#time in years\ntime_length(int, \"year\")\n\n\n[1] 2.759563\n\n#time in minutes\ntime_length(int, \"minutes\")\n\n\n[1] 1451520\n\nCreate a graph showing the number of responses each day for the years of data collection?\n\n\nlibrary(lubridate)\nlibrary(ggplot2)\n\ndatrep <- nz %>%\n  count(day = floor_date(date, \"day\")) %>%\n  dplyr::mutate(Year = factor(ifelse(\n    day < \"2010-01-01\",\n    2009,\n    ifelse(\n      day < \"2011-01-01\",\n      2010,\n      ifelse(\n        day < \"2012-01-01\",\n        2011,\n        ifelse(\n          day < \"2013-01-01\",\n          2012,\n          ifelse(\n            day < \"2014-01-01\",\n            2013,\n            ifelse(\n              day < \"2015-01-01\",\n              2014,\n              ifelse(\n                day < \"2016-01-01\",\n                2015,\n                ifelse(\n                  day < \"2017-01-01\",\n                  2016,\n                  ifelse(\n                    day < \"2018-01-01\",\n                    2017,\n                    ifelse(day < \"2019-01-01\", 2018,\n                           ifelse(day < \"2020-01-01\", 2019, 2020))\n                  )\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  ))) %>%\n  arrange(day)\n\n# create graph\nggplot(datrep, aes(day, n)) +\n  geom_col(aes(fill = Year)) + scale_x_date(date_labels = \"%b/%Y\")  +\n  xlab(\"Days\") + ylab(\"Count of Responses\") + ggtitle(\"Our Dataset's Daily Counts\")  + theme_classic()  +  scale_fill_viridis_d()\n\n\n\n\nArrange by date with the most responses\n\n\ndatrep%>%\n  arrange(desc(n))\n\n\n# A tibble: 607 x 3\n   day            n Year \n   <date>     <int> <fct>\n 1 2018-06-21   112 2018 \n 2 2018-06-22    93 2018 \n 3 2018-06-24    80 2018 \n 4 2018-06-20    67 2018 \n 5 2018-06-23    59 2018 \n 6 2018-06-26    58 2018 \n 7 2019-12-03    54 2019 \n 8 2018-06-25    52 2018 \n 9 2019-10-04    47 2019 \n10 2019-12-02    46 2019 \n# … with 597 more rows\n\nThere are no inherently stressful days. To see this, we can take average stress levels by day, and then see where the high average stress days fall.\n\n\ntn<-nz %>%\n  select(date,KESSLER6sum,Id) %>%\n  group_by(date)%>%\n  summarise(\n   av_distress =  mean(KESSLER6sum, na.rm = TRUE),\n   n = n_distinct(Id)\n  ) %>%\n  arrange(desc(av_distress))\ntn\n\n\n# A tibble: 607 x 3\n   date       av_distress     n\n   <date>           <dbl> <int>\n 1 2018-12-16        21       1\n 2 2020-07-06        16       1\n 3 2018-11-13        15.5     2\n 4 2019-01-05        15       1\n 5 2020-06-01        15       1\n 6 2020-07-01        15       1\n 7 2020-02-10        14       1\n 8 2020-06-23        14       1\n 9 2019-12-26        13       1\n10 2019-09-06        12.5     2\n# … with 597 more rows\n\nWe can all graph the densities\n\n\ntn%>%\n  ggplot(., aes(date, av_distress)) + \n  geom_col(aes(fill =(n))) + scale_x_date(date_labels = \"%b/%Y\")  + theme_classic() + scale_fill_viridis_c()\n\n\n\n\nConverting dates to days of the week.\n\n\nnz %>%\n  select(Id, date, KESSLER6sum) %>%\n  mutate(weekdays = wday(date, label = TRUE)) %>%\n  group_by(weekdays) %>%\n  summarise(\n    mn_k6 =  mean(KESSLER6sum, na.rm = TRUE),\n    sd_k6 =  sd(KESSLER6sum, na.rm = TRUE),\n    n_k6w = n()\n  ) %>%\n  mutate(\n    se_k6 = sd_k6 / sqrt(n_k6w),\n    lw_ci = mn_k6 - qt(1 - (0.05 / 2), n_k6w - 1) * se_k6,\n    up_ci = mn_k6 + qt(1 - (0.05 / 2), n_k6w - 1) * se_k6\n  ) %>%\n  ggplot(., aes(x = weekdays, y = mn_k6, colour = mn_k6)) +\n  geom_errorbar(aes(ymin = lw_ci, ymax = up_ci), width = .1) +\n  geom_point(size = 3)  +\n  scale_y_continuous(limits = c(0,7)) + \n  theme_classic() + scale_fill_viridis_d()\n\n\n\n\nSlice\nDplyr’s slice function can be handy. Say we only want the first six rows\n\n\ndatrep%>%\n  arrange(desc(n)) %>%\n  slice(1:6)\n\n\n# A tibble: 6 x 3\n  day            n Year \n  <date>     <int> <fct>\n1 2018-06-21   112 2018 \n2 2018-06-22    93 2018 \n3 2018-06-24    80 2018 \n4 2018-06-20    67 2018 \n5 2018-06-23    59 2018 \n6 2018-06-26    58 2018 \n\nSay we only want the 1st row, the 3rd row, and the 20th row\n\n\ndatrep%>%\n  dplyr::arrange(desc(n)) %>%\n  dplyr::slice(c(1,3,20))\n\n\n# A tibble: 3 x 3\n  day            n Year \n  <date>     <int> <fct>\n1 2018-06-21   112 2018 \n2 2018-06-24    80 2018 \n3 2020-03-18    38 2020 \n\nData Summaries\nboxplots\n\n\nboxplot(nz$Age, notch = TRUE)\n\n\n\nboxplot(KESSLER6sum ~ Wave, data = nz, notch = TRUE, col = c(\"cadetblue1\",\"orange\"))\n\n\n\nnz$KESSLER6sum\n\n\n   [1]  5.0  3.0  7.0  7.0  3.0  3.0  0.0  4.0   NA  2.0  2.0  2.0\n  [13]  2.0  2.0  5.0  2.0  2.0  0.0 10.0  9.0  0.0  0.0 17.0 14.0\n  [25]  0.0  2.0  7.0 10.0 10.0  5.0  0.0  0.0  4.0  1.0  1.0  2.0\n  [37] 13.0  2.0  1.0  0.0  0.0  2.0  8.0  4.0 10.0  4.0  0.0  5.0\n  [49]  7.0  5.0  8.0 13.0  4.0  4.0  7.0  6.0  1.0  0.0  4.0  2.0\n  [61]  4.0  5.0  6.0  6.0  5.0  3.0  0.0  0.0  0.0  0.0  1.0  3.0\n  [73]  0.0  1.0  1.0  0.0  2.0  3.0 12.0  5.0  5.0  7.0  3.0  5.0\n  [85] 10.0  7.2  3.0  3.0  2.0  2.0  0.0  0.0  1.0  1.0  6.0  4.0\n  [97] 10.0  4.0  9.0  7.0  6.0  5.0  0.0  0.0  8.0  2.0 12.0 11.0\n [109] 10.0  4.0 12.0  9.0  5.0 12.0  4.0  9.0  6.0  1.0  9.0  5.0\n [121]  1.0  2.0  4.0  4.0  4.0  3.0  7.0  6.0  1.0  3.0 16.0 17.0\n [133]  6.0  8.0  0.0  2.0  3.6  1.0  6.0  8.0  4.0  5.0  1.0  0.0\n [145]  7.0  9.0  1.0  1.0  1.0  0.0  5.0  5.0  3.0  2.0  2.0  1.0\n [157] 12.0  8.0  1.0  4.0 10.0 12.0  1.0  0.0  9.0  8.0  8.0  4.0\n [169]  1.0  5.0 11.0  6.0  2.0  3.0  3.0  3.0  1.0  2.0 12.0 12.0\n [181]  9.6  8.0  4.0  2.0  0.0  1.0  2.0  5.0  0.0  0.0  1.0  0.0\n [193]  4.0  2.0  9.0  3.0  7.0  2.0  7.2  3.0  3.0  3.0  5.0  6.0\n [205]  2.0  3.0  0.0  0.0 12.0  8.0  0.0  2.0  4.0  6.0  8.0  6.0\n [217]  3.0  1.0  4.0  8.0  3.0  2.0  4.0  5.0  8.0  3.0  6.0  9.0\n [229]  3.0  2.0  3.0  4.0  3.0  2.0  4.0  4.0  4.0  2.0  2.0  1.0\n [241]  9.0  8.0  3.0  1.0  5.0  6.0  3.0  1.0  1.0  1.0  3.0  3.0\n [253]  1.0  1.0  9.0  6.0  4.0  2.0  2.0  4.0  2.0  8.0  2.0  1.0\n [265]  2.0  2.0  5.0  3.0  1.0  3.0  0.0  3.0  1.2  6.0  2.0  1.0\n [277]  4.0  7.0  6.0  4.0  1.0  0.0 10.0  6.0  0.0  1.0  1.0  1.0\n [289] 14.0 16.0 11.0  9.0  4.0  9.0  3.0  3.0  3.0  3.0  6.0  4.0\n [301]  6.0  8.0  3.0  1.0  2.0  4.0  9.0  9.0  5.0  5.0  8.0  9.0\n [313]  5.0  6.0  6.0  3.0  8.0 11.0  6.0  4.0  5.0  5.0  9.0  1.0\n [325] 10.0  9.0 11.0 10.0  0.0 20.0 13.0 12.0  2.0  0.0  0.0  1.0\n [337]  4.0  2.0  8.0  6.0  6.0 10.0  7.0 11.0  4.0  4.0  5.0  5.0\n [349]  1.0  0.0  3.0  6.0  8.0 12.0  3.0  6.0  5.0  4.0  8.0  4.0\n [361]  0.0  5.0  9.0  7.0  4.0 14.0  5.0  0.0 10.0  3.0  4.0  5.0\n [373]  3.0  2.0  6.0  2.0  4.0  4.0  7.0  7.0  9.0  8.0  6.0  6.0\n [385]  2.0  4.0  3.0  7.0 13.0 13.0  0.0  0.0  6.0  6.0  3.0  6.0\n [397]  3.0  1.0  4.0  3.0  3.0  3.0  4.0  7.0  4.0  2.0  3.0  4.0\n [409] 11.0 13.0  5.0  6.0 14.0 12.0  5.0  4.0  2.0  2.0  9.0  5.0\n [421]  0.0  3.0  4.0  6.0  0.0  0.0  4.0  5.0  0.0  5.0 13.0 14.0\n [433]  5.0  2.0  2.0  2.0 19.0 21.0  1.0  1.0  7.0  8.0  4.0  4.0\n [445]  0.0  3.0  4.0  2.0  0.0  6.0  1.0  3.0  7.0 13.0  0.0  0.0\n [457] 11.0 12.0  9.0  6.0  6.0  2.0 10.0 11.0  2.0  2.0  5.0  4.0\n [469]  5.0  4.0  4.0 15.0  1.0  2.0  1.0  1.0  6.0  5.0  5.0  0.0\n [481]  6.0  7.0  2.0  3.0  3.0  4.0 11.0 11.0  2.0  0.0  1.0  0.0\n [493]  6.0  3.0  2.0  2.0  0.0  0.0  6.0  4.0  4.0  3.0  0.0  0.0\n [505]  5.0  5.0  3.0  0.0  5.0  1.0 18.0 18.0  2.0  2.0  5.0  8.0\n [517]  3.0  5.0 11.0  9.0  4.0  6.0  9.0  8.0  2.0  3.0  2.0  1.0\n [529]  5.0  3.0  2.0  3.0  3.0  4.0  5.0  4.0  2.0  0.0  5.0  4.0\n [541]  2.0  1.0  1.0  0.0  4.0  2.0 10.0  6.0  6.0  4.0  3.0  3.0\n [553]  2.0  6.0  4.0  6.0  3.0  4.0 15.0 13.0  3.0  1.0  2.0  3.0\n [565]  2.0  7.0  4.0  4.0  0.0  0.0  2.0  8.0  1.0  2.0  0.0  0.0\n [577]  5.0  5.0 11.0  8.0  1.0  7.0  2.0  4.0  2.0  1.2  7.0  5.0\n [589]  4.0  4.0  3.0  4.0 11.0 10.0  1.0  2.0  7.0  5.0  6.0  6.0\n [601]  3.0  5.0  7.2   NA  4.0  4.0  3.0  3.0  0.0  2.0  1.0  3.0\n [613]  4.0  4.0  4.0  3.0  5.0  4.0  1.0  1.0  9.0  9.0  7.0  7.0\n [625]  5.0  4.0  4.0  3.0  6.0  2.0  6.0  3.0  8.0  8.0  3.0  4.0\n [637]  7.0  4.0 15.0 14.0 16.0  2.0  3.0  1.0  1.0  1.0  6.0  7.0\n [649]  2.0  4.0  7.0  8.0  1.0  1.0  2.0  0.0  2.0  3.0  3.0  5.0\n [661]  5.0  6.0  8.0  8.0 12.0  5.0  6.0  6.0  9.0  5.0  9.0  8.0\n [673] 11.0  5.0  9.0  7.0  9.0 11.0  7.0  7.0  6.0  4.0  4.0  3.0\n [685]  6.0  3.0  5.0  7.0  6.0  6.0  2.0  3.0  7.0  5.0  6.0  5.0\n [697] 13.0 12.0 16.0  7.0  5.0  5.0  1.0  2.0  7.0  9.0  4.0  4.0\n [709]  3.0  3.0  4.0  4.8 14.0 12.0  8.0  6.0  3.0  3.0  5.0  3.0\n [721]  6.0  7.0  2.0  1.0   NA  4.0  0.0  1.0  1.0  1.0  3.0  6.0\n [733]  8.0 11.0 14.0 12.0  0.0  1.0  6.0  4.0  2.0  1.0  1.0  1.0\n [745]  5.0  2.0  0.0  0.0  1.0  4.0  3.0  0.0 15.0 13.0  4.0  5.0\n [757]  7.0  2.0 10.0  7.0  4.0  2.0  3.0  3.0  3.0  5.0  3.0  1.0\n [769]  2.0  2.0  4.0  3.0  5.0  6.0  1.0  0.0  8.0  7.0  5.0  9.0\n [781]  4.0  8.0  5.0  2.0  2.0  2.0  6.0  7.0  1.0  1.0  2.0  2.0\n [793]  2.0  2.0  3.0  3.0  3.0  9.0  3.0  5.0  7.0  4.0  5.0  8.0\n [805]  4.0  6.0  6.0  3.0  7.0  5.0  2.0  3.0  4.0  4.0  8.0 10.0\n [817]  3.0  4.0  2.0  4.0  4.0  2.0  0.0  0.0  3.0  5.0  5.0  6.0\n [829] 12.0  5.0  4.0  3.0  2.0  2.0  4.8  3.0  0.0  0.0  1.2  2.0\n [841]  7.0   NA  0.0  0.0 15.0 15.0  4.0  3.0  8.0  1.0  5.0  5.0\n [853]  1.0   NA  3.0  4.0  5.0  3.0  4.0  3.0  4.0 11.0 13.0 14.0\n [865] 12.0  7.0  6.0  5.0 13.0 10.0  0.0  2.0  6.0  4.0  5.0  7.0\n [877]  0.0  0.0  4.0  7.0  4.0  3.0 14.0 14.0  4.0  1.0 16.0 12.0\n [889]  9.0 14.0  6.0  7.0  4.0  5.0  2.0  3.0  5.0  3.0  0.0  1.0\n [901]  4.0  3.0  8.0  3.0  5.0  8.0  4.0  2.0 11.0 10.0  9.0 13.0\n [913]  9.0  7.0  5.0  5.0  6.0  7.0  8.0  7.0 14.0  5.0  0.0  0.0\n [925]  6.0  9.0  3.0  5.0  0.0  0.0  3.0  2.0  3.0  5.0  4.0  7.0\n [937]  4.0  6.0  1.0  1.0  3.0  9.0  3.0  2.0  5.0  4.0  4.0  2.0\n [949]  9.0  6.0  9.0 14.0  8.0  7.0 10.0  6.0  1.0  3.0  0.0  5.0\n [961] 11.0  2.0  2.0  1.0  5.0 10.0  0.0  1.0  1.0  3.0  2.0  0.0\n [973]  8.0  8.0  3.0 10.0  6.0  9.0 10.0  5.0  9.0  6.0  1.0  1.0\n [985]  4.0  5.0 24.0 23.0 10.0  9.0  8.0  5.0  4.0  5.0  9.0  3.0\n [997] 13.0 13.0  5.0  2.0  4.0  1.0 13.0 12.0 13.0 13.0  7.0  5.0\n[1009]  5.0  6.0  0.0  2.0  6.0  7.0  0.0  1.0  4.0  2.0  1.0  2.0\n[1021]  2.0  5.0  3.0  3.0  7.0  8.0  5.0  2.0  3.0  4.0  3.0  3.0\n[1033]  2.0  0.0  0.0  0.0  9.0  5.0  6.0  5.0 10.0 14.0  4.0  1.0\n[1045]  5.0  4.0  8.0  6.0  9.0 11.0  6.0 10.0 11.0 15.0  6.0  8.0\n[1057]  3.0  3.0 11.0  8.0  7.0  9.0  4.0  2.0  3.0  2.0 11.0  5.0\n[1069]  4.0  6.0  6.0  3.0  8.0  6.0  2.0  2.0  4.0  6.0  6.0  6.0\n[1081]  4.0  3.0  1.0  2.0  2.0  1.0  0.0  0.0 18.0 13.0  8.0  5.0\n[1093] 13.0 10.0  0.0  0.0  9.0 11.0  8.0 11.0  7.0  6.0  0.0  1.0\n[1105]  2.0  0.0  3.0  2.0  4.0  3.0 10.0 12.0  8.0 10.0 18.0 11.0\n[1117]  3.0  2.0 12.0 13.0  1.0  2.0  2.0  3.0  0.0  3.0  5.0  8.0\n[1129]  5.0  6.0  5.0  7.0  0.0  0.0  6.0  4.0  2.0  4.0  4.0  4.0\n[1141]  2.0  1.0  2.0  0.0 11.0  9.0  4.0  7.0 12.0 15.0  5.0  6.0\n[1153]  4.0  5.0  7.0  8.0 10.0 14.0  4.0  3.6 13.0 12.0  4.0  4.0\n[1165]  6.0  6.0  9.0  8.0  3.0  3.0  0.0  0.0  2.0  3.0  0.0  3.0\n[1177]  5.0  3.0  1.0  0.0  4.0  2.0  0.0  0.0  3.0  6.0  1.0  3.0\n[1189]  8.0  4.0  4.0  1.0 14.0 23.0  9.0  7.0  1.0  2.0  6.0  5.0\n[1201] 16.0  2.0 17.0 14.0   NA  0.0  0.0  2.0  5.0  5.0  4.0  4.0\n[1213]  6.0  8.0  3.0  4.0 12.0  9.0 14.0 10.0  4.8  3.0  8.0  9.0\n[1225]  0.0  2.0  2.0  4.0  9.0  8.0  4.0  1.0  3.0  3.0  8.0  7.0\n[1237] 10.0 16.0   NA  0.0 10.0  6.0  7.0  5.0  2.0  5.0  3.0  6.0\n[1249]  5.0  5.0  5.0  9.0  5.0  2.0  0.0  1.0  4.0  1.0  3.0  5.0\n[1261]  7.0  1.0  5.0  1.0 10.0  6.0  8.0  4.0 13.0 11.0  4.0  7.0\n[1273] 13.0 11.0  4.0  3.0 12.0  5.0  6.0  9.6 13.0 17.0  6.0  8.0\n[1285]  2.0  1.0  1.0  2.0  0.0  1.0 16.0 17.0  6.0 15.0  7.0  4.0\n[1297]  3.0  6.0  5.0  5.0  5.0  5.0  3.0  3.0  6.0  6.0  4.0  4.0\n[1309]  4.0  5.0  2.0  4.0  0.0  0.0  6.0  2.0  2.0  2.0 12.0 12.0\n[1321]  0.0  0.0  6.0  4.0 12.0  0.0  4.0  3.0  8.0 12.0  5.0  4.0\n[1333]  2.0  3.0  4.0  3.0 10.8 10.0  4.0  8.0  5.0  3.0  5.0  9.0\n[1345]  6.0  9.0  4.0  4.0  8.0  8.0  0.0  0.0  2.0  2.0  7.0  3.0\n[1357]  7.0  1.0  1.0  5.0  2.0  4.0 10.0  6.0 15.0  9.0  6.0  4.0\n[1369]  9.0  4.0  3.0  3.0  3.0  6.0 11.0  8.0 13.0 10.0  7.0  6.0\n[1381]  3.0  2.0  2.0  5.0  1.0  1.0  2.0  1.0  0.0  0.0  1.0  0.0\n[1393] 12.0  9.0  3.0  3.0  4.0  4.0  5.0  7.0  1.0  1.0  5.0  4.0\n[1405]  9.0  5.0  1.0  3.0  6.0  6.0  2.0  3.0  2.0  3.0  3.0  0.0\n[1417]  5.0  3.0  3.0  2.0  4.0  2.0  0.0  0.0  6.0  2.0  5.0  0.0\n[1429]  9.0 12.0  7.0  9.0  7.0 12.0 10.8 12.0 13.0 12.0  5.0  1.0\n[1441]  7.0  4.0  2.0 14.0 10.0 10.0  8.0  6.0  5.0  8.0  3.0  2.0\n[1453]  2.0  1.0 10.0 12.0  1.0  5.0  8.0  7.0  2.0  1.0  5.0  6.0\n[1465]  4.0  6.0  1.0  3.0  2.0  6.0  1.0  5.0 13.0 14.0  9.0  7.0\n[1477]  2.0  4.0  6.0  4.0  0.0  3.0  3.0  2.0  6.0 10.0  5.0  3.0\n[1489]  1.0  3.0  4.0  6.0  9.0 11.0  5.0  2.0  7.0  6.0  1.0  0.0\n[1501] 10.0  5.0  5.0  3.0 10.0 17.0 12.0 10.0  1.0  3.0  4.0  3.0\n[1513]  8.0  5.0  2.0  3.0  4.0  2.0  4.0  5.0  2.0  1.0  1.0  2.0\n[1525]  5.0  5.0  1.0  3.0  6.0  3.0  5.0  0.0  8.0 12.0  1.0  2.0\n[1537]  0.0  0.0  3.0  2.0  4.0  1.0 20.4 13.0  8.0  6.0  3.0  5.0\n[1549]  3.0  4.0  7.0  7.0  6.0 11.0  2.0  1.0  0.0  0.0  0.0  1.0\n[1561]  5.0  2.0  4.0  0.0  6.0  8.0  1.0  2.0  1.0  1.0  5.0 10.0\n[1573]  4.0  8.0  2.0  1.0  6.0  5.0  1.0  0.0  3.0  8.0  3.0  5.0\n[1585] 14.0 10.0  9.0 11.0  3.0  7.0  1.0  2.0  4.0  4.0  3.0  3.0\n[1597]  2.0  6.0  1.0  0.0  3.0  3.0 11.0  8.0  1.0  4.0  3.0  3.0\n[1609]  5.0  7.0 12.0  8.0 13.0 18.0  0.0  2.0  2.0  1.0  1.0  1.0\n[1621]  8.0  8.0  2.0  3.0  3.0  2.0  2.0  5.0  2.0  2.0  3.0  5.0\n[1633]  2.0  2.0  2.0  2.0  8.0 10.0  6.0  5.0  7.0  1.0  0.0  0.0\n[1645]  3.0  7.0  7.0  6.0  4.0  5.0  0.0  0.0  7.2  9.0  3.0  2.0\n[1657]  2.0  0.0  3.0  5.0  3.0  5.0  7.0  7.0  2.0  0.0  2.0  9.0\n[1669] 11.0  2.4 10.0  6.0  1.0  3.0  9.0  8.0  3.0  4.0  5.0  3.0\n[1681]  6.0  6.0  2.0  2.0  4.0  5.0  5.0  9.0  2.0  4.0  5.0  3.0\n[1693]  1.0  1.0  1.0  1.0  5.0 11.0  1.0  1.0  5.0  2.0  6.0  4.0\n[1705]  5.0  4.0  8.0  6.0  4.0  5.0  7.0  6.0  1.0  3.0 16.0 18.0\n[1717] 10.0  7.0  1.0  0.0  3.0  2.0  5.0  1.0  6.0  5.0 11.0  6.0\n[1729]  1.0  2.0  6.0  7.0 19.0  9.0  3.0  3.0  7.0  8.0  7.0  3.0\n[1741]  4.0  3.0  6.0 11.0  3.0  3.0  5.0  5.0  3.0  2.0  2.0  4.0\n[1753]  1.0  2.0  3.0  6.0  6.0  6.0  2.0  8.0  1.0  0.0  0.0  4.0\n[1765]  3.0  5.0 11.0  3.0  4.0  5.0  7.0  7.0 11.0 10.0 10.0  4.0\n[1777]  3.0  4.0 15.0  8.0  1.0  1.0  4.0  4.0 12.0 14.0  2.0  2.0\n[1789]  3.0  2.0  6.0  1.0  5.0  1.0  6.0  4.0  4.0  3.0  9.0 12.0\n[1801]  9.0  5.0  7.0  6.0  0.0  1.0  4.0  4.0  4.0  5.0  6.0  1.0\n[1813]  1.0  1.0  0.0  1.0  3.0  4.0  5.0  6.0  6.0  6.0  1.0  0.0\n[1825]  3.0  7.0  0.0  1.0  3.0  6.0  0.0  1.0  8.0  2.0  7.0  3.0\n[1837] 16.0 23.0  8.0  8.0  4.0  2.0  1.0  0.0  2.0  3.0  1.0  3.0\n[1849]  3.0  3.0  7.0  6.0 20.0 19.0  9.0 11.0  7.0  6.0  3.0  3.0\n[1861]  0.0  6.0 13.0 13.0 14.0 11.0  6.0  4.0 12.0  7.0  5.0  6.0\n[1873]  7.0 11.0  5.0  4.0  3.0  2.0  3.0  5.0  1.0  3.0  5.0  4.0\n[1885]  3.0  2.0  0.0  0.0  6.0  8.0 11.0  5.0  3.0  4.0  2.0  7.0\n[1897]  7.0  7.0  3.0  3.0  8.0  7.0  0.0  0.0  8.0  8.0  8.0 11.0\n[1909]  3.0  3.0  2.0  1.0  4.0  3.0 13.0 10.0  5.0  3.0  2.0  3.0\n[1921]  2.0  3.0  4.0  3.0  0.0  0.0 15.0 13.0  7.0  5.0 11.0  5.0\n[1933]  5.0  5.0  0.0  1.0  0.0  5.0  1.0  3.0  2.0  1.0  3.0  5.0\n[1945] 13.0  9.0  8.0  5.0  9.0  6.0  6.0  7.0  5.0  3.0  2.0  3.0\n[1957]  0.0  1.0  5.0  2.0  6.0  2.0  4.0  4.0  4.0  2.0  2.0  3.0\n[1969]  1.0  3.0  8.4  9.0  2.0   NA  5.0  2.0  4.0  1.0  3.0  6.0\n[1981]  0.0  0.0  7.0  6.0  3.0  5.0  4.0  3.0  5.0  6.0 12.0  9.0\n[1993]  8.0  9.0 13.0 11.0  7.0  9.0  4.0  8.0  8.0  7.0  5.0  6.0\n[2005]  0.0  0.0  6.0 14.0  1.0  0.0  3.6   NA  2.0  1.0  2.0  2.0\n[2017]  4.0  2.0 10.0 12.0  4.0  1.0  2.0  3.0  1.0  1.0  7.0  7.0\n[2029]  5.0  5.0  3.0  5.0 13.0 11.0  9.0 14.0  3.0  5.0  4.0  7.0\n[2041]  2.0  6.0  2.0  3.0  4.0  3.0  3.0  5.0  7.0 10.0 10.0  7.0\n[2053]  4.0  4.0 13.0 16.0  3.0  0.0  9.0  9.0  4.0  5.0  4.0  7.0\n[2065]  8.0 11.0  2.0  2.0  0.0  1.0  5.0  5.0 10.0 24.0  5.0  4.0\n[2077]  7.0  6.0  0.0  0.0 10.0  9.0  3.0  1.0  3.0  4.0  4.0  6.0\n[2089]  0.0  0.0  6.0  7.0  3.0  3.0  5.0  5.0  3.0  2.0  8.0 15.0\n[2101]  7.0  7.0  6.0 10.0 11.0 14.0  3.0  3.0  1.0  3.0  9.0  6.0\n[2113]  5.0  2.0  7.0  6.0  2.0  3.0  4.0  3.0  2.0  7.0  5.0  3.0\n[2125]  2.0  4.0  5.0  2.0  5.0  2.0  6.0 11.0  6.0  6.0  1.0  0.0\n[2137]  5.0  3.0  2.0 10.0  1.0  2.0  0.0  1.0  3.0  5.0  5.0  9.0\n[2149]  1.0  4.0  0.0  4.0  0.0  0.0 16.0 13.0 10.0 10.0  3.0  6.0\n[2161] 11.0 15.0  5.0  3.0 12.0 12.0 14.0  9.0  0.0  2.0  5.0  5.0\n[2173]  3.0  5.0  4.0  1.0  3.0  3.0  3.0  2.0  0.0  7.0  0.0  7.0\n[2185]  0.0  1.0  0.0  7.0  2.0  2.0  6.0  3.0  5.0  4.0  6.0  8.0\n[2197]  8.0  4.0 13.0 14.0  5.0  2.0  3.0  5.0  6.0  6.0  1.0  7.0\n[2209]  2.0  0.0  2.0  0.0  3.0  3.0  3.0  5.0  2.0  2.0  8.0  2.0\n[2221] 10.0  4.0 10.0  9.0  4.0  5.0 17.0  9.0  1.0  1.0  2.0  5.0\n[2233]  3.0  4.0  8.0  8.0  2.0  3.0  2.0  3.0  6.0  4.0  1.0  0.0\n[2245] 10.0  9.0  0.0  1.0 24.0 11.0 10.0  3.0  7.0 12.0  5.0  3.0\n[2257]  3.0  4.0  5.0  6.0  3.0  2.0  4.0  4.0  9.0  4.0  7.0  4.0\n[2269]  9.0  6.0  5.0  6.0  4.0  5.0  3.0  2.0  6.0  4.0  6.0  4.0\n[2281]  9.0  6.0  2.0  1.0  4.0  4.0  2.0  4.0  1.0  3.0  2.0  3.0\n[2293] 10.0  8.0 12.0 12.0  5.0  5.0  2.0  2.0 11.0  9.0  7.0  7.0\n[2305]   NA  3.0  4.0  3.0 10.0  8.0  3.0  3.0 11.0  2.0 10.0 11.0\n[2317] 10.0  5.0  2.0  3.0  6.0  9.0  3.0  5.0  8.0  6.0  4.0  3.0\n[2329]  2.0  6.0  6.0  5.0  7.0  2.0  5.0  5.0  3.0  0.0  2.0  2.0\n[2341] 16.0 17.0  3.0  3.0  6.0 10.0  0.0  1.0  4.0  0.0  0.0  1.0\n[2353]  1.0  1.0 12.0 11.0  3.0  3.0  3.0  4.0  2.0  0.0 14.0 16.0\n[2365]  9.0  7.0  0.0  0.0  3.0  1.0  5.0  6.0  4.8  6.0  3.0  4.0\n[2377]  3.0  2.0  7.0  5.0  5.0  4.0  5.0  8.0  1.0  0.0  9.0 14.0\n[2389]  4.0  5.0 10.8  9.0  5.0  1.0  0.0  4.0  3.0   NA  4.0  4.0\n[2401]  1.0  2.0  8.0 10.0  3.0  3.0  4.0  6.0  6.0  7.0  2.0  0.0\n[2413]  1.0  1.0  3.0  4.0  8.0  6.0 11.0 17.0  2.0  2.0  2.0 12.0\n[2425]  6.0  2.0  6.0  7.0 13.0  8.0  7.0  7.0 10.0  9.6  1.0  3.0\n[2437] 13.0 16.0  2.0  5.0  3.0  6.0  6.0  5.0  5.0  5.0  1.0  1.0\n[2449]  5.0  4.0  6.0  6.0  4.0  5.0  3.0  3.0  7.0  7.0  7.0  9.0\n[2461]  5.0  7.0  1.0  0.0  3.0  6.0  4.0  6.0  1.0  0.0 13.0 14.0\n[2473]  2.0  1.0  4.0  6.0  0.0  0.0  1.0  2.0  4.0  7.0  5.0  2.0\n[2485]  1.0  2.0 13.0 13.0  4.0  3.0  3.0  0.0  8.0  9.0  5.0  6.0\n[2497]  4.0  5.0  4.0 10.0  7.0  4.0 12.0 11.0  3.0  6.0  5.0  2.0\n[2509]  7.0  3.0  7.0  8.0  9.0  6.0 11.0  6.0  3.0  1.0  6.0 14.0\n[2521]  4.0  7.0 14.0 13.0  3.0   NA  5.0  3.0  5.0  6.0  8.0  5.0\n[2533]  5.0  7.0  6.0  2.0  4.0  5.0  6.0 10.0  5.0  5.0  6.0  0.0\n[2545]  4.0  7.0  2.0  2.0  6.0  2.0  2.0  1.0  4.0  7.0  4.0  4.0\n[2557]  7.0  5.0  4.0  3.0  9.0 10.0  8.0  9.0  4.0  2.0  3.0  5.0\n[2569]  4.0  2.0  3.0  3.0  1.0  0.0  3.0  4.0  7.2  8.0  3.0  2.0\n[2581]  7.0  7.0  8.0 12.0  4.0  0.0  7.0  4.0  8.0  8.0  9.0  4.0\n[2593]  6.0  5.0  3.0  5.0  8.0  4.0  7.0  4.0 11.0  3.0 14.0  7.0\n[2605]  1.0  5.0  4.0  7.0  2.0  3.0  5.0  1.0  2.0  5.0  6.0  5.0\n[2617]  6.0  7.0  6.0  4.0  7.0  5.0  9.0 11.0  3.0  1.0 12.0  5.0\n[2629]  3.0  3.0  5.0  1.0  9.0 14.0  3.0  7.0  8.0 10.0  1.0  3.0\n[2641]  5.0  2.0  3.0  0.0  3.0 22.0  5.0  3.0  1.0   NA  6.0  3.0\n[2653]  1.0  2.0  9.0  7.0  3.0  1.0  2.0  4.0  6.0  0.0  9.0  6.0\n[2665]  5.0  5.0  5.0  6.0  4.0  4.0  1.0  2.0 12.0  9.0  5.0   NA\n[2677]  2.0  5.0  8.0 10.0  4.0  4.0  8.0  9.0  5.0 12.0 13.0  4.0\n[2689]  4.0  4.0  8.0  8.0  2.0  6.0  4.0   NA  4.0  6.0  4.0  4.0\n[2701]  3.0  4.0  5.0  6.0  5.0  9.0  5.0  6.0 10.0  6.0  8.0 15.0\n[2713]  8.0 11.0  8.0  6.0  4.0  0.0  5.0   NA  9.0   NA 10.0  8.0\n[2725]  2.0  2.0  2.0  0.0  0.0  1.0  3.0  2.0  6.0  6.0 16.0  8.0\n[2737]  0.0  3.0  5.0  5.0  4.0  7.0  5.0  3.0  6.0  9.0 11.0   NA\n[2749]  6.0 13.0  5.0 11.0  2.0  4.0  3.0  2.0  3.0   NA 11.0 11.0\n[2761] 10.0 14.0  5.0  4.0  4.0  5.0  4.0  2.0  0.0  5.0 13.0  7.0\n[2773]  8.0 10.0  3.0  6.0  5.0  2.0  3.0  4.0  4.0  6.0  2.0  2.0\n[2785]  4.0  3.0  7.0  6.0  1.0  4.0  6.0 10.0 10.0  7.0  3.0  5.0\n[2797]  2.0  1.0 11.0 12.0 14.0  9.0  5.0  2.0  6.0  1.0  2.0  0.0\n[2809]  5.0  6.0  5.0  3.0  3.0  3.0  4.0  2.0  2.0  2.0  2.0  2.0\n[2821]  4.0  1.0  9.0 11.0  1.0  0.0 13.2 19.0  7.0  5.0  9.0 11.0\n[2833]  6.0  7.0  9.0  9.0  6.0  5.0  5.0  5.0  1.0  5.0  6.0  9.0\n[2845]  6.0  1.0  0.0  3.0  5.0  3.0  2.0  2.0  3.0  5.0 13.0 15.0\n[2857]  6.0  7.0  8.0   NA 14.0 16.0  6.0  8.0  7.0  3.0 10.0  3.0\n[2869]  2.0  2.0  2.0  2.0 15.0  8.0  0.0  2.0  8.0  5.0  2.0  1.0\n[2881]  7.0   NA  2.0  2.0  1.0  3.0 10.0  8.4  1.0  2.0  3.0  3.0\n[2893]  6.0  8.0  8.0  8.0  4.0  4.0 12.0  6.0  7.0  4.0  0.0  0.0\n[2905]  3.0  5.0  4.0  1.0  9.0  4.0  4.0  1.0  3.0  2.0  1.0  2.0\n[2917]  6.0  0.0  0.0  3.0  1.0 10.0 11.0  6.0  7.0  0.0  5.0  5.0\n[2929]  2.0  3.0  1.0  7.0  9.0  8.0  2.0  2.0  5.0  5.0  8.0  9.0\n[2941]  1.0  0.0  1.0  2.0  0.0 11.0  4.0  3.0  5.0  2.0 14.0  7.0\n[2953]  3.0  5.0  4.0  2.0  5.0  4.0  1.0   NA  4.0 10.8  3.0  3.0\n[2965]  2.0  3.0 18.0 14.0  5.0  5.0 10.0 11.0  4.0  5.0  4.0  3.0\n[2977] 11.0  6.0  7.0  5.0 10.0 11.0 10.0  7.0  2.0  0.0  2.0  1.0\n[2989]  2.0  2.0  6.0  3.0   NA  9.0  5.0  0.0  1.0  2.0 11.0   NA\n[3001]  6.0  5.0  9.0  6.0  7.0  5.0  4.0  1.0  3.0  3.0 10.0  6.0\n[3013] 17.0  3.0  7.0  6.0 15.0 12.0  6.0  7.0  4.0  3.0  7.0  4.0\n[3025]  6.0  5.0  0.0  1.0  7.0  9.6 17.0 17.0  5.0  5.0 10.0 11.0\n[3037]  8.0  5.0  9.0  7.0 12.0 10.0 10.0 14.0  4.0  5.0  4.0  0.0\n[3049]  8.0 15.0 13.0  9.0  0.0  2.0  1.0  3.0  7.0  3.0  3.0  3.0\n[3061] 11.0  1.0  7.0  6.0  6.0  3.0  4.0  5.0  0.0  1.0 13.0  7.0\n[3073] 10.0  3.0  4.0  1.0  2.0  4.0  3.0   NA  8.0  0.0  6.0 11.0\n[3085]  3.0  2.0  8.0  9.0  2.0  2.0  7.0  5.0 19.0 17.0  6.0  1.0\n[3097]  0.0  0.0 14.0  8.0  3.0  2.0  1.0  4.0  5.0  4.0  8.0  3.0\n[3109] 12.0  5.0  2.0  2.0  0.0  1.0  4.0  7.0  5.0  6.0  7.0  6.0\n[3121]  6.0  9.0   NA  1.2  4.0  5.0  6.0  2.0  6.0  3.0  8.0 10.0\n[3133]  6.0  6.0  8.0 20.0 12.0 13.0  5.0  7.0  3.0  3.0  8.0  0.0\n[3145] 10.0  4.0  8.0  2.0  5.0  6.0  6.0  9.0  5.0  2.0 13.0 15.0\n[3157]  8.0  3.0  4.0  6.0  7.0  8.0 10.0  6.0  5.0 10.0  8.0  8.0\n[3169]  1.0  0.0 15.0 13.0  5.0  5.0  2.0  2.0  5.0  4.0 15.0  6.0\n[3181]  0.0  3.0  2.0  7.0  4.0  1.0  7.0  7.5  4.0  1.0  1.0  1.0\n[3193]  8.0  7.0  2.0  4.8 12.0 12.0  5.0  5.0  5.0  5.0  6.0  6.0\n[3205]  7.0  9.0  3.0  4.0  2.0  4.0  7.0 19.0  6.0  5.0  5.0  5.0\n[3217] 12.0 11.0  2.0  5.0 14.0  4.0  8.0  6.0  4.0  3.0  6.0  4.0\n[3229]  0.0  1.0  3.0  5.0  7.0   NA  2.0  0.0  4.0  1.0  6.0  6.0\n[3241]  3.0  3.6 10.0   NA  1.0  0.0  3.0  7.0  2.0  3.0  4.0  3.0\n[3253]  8.0  8.0  4.0  8.0 11.0   NA  2.0  2.0 16.0 11.0 15.0 14.0\n[3265] 14.0  5.0  6.0  1.0  7.0  4.0  5.0  0.0  7.0  4.0 10.0  9.0\n[3277]  6.0  4.0  5.0  3.0  6.0  6.0  5.0  8.0 11.0 10.0  0.0  2.0\n[3289]  3.0  3.0  7.0  5.0  1.0  4.0  4.0  6.0  2.0  1.0  6.0  3.0\n[3301]  4.0  5.0  0.0  2.4  4.0  4.0  8.0  4.0  9.0  6.0  0.0  0.0\n[3313]  6.0  4.0 13.0 11.0  0.0  1.0  5.0  4.0  3.0  1.0 10.0 11.0\n[3325]  4.0  3.0  4.0  3.0  4.0  3.0  8.0  8.0 11.0 12.0  8.0 11.0\n[3337]  3.0  6.0  0.0  0.0 13.0 13.0 10.0  0.0  3.0  7.0  9.0 17.0\n[3349]  1.0  3.0  8.0  6.0  4.0  5.0  4.0 11.0 12.0 13.0  4.0  2.0\n[3361]  3.0  5.0  0.0  0.0 10.0 14.0  2.0  4.0  2.0  2.0  5.0  3.0\n[3373]  0.0  0.0  6.0  8.0 16.0  8.0 15.0 12.0  3.0  2.0  1.0  0.0\n[3385]  5.0  3.0  3.0  2.0  5.0  2.0  4.0  0.0 12.0  9.0  8.0  4.0\n[3397]   NA  1.0 16.0  6.0  2.0  2.0  3.0  3.0  5.0  5.0  2.0  3.0\n[3409]  2.0  0.0 10.0  7.0  0.0  0.0  7.0  9.0  4.0  0.0  9.0  4.0\n[3421]  6.0  6.0  2.0  2.0  4.0  5.0  0.0  1.0  9.0  8.0  9.0  8.0\n[3433] 11.0 11.0  1.0  1.0  4.0  8.0  4.0  0.0  8.0  2.0  9.0  8.0\n[3445]  5.0  1.0  7.0  4.0  2.0  2.4  5.0  7.0  5.0  4.0  4.0  3.0\n[3457]  5.0 10.0  5.0  4.0 17.0 11.0  5.0  6.0  4.0 11.0 10.0 10.0\n[3469]  6.0  6.0  0.0  0.0  6.0  7.0  4.0   NA  2.0  0.0 10.0 12.0\n[3481]  2.0  3.0  3.0  1.0  6.0  6.0  4.0  5.0  6.0  3.0  6.0  0.0\n[3493]  3.0  3.0  2.0  3.0  3.0  1.0  0.0  2.0  0.0  0.0  7.0  7.0\n[3505]  4.0  2.0  6.0  9.0  4.0  5.0  4.0  3.0  3.0  3.0  9.0  2.0\n[3517]  5.0  5.0 11.0 16.0 17.0 17.0  7.0 11.0 14.0 14.0  6.0  5.0\n[3529]  0.0  1.0  7.0  1.0  6.0  7.0  2.0  1.0  4.0  7.0  2.0  3.0\n[3541]  3.0  1.0 15.0  7.0 13.0 11.0  5.0  0.0  5.0  4.0  3.0  3.0\n[3553]  7.0 13.0  3.0  1.0  9.0  5.0 14.0 11.0  4.0  7.0  1.0  1.0\n[3565]  3.0  3.0  7.0  8.0 21.0 17.0  1.0  1.0  3.0  4.0  4.0  3.0\n[3577]  1.0   NA  6.0  4.0  3.0  8.0  0.0  1.0  4.0  4.0  8.0  7.0\n[3589] 14.0 15.0 10.0  9.0  2.0   NA 13.0 12.0  0.0  1.0 10.0  9.0\n[3601]  4.0  5.0  5.0  5.0  1.0  1.0  0.0  0.0  4.0  4.0  1.0  0.0\n[3613]  4.0  2.0  8.0  1.0  3.0  8.0  0.0  0.0  2.0  4.0  8.0  7.0\n[3625]  3.0  1.0 13.0 11.0 12.0 14.0  0.0  2.0  5.0  4.0  0.0  6.0\n[3637]  7.0  7.0  5.0  5.0 22.0 24.0  4.0  5.0 13.0 12.0 12.0 16.0\n[3649]  2.0  7.0 15.0  6.0  6.0  4.0  6.0  3.0  2.0  2.0  6.0  5.0\n[3661]  3.0  5.0  3.0  1.0  2.0  2.0  0.0  6.0  5.0  6.0  2.0  3.0\n[3673]  8.0  8.0  7.0  6.0  1.0  0.0  2.0  6.0  8.0 11.0  2.0  3.0\n[3685]  4.0  4.0  9.0  5.0 11.0 14.0  7.0  8.0  1.0  1.0 16.0 15.0\n[3697]  6.0  3.0  4.0  4.0  2.0  4.0  4.0  1.0  5.0  1.0  1.0  1.0\n[3709]  3.0  3.0  6.0  5.0  5.0  2.0  8.0  7.0 13.0 13.0  7.0  2.4\n[3721]  2.0  1.0  3.0  4.0  5.0  4.0  7.0  5.0  4.0  0.0  2.0  0.0\n[3733]  1.0  2.0  3.0  2.0 13.0  9.0  4.0  3.0  5.0  4.0  3.0  2.0\n[3745] 11.0 12.0 16.0  7.0 12.0  4.0  3.0  4.0  3.0  4.0  5.0  6.0\n[3757]  4.0  3.0  1.0  3.0  4.0  5.0  5.0 10.0  1.0  2.0  2.0  3.0\n[3769] 12.0 12.0  0.0  0.0  8.0  4.0  5.0 13.0  2.0   NA  7.0  8.0\n[3781]  6.0  7.0  4.0  4.0 14.0 24.0  7.0  5.0  8.0 10.0  2.0  3.0\n[3793]  6.0  8.0  7.0  5.0  4.0  3.0  8.0  4.0  5.0  9.0  4.0  4.0\n[3805]  0.0  0.0  4.0  6.0  7.0 13.0  5.0  4.0  1.0  3.0 12.0 13.0\n[3817]  5.0  5.0  1.0  1.0  7.0  8.0  4.0  4.0 18.0 18.0  3.0  3.0\n[3829] 11.0 17.0  1.0  0.0  7.0  5.0  3.0  1.0 11.0  5.0  2.0  1.0\n[3841]  1.0  4.0  5.0  5.0  7.0  6.0  5.0  9.0  8.0  6.0 16.0 18.0\n[3853] 12.0 15.0  3.0  4.0  7.0  9.0 12.0  7.0  6.0 13.0 10.0  8.0\n[3865]  3.0  2.0  1.0  2.0  7.0 11.0  3.0 18.0  2.0  2.0  1.0  1.0\n[3877] 14.0 11.0  5.0  5.0  5.0  5.0 12.0  6.0 12.0  0.0 10.0 10.0\n[3889] 17.0 19.0  3.0  3.0  5.0  4.0  3.0  2.0   NA  5.0  1.0  5.0\n[3901]  4.0  8.0  8.0  7.0  1.0  2.0  4.0  3.0  1.0  3.0  1.0  0.0\n[3913] 16.0  9.0  4.0  5.0  3.0  6.0  7.0  6.0  7.0  8.0  2.0  1.0\n[3925]  7.0  7.0  7.0  9.0  4.0  3.0 11.0  9.0  4.0  1.0  6.0 10.0\n[3937]  5.0 15.0 13.0 12.0 18.0 13.0  0.0  5.0 10.0 10.0  7.0  9.0\n[3949] 13.0  5.0  3.0  4.0 15.0 10.0  4.0  4.0 14.0  1.0  5.0  0.0\n[3961]  7.0  6.0  7.0  4.0  7.0  5.0  7.0  4.0  7.0  7.0  4.0  8.0\n[3973]  6.0  8.0 12.0 15.0  2.0  1.0  4.0 10.0  0.0  0.0 14.0  5.0\n[3985]  5.0  4.0  4.0  5.0  3.0  9.0 10.0 13.0  1.0  1.0  3.0  2.0\n[3997]  5.0  2.0  3.0   NA 11.0  7.0  0.0  0.0  8.0  8.0  0.0  0.0\n[4009]  5.0  8.0  5.0  2.0  5.0  7.0 10.0  9.0  3.0  0.0  3.0  2.0\n[4021]  5.0  4.0  5.0  2.0  6.0  5.0  1.0  2.0  1.0  1.0  3.0  2.4\n[4033]  6.0  7.2  7.0  5.0   NA  4.8  8.0   NA  7.0  5.0 17.0 11.0\n[4045] 13.0 13.0  3.0  5.0  5.0  4.0  9.0  5.0  3.0  6.0 17.0  9.0\n[4057]  0.0  1.0  1.0  0.0  4.0  5.0  0.0  0.0  7.0  4.0 10.0  9.0\n[4069]  4.0  8.0  6.0  2.0   NA  8.0  5.0  8.0  3.0  3.0  6.0 14.0\n[4081]  7.0  7.0 11.0 10.0  5.0  5.0 11.0  6.0  8.0  6.0  2.0  4.0\n[4093]  0.0  3.0  1.0  1.0  2.0  0.0  8.0  6.0  8.0  8.0 11.0 11.0\n[4105]  5.0  6.0  4.0  7.0  8.0  6.0  3.0  1.0 10.0 10.0  6.0  4.0\n[4117]  6.0  3.0  7.0  5.0 18.0  8.0  4.0  4.0 17.0 15.0\n\nR has loads of canned solutions\nskimr\nThe skimmer package can be helpful in detecting problems. A drawback note that it is interpreting all factors as numbers).\nFor example, “spiritual identification” was not measured in 2019/2020.\n\n\nlibrary(\"skimr\")\n\nnz %>%\n  dplyr::group_by(Wave) %>%\n  skim()\n\n\n\nTable 2: Data summary\n\n\n\n\n\n\nName\n\n\nPiped data\n\n\nNumber of rows\n\n\n4126\n\n\nNumber of columns\n\n\n66\n\n\n_______________________\n\n\n\n\nColumn type frequency:\n\n\n\n\nDate\n\n\n1\n\n\nfactor\n\n\n13\n\n\nlogical\n\n\n1\n\n\nnumeric\n\n\n50\n\n\n________________________\n\n\n\n\nGroup variables\n\n\nWave\n\nVariable type: Date\n\nskim_variable\n\n\nWave\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmin\n\n\nmax\n\n\nmedian\n\n\nn_unique\n\n\ndate\n\n\n2018\n\n\n0\n\n\n1\n\n\n2018-01-02\n\n\n2019-10-25\n\n\n2018-08-09\n\n\n312\n\n\ndate\n\n\n2019\n\n\n0\n\n\n1\n\n\n2019-09-09\n\n\n2020-10-06\n\n\n2019-12-07\n\n\n298\n\nVariable type: factor\n\nskim_variable\n\n\nWave\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\nMale\n\n\n2018\n\n\n3\n\n\n1.00\n\n\nFALSE\n\n\n2\n\n\nNot: 1305, Mal: 755\n\n\nMale\n\n\n2019\n\n\n6\n\n\n1.00\n\n\nFALSE\n\n\n2\n\n\nNot: 1305, Mal: 752\n\n\nBigDoms\n\n\n2018\n\n\n42\n\n\n0.98\n\n\nFALSE\n\n\n5\n\n\nNot: 1301, Chr: 624, The: 75, Bud: 17\n\n\nBigDoms\n\n\n2019\n\n\n52\n\n\n0.97\n\n\nFALSE\n\n\n5\n\n\nNot: 1354, Chr: 580, The: 53, Bud: 19\n\n\nGenCohort\n\n\n2018\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n5\n\n\nGen: 915, Gen: 676, Gen: 368, Gen: 53\n\n\nGenCohort\n\n\n2019\n\n\n0\n\n\n1.00\n\n\nFALSE\n\n\n5\n\n\nGen: 915, Gen: 676, Gen: 368, Gen: 53\n\n\nReligious\n\n\n2018\n\n\n42\n\n\n0.98\n\n\nFALSE\n\n\n2\n\n\nNot: 1301, Rel: 720\n\n\nReligious\n\n\n2019\n\n\n52\n\n\n0.97\n\n\nFALSE\n\n\n2\n\n\nNot: 1354, Rel: 657\n\n\nBelieve.God\n\n\n2018\n\n\n95\n\n\n0.95\n\n\nFALSE\n\n\n2\n\n\nNot: 1075, Bel: 893\n\n\nBelieve.God\n\n\n2019\n\n\n51\n\n\n0.98\n\n\nFALSE\n\n\n2\n\n\nNot: 1169, Bel: 843\n\n\nBelieve.Spirit\n\n\n2018\n\n\n95\n\n\n0.95\n\n\nFALSE\n\n\n2\n\n\nBel: 1330, Not: 638\n\n\nBelieve.Spirit\n\n\n2019\n\n\n51\n\n\n0.98\n\n\nFALSE\n\n\n2\n\n\nBel: 1291, Not: 721\n\n\nFeelHopeless\n\n\n2018\n\n\n30\n\n\n0.99\n\n\nFALSE\n\n\n5\n\n\nNon: 1012, A L: 627, Som: 325, Mos: 58\n\n\nFeelHopeless\n\n\n2019\n\n\n19\n\n\n0.99\n\n\nFALSE\n\n\n5\n\n\nNon: 961, A L: 660, Som: 348, Mos: 67\n\n\nFeelDepressed\n\n\n2018\n\n\n34\n\n\n0.98\n\n\nFALSE\n\n\n5\n\n\nNon: 1481, A L: 345, Som: 156, Mos: 35\n\n\nFeelDepressed\n\n\n2019\n\n\n12\n\n\n0.99\n\n\nFALSE\n\n\n5\n\n\nNon: 1399, A L: 403, Som: 195, Mos: 47\n\n\nFeelRestless\n\n\n2018\n\n\n35\n\n\n0.98\n\n\nFALSE\n\n\n5\n\n\nA L: 751, Som: 585, Non: 520, Mos: 143\n\n\nFeelRestless\n\n\n2019\n\n\n16\n\n\n0.99\n\n\nFALSE\n\n\n5\n\n\nA L: 794, Som: 577, Non: 501, Mos: 158\n\n\nEverythingIsEffort\n\n\n2018\n\n\n32\n\n\n0.98\n\n\nFALSE\n\n\n5\n\n\nA L: 827, Non: 523, Som: 490, Mos: 159\n\n\nEverythingIsEffort\n\n\n2019\n\n\n15\n\n\n0.99\n\n\nFALSE\n\n\n5\n\n\nA L: 798, Som: 572, Non: 470, Mos: 179\n\n\nFeelWorthless\n\n\n2018\n\n\n32\n\n\n0.98\n\n\nFALSE\n\n\n5\n\n\nNon: 1469, A L: 348, Som: 147, Mos: 48\n\n\nFeelWorthless\n\n\n2019\n\n\n14\n\n\n0.99\n\n\nFALSE\n\n\n5\n\n\nNon: 1434, A L: 359, Som: 193, Mos: 51\n\n\nFeelNervous\n\n\n2018\n\n\n37\n\n\n0.98\n\n\nFALSE\n\n\n5\n\n\nA L: 804, Non: 537, Som: 509, Mos: 148\n\n\nFeelNervous\n\n\n2019\n\n\n13\n\n\n0.99\n\n\nFALSE\n\n\n5\n\n\nA L: 824, Non: 540, Som: 539, Mos: 122\n\n\nk6cats\n\n\n2018\n\n\n28\n\n\n0.99\n\n\nFALSE\n\n\n3\n\n\nLow: 1272, Mod: 675, Ser: 88\n\n\nk6cats\n\n\n2019\n\n\n11\n\n\n0.99\n\n\nFALSE\n\n\n3\n\n\nLow: 1238, Mod: 725, Ser: 89\n\nVariable type: logical\n\nskim_variable\n\n\nWave\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\ncount\n\n\nEnv.SacMade\n\n\n2018\n\n\n2063\n\n\n0\n\n\nNaN\n\n\n:\n\n\nEnv.SacMade\n\n\n2019\n\n\n2063\n\n\n0\n\n\nNaN\n\n\n:\n\nVariable type: numeric\n\nskim_variable\n\n\nWave\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\nId\n\n\n2018\n\n\n0\n\n\n1.00\n\n\n1032.00\n\n\n595.68\n\n\n1.00\n\n\n516.50\n\n\n1032.00\n\n\n1547.50\n\n\n2063.00\n\n\n▇▇▇▇▇\n\n\nId\n\n\n2019\n\n\n0\n\n\n1.00\n\n\n1032.00\n\n\n595.68\n\n\n1.00\n\n\n516.50\n\n\n1032.00\n\n\n1547.50\n\n\n2063.00\n\n\n▇▇▇▇▇\n\n\nyears\n\n\n2018\n\n\n0\n\n\n1.00\n\n\n9.09\n\n\n0.33\n\n\n8.34\n\n\n8.82\n\n\n8.94\n\n\n9.28\n\n\n10.15\n\n\n▁▇▃▂▁\n\n\nyears\n\n\n2019\n\n\n0\n\n\n1.00\n\n\n10.34\n\n\n0.24\n\n\n10.03\n\n\n10.14\n\n\n10.27\n\n\n10.47\n\n\n11.10\n\n\n▇▇▃▁▁\n\n\nAge\n\n\n2018\n\n\n0\n\n\n1.00\n\n\n50.20\n\n\n13.40\n\n\n18.00\n\n\n41.00\n\n\n52.00\n\n\n60.00\n\n\n91.00\n\n\n▂▅▇▃▁\n\n\nAge\n\n\n2019\n\n\n0\n\n\n1.00\n\n\n51.95\n\n\n13.37\n\n\n19.47\n\n\n43.09\n\n\n54.16\n\n\n62.16\n\n\n92.87\n\n\n▂▅▇▃▁\n\n\nGender\n\n\n2018\n\n\n3\n\n\n1.00\n\n\n0.37\n\n\n0.48\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n▇▁▁▁▅\n\n\nGender\n\n\n2019\n\n\n6\n\n\n1.00\n\n\n0.37\n\n\n0.48\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n▇▁▁▁▅\n\n\nEdu\n\n\n2018\n\n\n77\n\n\n0.96\n\n\n5.54\n\n\n2.73\n\n\n0.00\n\n\n3.00\n\n\n7.00\n\n\n7.00\n\n\n10.00\n\n\n▃▃▂▇▃\n\n\nEdu\n\n\n2019\n\n\n50\n\n\n0.98\n\n\n5.71\n\n\n2.66\n\n\n0.00\n\n\n3.00\n\n\n7.00\n\n\n8.00\n\n\n10.00\n\n\n▃▃▂▇▂\n\n\nPartner\n\n\n2018\n\n\n83\n\n\n0.96\n\n\n0.76\n\n\n0.42\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n▂▁▁▁▇\n\n\nPartner\n\n\n2019\n\n\n57\n\n\n0.97\n\n\n0.77\n\n\n0.42\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n▂▁▁▁▇\n\n\nBornNZ\n\n\n2018\n\n\n40\n\n\n0.98\n\n\n0.79\n\n\n0.41\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n▂▁▁▁▇\n\n\nBornNZ\n\n\n2019\n\n\n2\n\n\n1.00\n\n\n0.79\n\n\n0.41\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n▂▁▁▁▇\n\n\nTSCORE\n\n\n2018\n\n\n0\n\n\n1.00\n\n\n3381.65\n\n\n120.68\n\n\n3108.00\n\n\n3283.00\n\n\n3327.00\n\n\n3450.00\n\n\n3769.00\n\n\n▁▇▃▂▁\n\n\nTSCORE\n\n\n2019\n\n\n0\n\n\n1.00\n\n\n3838.32\n\n\n88.37\n\n\n3723.00\n\n\n3764.00\n\n\n3812.00\n\n\n3884.00\n\n\n4116.00\n\n\n▇▇▃▁▁\n\n\nReligion.Church\n\n\n2018\n\n\n49\n\n\n0.98\n\n\n0.77\n\n\n3.01\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n60.00\n\n\n▇▁▁▁▁\n\n\nReligion.Church\n\n\n2019\n\n\n60\n\n\n0.97\n\n\n0.74\n\n\n2.93\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n60.00\n\n\n▇▁▁▁▁\n\n\nReligion.Believe.Cats\n\n\n2018\n\n\n95\n\n\n0.95\n\n\n2.42\n\n\n1.29\n\n\n1.00\n\n\n1.00\n\n\n3.00\n\n\n4.00\n\n\n4.00\n\n\n▇▁▁▅▆\n\n\nReligion.Believe.Cats\n\n\n2019\n\n\n51\n\n\n0.98\n\n\n2.52\n\n\n1.26\n\n\n1.00\n\n\n1.00\n\n\n3.00\n\n\n4.00\n\n\n4.00\n\n\n▇▁▁▆▆\n\n\nRelid\n\n\n2018\n\n\n51\n\n\n0.98\n\n\n1.69\n\n\n2.58\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n4.00\n\n\n7.00\n\n\n▇▁▁▁▂\n\n\nRelid\n\n\n2019\n\n\n112\n\n\n0.95\n\n\n1.56\n\n\n2.57\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n3.00\n\n\n7.00\n\n\n▇▁▁▁▂\n\n\nHLTH.Fatigue\n\n\n2018\n\n\n28\n\n\n0.99\n\n\n1.58\n\n\n1.05\n\n\n0.00\n\n\n1.00\n\n\n2.00\n\n\n2.00\n\n\n4.00\n\n\n▃▇▇▃▁\n\n\nHLTH.Fatigue\n\n\n2019\n\n\n13\n\n\n0.99\n\n\n1.64\n\n\n1.04\n\n\n0.00\n\n\n1.00\n\n\n2.00\n\n\n2.00\n\n\n4.00\n\n\n▃▇▇▃▁\n\n\nHLTH.SleepHours\n\n\n2018\n\n\n103\n\n\n0.95\n\n\n6.92\n\n\n1.07\n\n\n3.00\n\n\n6.00\n\n\n7.00\n\n\n8.00\n\n\n11.00\n\n\n▁▆▇▅▁\n\n\nHLTH.SleepHours\n\n\n2019\n\n\n78\n\n\n0.96\n\n\n6.88\n\n\n1.09\n\n\n2.00\n\n\n6.00\n\n\n7.00\n\n\n8.00\n\n\n12.00\n\n\n▁▃▇▁▁\n\n\nHomeOwner\n\n\n2018\n\n\n82\n\n\n0.96\n\n\n0.77\n\n\n0.42\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n▂▁▁▁▇\n\n\nHomeOwner\n\n\n2019\n\n\n2063\n\n\n0.00\n\n\nNaN\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\nPol.Orient\n\n\n2018\n\n\n119\n\n\n0.94\n\n\n3.58\n\n\n1.40\n\n\n1.00\n\n\n2.00\n\n\n4.00\n\n\n4.00\n\n\n7.00\n\n\n▇▅▇▃▂\n\n\nPol.Orient\n\n\n2019\n\n\n69\n\n\n0.97\n\n\n3.58\n\n\n1.37\n\n\n1.00\n\n\n2.00\n\n\n4.00\n\n\n4.00\n\n\n7.00\n\n\n▇▆▇▅▂\n\n\nPATRIOT\n\n\n2018\n\n\n2\n\n\n1.00\n\n\n5.86\n\n\n1.05\n\n\n1.00\n\n\n5.50\n\n\n6.00\n\n\n6.50\n\n\n7.00\n\n\n▁▁▁▃▇\n\n\nPATRIOT\n\n\n2019\n\n\n2\n\n\n1.00\n\n\n5.96\n\n\n1.01\n\n\n1.00\n\n\n5.50\n\n\n6.00\n\n\n7.00\n\n\n7.00\n\n\n▁▁▁▂▇\n\n\nEnv.SatNZEnvironment\n\n\n2018\n\n\n8\n\n\n1.00\n\n\n5.33\n\n\n2.48\n\n\n0.00\n\n\n3.00\n\n\n6.00\n\n\n7.00\n\n\n10.00\n\n\n▅▇▇▇▃\n\n\nEnv.SatNZEnvironment\n\n\n2019\n\n\n3\n\n\n1.00\n\n\n5.23\n\n\n2.46\n\n\n0.00\n\n\n3.00\n\n\n5.00\n\n\n7.00\n\n\n10.00\n\n\n▅▆▇▇▂\n\n\nEnv.MotorwaySpend\n\n\n2018\n\n\n15\n\n\n0.99\n\n\n4.69\n\n\n1.64\n\n\n1.00\n\n\n4.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n▂▂▅▅▇\n\n\nEnv.MotorwaySpend\n\n\n2019\n\n\n18\n\n\n0.99\n\n\n4.79\n\n\n1.69\n\n\n1.00\n\n\n4.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n▂▂▅▅▇\n\n\nEnv.PubTransSubs\n\n\n2018\n\n\n17\n\n\n0.99\n\n\n5.66\n\n\n1.29\n\n\n1.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n7.00\n\n\n▁▁▂▃▇\n\n\nEnv.PubTransSubs\n\n\n2019\n\n\n18\n\n\n0.99\n\n\n5.55\n\n\n1.32\n\n\n1.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n7.00\n\n\n▁▁▂▃▇\n\n\nEnv.ClimateChgConcern\n\n\n2018\n\n\n125\n\n\n0.94\n\n\n5.35\n\n\n1.68\n\n\n1.00\n\n\n4.00\n\n\n6.00\n\n\n7.00\n\n\n7.00\n\n\n▁▁▂▂▇\n\n\nEnv.ClimateChgConcern\n\n\n2019\n\n\n56\n\n\n0.97\n\n\n5.25\n\n\n1.75\n\n\n1.00\n\n\n4.00\n\n\n6.00\n\n\n7.00\n\n\n7.00\n\n\n▂▁▂▃▇\n\n\nLIFEMEANING\n\n\n2018\n\n\n0\n\n\n1.00\n\n\n5.50\n\n\n1.17\n\n\n1.00\n\n\n5.00\n\n\n5.50\n\n\n6.50\n\n\n7.00\n\n\n▁▁▃▅▇\n\n\nLIFEMEANING\n\n\n2019\n\n\n0\n\n\n1.00\n\n\n5.48\n\n\n1.17\n\n\n1.00\n\n\n5.00\n\n\n5.50\n\n\n6.50\n\n\n7.00\n\n\n▁▁▃▅▇\n\n\nHours.Internet\n\n\n2018\n\n\n57\n\n\n0.97\n\n\n13.54\n\n\n15.57\n\n\n0.00\n\n\n4.00\n\n\n10.00\n\n\n20.00\n\n\n140.00\n\n\n▇▁▁▁▁\n\n\nHours.Internet\n\n\n2019\n\n\n46\n\n\n0.98\n\n\n14.24\n\n\n15.36\n\n\n0.00\n\n\n5.00\n\n\n10.00\n\n\n20.00\n\n\n168.00\n\n\n▇▁▁▁▁\n\n\nIssue.GovtSurveillance\n\n\n2018\n\n\n22\n\n\n0.99\n\n\n4.35\n\n\n1.81\n\n\n1.00\n\n\n3.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n▅▃▅▅▇\n\n\nIssue.GovtSurveillance\n\n\n2019\n\n\n15\n\n\n0.99\n\n\n4.64\n\n\n1.75\n\n\n1.00\n\n\n4.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n▃▂▅▅▇\n\n\nIssue.RegulateAI\n\n\n2018\n\n\n26\n\n\n0.99\n\n\n4.37\n\n\n1.64\n\n\n1.00\n\n\n3.00\n\n\n4.00\n\n\n6.00\n\n\n7.00\n\n\n▃▃▇▅▇\n\n\nIssue.RegulateAI\n\n\n2019\n\n\n2063\n\n\n0.00\n\n\nNaN\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\nIssue.IncomeRedistribution\n\n\n2018\n\n\n16\n\n\n0.99\n\n\n4.25\n\n\n1.90\n\n\n1.00\n\n\n3.00\n\n\n4.00\n\n\n6.00\n\n\n7.00\n\n\n▆▃▅▅▇\n\n\nIssue.IncomeRedistribution\n\n\n2019\n\n\n18\n\n\n0.99\n\n\n4.28\n\n\n1.95\n\n\n1.00\n\n\n3.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n▆▃▃▅▇\n\n\nHours.Exercise\n\n\n2018\n\n\n57\n\n\n0.97\n\n\n5.63\n\n\n7.45\n\n\n0.00\n\n\n2.00\n\n\n4.00\n\n\n7.00\n\n\n80.00\n\n\n▇▁▁▁▁\n\n\nHours.Exercise\n\n\n2019\n\n\n46\n\n\n0.98\n\n\n5.77\n\n\n8.59\n\n\n0.00\n\n\n2.00\n\n\n4.00\n\n\n7.00\n\n\n168.00\n\n\n▇▁▁▁▁\n\n\nHours.Work\n\n\n2018\n\n\n57\n\n\n0.97\n\n\n27.78\n\n\n20.31\n\n\n0.00\n\n\n1.00\n\n\n35.00\n\n\n42.00\n\n\n100.00\n\n\n▇▇▅▁▁\n\n\nHours.Work\n\n\n2019\n\n\n46\n\n\n0.98\n\n\n26.66\n\n\n20.21\n\n\n0.00\n\n\n0.00\n\n\n32.00\n\n\n40.00\n\n\n108.00\n\n\n▇▇▃▁▁\n\n\nHours.News\n\n\n2018\n\n\n57\n\n\n0.97\n\n\n4.30\n\n\n4.97\n\n\n0.00\n\n\n1.00\n\n\n3.00\n\n\n6.00\n\n\n70.00\n\n\n▇▁▁▁▁\n\n\nHours.News\n\n\n2019\n\n\n46\n\n\n0.98\n\n\n4.25\n\n\n4.65\n\n\n0.00\n\n\n1.00\n\n\n3.00\n\n\n6.00\n\n\n90.00\n\n\n▇▁▁▁▁\n\n\nCONSCIENTIOUSNESS\n\n\n2018\n\n\n26\n\n\n0.99\n\n\n5.15\n\n\n1.05\n\n\n1.00\n\n\n4.50\n\n\n5.25\n\n\n6.00\n\n\n7.00\n\n\n▁▁▅▇▅\n\n\nCONSCIENTIOUSNESS\n\n\n2019\n\n\n11\n\n\n0.99\n\n\n5.12\n\n\n1.03\n\n\n1.75\n\n\n4.50\n\n\n5.25\n\n\n6.00\n\n\n7.00\n\n\n▁▂▆▇▆\n\n\nEXTRAVERSION\n\n\n2018\n\n\n26\n\n\n0.99\n\n\n3.90\n\n\n1.19\n\n\n1.00\n\n\n3.00\n\n\n4.00\n\n\n4.75\n\n\n7.00\n\n\n▁▆▇▅▁\n\n\nEXTRAVERSION\n\n\n2019\n\n\n11\n\n\n0.99\n\n\n3.85\n\n\n1.21\n\n\n1.00\n\n\n3.00\n\n\n3.75\n\n\n4.75\n\n\n7.00\n\n\n▂▆▇▅▁\n\n\nAGREEABLENESS\n\n\n2018\n\n\n28\n\n\n0.99\n\n\n5.38\n\n\n0.98\n\n\n1.00\n\n\n4.75\n\n\n5.50\n\n\n6.00\n\n\n7.00\n\n\n▁▁▃▇▆\n\n\nAGREEABLENESS\n\n\n2019\n\n\n11\n\n\n0.99\n\n\n5.36\n\n\n0.99\n\n\n1.00\n\n\n4.75\n\n\n5.50\n\n\n6.00\n\n\n7.00\n\n\n▁▁▃▇▆\n\n\nOPENNESS\n\n\n2018\n\n\n26\n\n\n0.99\n\n\n4.98\n\n\n1.14\n\n\n1.00\n\n\n4.25\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n▁▂▆▇▆\n\n\nOPENNESS\n\n\n2019\n\n\n11\n\n\n0.99\n\n\n4.99\n\n\n1.11\n\n\n1.00\n\n\n4.25\n\n\n5.00\n\n\n5.75\n\n\n7.00\n\n\n▁▂▆▇▅\n\n\nSpiritual.Identification\n\n\n2018\n\n\n116\n\n\n0.94\n\n\n3.88\n\n\n2.13\n\n\n1.00\n\n\n2.00\n\n\n4.00\n\n\n6.00\n\n\n7.00\n\n\n▇▂▃▃▇\n\n\nSpiritual.Identification\n\n\n2019\n\n\n2063\n\n\n0.00\n\n\nNaN\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\nHoursCharity\n\n\n2018\n\n\n57\n\n\n0.97\n\n\n1.61\n\n\n4.83\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n100.00\n\n\n▇▁▁▁▁\n\n\nHoursCharity\n\n\n2019\n\n\n46\n\n\n0.98\n\n\n1.58\n\n\n4.87\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n90.00\n\n\n▇▁▁▁▁\n\n\nCharityDonate\n\n\n2018\n\n\n111\n\n\n0.95\n\n\n1423.08\n\n\n14107.80\n\n\n0.00\n\n\n30.00\n\n\n165.00\n\n\n500.00\n\n\n500000.00\n\n\n▇▁▁▁▁\n\n\nCharityDonate\n\n\n2019\n\n\n103\n\n\n0.95\n\n\n1312.83\n\n\n15426.74\n\n\n0.00\n\n\n24.00\n\n\n150.00\n\n\n500.00\n\n\n650000.00\n\n\n▇▁▁▁▁\n\n\nYour.Personal.Relationships\n\n\n2018\n\n\n7\n\n\n1.00\n\n\n7.76\n\n\n2.22\n\n\n0.00\n\n\n7.00\n\n\n8.00\n\n\n9.00\n\n\n10.00\n\n\n▁▁▂▆▇\n\n\nYour.Personal.Relationships\n\n\n2019\n\n\n4\n\n\n1.00\n\n\n7.71\n\n\n2.28\n\n\n0.00\n\n\n7.00\n\n\n8.00\n\n\n9.00\n\n\n10.00\n\n\n▁▁▂▅▇\n\n\nYour.Future.Security\n\n\n2018\n\n\n7\n\n\n1.00\n\n\n6.30\n\n\n2.32\n\n\n0.00\n\n\n5.00\n\n\n7.00\n\n\n8.00\n\n\n10.00\n\n\n▂▃▆▇▃\n\n\nYour.Future.Security\n\n\n2019\n\n\n2\n\n\n1.00\n\n\n6.27\n\n\n2.35\n\n\n0.00\n\n\n5.00\n\n\n7.00\n\n\n8.00\n\n\n10.00\n\n\n▂▃▆▇▃\n\n\nStandard.Living\n\n\n2018\n\n\n3\n\n\n1.00\n\n\n7.65\n\n\n1.98\n\n\n0.00\n\n\n7.00\n\n\n8.00\n\n\n9.00\n\n\n10.00\n\n\n▁▁▂▇▆\n\n\nStandard.Living\n\n\n2019\n\n\n12\n\n\n0.99\n\n\n7.77\n\n\n1.93\n\n\n0.00\n\n\n7.00\n\n\n8.00\n\n\n9.00\n\n\n10.00\n\n\n▁▁▂▇▇\n\n\nNZ.Economic.Situation\n\n\n2018\n\n\n10\n\n\n1.00\n\n\n5.48\n\n\n2.17\n\n\n0.00\n\n\n4.00\n\n\n6.00\n\n\n7.00\n\n\n10.00\n\n\n▂▅▇▇▁\n\n\nNZ.Economic.Situation\n\n\n2019\n\n\n9\n\n\n1.00\n\n\n5.35\n\n\n2.16\n\n\n0.00\n\n\n4.00\n\n\n5.00\n\n\n7.00\n\n\n10.00\n\n\n▂▅▇▆▁\n\n\nNZ.Social.Conditions\n\n\n2018\n\n\n10\n\n\n1.00\n\n\n4.63\n\n\n2.19\n\n\n0.00\n\n\n3.00\n\n\n5.00\n\n\n6.00\n\n\n10.00\n\n\n▅▇▇▅▁\n\n\nNZ.Social.Conditions\n\n\n2019\n\n\n6\n\n\n1.00\n\n\n4.65\n\n\n2.19\n\n\n0.00\n\n\n3.00\n\n\n5.00\n\n\n6.00\n\n\n10.00\n\n\n▅▇▇▅▁\n\n\nNZ.Business.Conditions\n\n\n2018\n\n\n18\n\n\n0.99\n\n\n5.83\n\n\n1.88\n\n\n0.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n10.00\n\n\n▁▂▇▆▁\n\n\nNZ.Business.Conditions\n\n\n2019\n\n\n13\n\n\n0.99\n\n\n5.61\n\n\n1.93\n\n\n0.00\n\n\n5.00\n\n\n5.00\n\n\n7.00\n\n\n10.00\n\n\n▁▂▇▅▁\n\n\nEmp.JobSecure\n\n\n2018\n\n\n510\n\n\n0.75\n\n\n5.48\n\n\n1.56\n\n\n1.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n7.00\n\n\n▁▁▁▂▇\n\n\nEmp.JobSecure\n\n\n2019\n\n\n515\n\n\n0.75\n\n\n5.41\n\n\n1.57\n\n\n1.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n7.00\n\n\n▁▁▂▂▇\n\n\nIssue.Food.GMO\n\n\n2018\n\n\n19\n\n\n0.99\n\n\n4.95\n\n\n1.82\n\n\n1.00\n\n\n4.00\n\n\n5.00\n\n\n7.00\n\n\n7.00\n\n\n▂▂▃▂▇\n\n\nIssue.Food.GMO\n\n\n2019\n\n\n15\n\n\n0.99\n\n\n4.73\n\n\n1.85\n\n\n1.00\n\n\n4.00\n\n\n5.00\n\n\n6.00\n\n\n7.00\n\n\n▃▂▅▃▇\n\n\nKESSLER6sum\n\n\n2018\n\n\n28\n\n\n0.99\n\n\n5.14\n\n\n4.00\n\n\n0.00\n\n\n2.00\n\n\n4.00\n\n\n7.00\n\n\n24.00\n\n\n▇▆▂▁▁\n\n\nKESSLER6sum\n\n\n2019\n\n\n11\n\n\n0.99\n\n\n5.35\n\n\n3.97\n\n\n0.00\n\n\n2.00\n\n\n5.00\n\n\n7.00\n\n\n24.00\n\n\n▇▆▂▁▁\n\n\nSWB.Kessler01\n\n\n2018\n\n\n30\n\n\n0.99\n\n\n0.74\n\n\n0.87\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n4.00\n\n\n▇▅▂▁▁\n\n\nSWB.Kessler01\n\n\n2019\n\n\n19\n\n\n0.99\n\n\n0.78\n\n\n0.87\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n4.00\n\n\n▇▆▃▁▁\n\n\nSWB.Kessler02\n\n\n2018\n\n\n34\n\n\n0.98\n\n\n0.40\n\n\n0.75\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n4.00\n\n\n▇▂▁▁▁\n\n\nSWB.Kessler02\n\n\n2019\n\n\n12\n\n\n0.99\n\n\n0.47\n\n\n0.79\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n4.00\n\n\n▇▂▁▁▁\n\n\nSWB.Kessler03\n\n\n2018\n\n\n35\n\n\n0.98\n\n\n1.22\n\n\n0.95\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n2.00\n\n\n4.00\n\n\n▆▇▆▂▁\n\n\nSWB.Kessler03\n\n\n2019\n\n\n16\n\n\n0.99\n\n\n1.22\n\n\n0.93\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n2.00\n\n\n4.00\n\n\n▅▇▆▂▁\n\n\nSWB.Kessler04\n\n\n2018\n\n\n32\n\n\n0.98\n\n\n1.19\n\n\n0.96\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n2.00\n\n\n4.00\n\n\n▅▇▅▂▁\n\n\nSWB.Kessler04\n\n\n2019\n\n\n15\n\n\n0.99\n\n\n1.27\n\n\n0.96\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n2.00\n\n\n4.00\n\n\n▅▇▆▂▁\n\n\nSWB.Kessler05\n\n\n2018\n\n\n32\n\n\n0.98\n\n\n0.42\n\n\n0.80\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n4.00\n\n\n▇▂▁▁▁\n\n\nSWB.Kessler05\n\n\n2019\n\n\n14\n\n\n0.99\n\n\n0.46\n\n\n0.81\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n4.00\n\n\n▇▂▁▁▁\n\n\nSWB.Kessler06\n\n\n2018\n\n\n37\n\n\n0.98\n\n\n1.17\n\n\n0.95\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n2.00\n\n\n4.00\n\n\n▆▇▅▂▁\n\n\nSWB.Kessler06\n\n\n2019\n\n\n13\n\n\n0.99\n\n\n1.16\n\n\n0.92\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n2.00\n\n\n4.00\n\n\n▅▇▅▁▁\n\n\nWe can use skimr to find outlier\n\n\nnz %>%\n  dplyr::group_by(Wave) %>%\n  skim(KESSLER6sum)\n\n\n\nTable 3: Data summary\n\n\n\n\n\n\nName\n\n\nPiped data\n\n\nNumber of rows\n\n\n4126\n\n\nNumber of columns\n\n\n66\n\n\n_______________________\n\n\n\n\nColumn type frequency:\n\n\n\n\nnumeric\n\n\n1\n\n\n________________________\n\n\n\n\nGroup variables\n\n\nWave\n\nVariable type: numeric\n\nskim_variable\n\n\nWave\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\nKESSLER6sum\n\n\n2018\n\n\n28\n\n\n0.99\n\n\n5.14\n\n\n4.00\n\n\n0\n\n\n2\n\n\n4\n\n\n7\n\n\n24\n\n\n▇▆▂▁▁\n\n\nKESSLER6sum\n\n\n2019\n\n\n11\n\n\n0.99\n\n\n5.35\n\n\n3.97\n\n\n0\n\n\n2\n\n\n5\n\n\n7\n\n\n24\n\n\n▇▆▂▁▁\n\n\nTry a workflow similar to what we used above\n\n\nnz%>%\n  select(date, KESSLER6sum) %>%\n  mutate(weekdays = wday(date, label = TRUE)) %>%\n  group_by(weekdays) %>%\n  skim()\n\n\n\nTable 4: Data summary\n\n\n\n\n\n\nName\n\n\nPiped data\n\n\nNumber of rows\n\n\n4126\n\n\nNumber of columns\n\n\n3\n\n\n_______________________\n\n\n\n\nColumn type frequency:\n\n\n\n\nDate\n\n\n1\n\n\nnumeric\n\n\n1\n\n\n________________________\n\n\n\n\nGroup variables\n\n\nweekdays\n\nVariable type: Date\n\nskim_variable\n\n\nweekdays\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmin\n\n\nmax\n\n\nmedian\n\n\nn_unique\n\n\ndate\n\n\nSun\n\n\n0\n\n\n1\n\n\n2018-06-24\n\n\n2020-09-27\n\n\n2019-06-02\n\n\n84\n\n\ndate\n\n\nMon\n\n\n0\n\n\n1\n\n\n2018-06-04\n\n\n2020-10-05\n\n\n2019-10-07\n\n\n101\n\n\ndate\n\n\nTue\n\n\n0\n\n\n1\n\n\n2018-01-02\n\n\n2020-10-06\n\n\n2019-10-15\n\n\n89\n\n\ndate\n\n\nWed\n\n\n0\n\n\n1\n\n\n2018-06-20\n\n\n2020-09-30\n\n\n2019-09-04\n\n\n85\n\n\ndate\n\n\nThu\n\n\n0\n\n\n1\n\n\n2018-06-21\n\n\n2020-09-03\n\n\n2019-01-24\n\n\n86\n\n\ndate\n\n\nFri\n\n\n0\n\n\n1\n\n\n2018-06-22\n\n\n2020-09-11\n\n\n2019-06-24\n\n\n87\n\n\ndate\n\n\nSat\n\n\n0\n\n\n1\n\n\n2018-06-23\n\n\n2020-10-03\n\n\n2019-07-27\n\n\n75\n\nVariable type: numeric\n\nskim_variable\n\n\nweekdays\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\nKESSLER6sum\n\n\nSun\n\n\n2\n\n\n1.00\n\n\n4.97\n\n\n3.77\n\n\n0\n\n\n2\n\n\n4\n\n\n7\n\n\n21\n\n\n▇▅▂▁▁\n\n\nKESSLER6sum\n\n\nMon\n\n\n4\n\n\n0.99\n\n\n5.25\n\n\n3.99\n\n\n0\n\n\n2\n\n\n4\n\n\n7\n\n\n24\n\n\n▇▅▂▁▁\n\n\nKESSLER6sum\n\n\nTue\n\n\n10\n\n\n0.98\n\n\n5.30\n\n\n4.08\n\n\n0\n\n\n2\n\n\n4\n\n\n7\n\n\n24\n\n\n▇▆▂▁▁\n\n\nKESSLER6sum\n\n\nWed\n\n\n5\n\n\n0.99\n\n\n5.34\n\n\n4.04\n\n\n0\n\n\n2\n\n\n5\n\n\n8\n\n\n18\n\n\n▇▇▃▂▁\n\n\nKESSLER6sum\n\n\nThu\n\n\n9\n\n\n0.99\n\n\n5.30\n\n\n4.08\n\n\n0\n\n\n2\n\n\n5\n\n\n8\n\n\n24\n\n\n▇▆▂▁▁\n\n\nKESSLER6sum\n\n\nFri\n\n\n6\n\n\n0.99\n\n\n5.25\n\n\n4.00\n\n\n0\n\n\n2\n\n\n5\n\n\n7\n\n\n23\n\n\n▇▆▂▁▁\n\n\nKESSLER6sum\n\n\nSat\n\n\n3\n\n\n0.99\n\n\n5.24\n\n\n3.86\n\n\n0\n\n\n3\n\n\n4\n\n\n7\n\n\n24\n\n\n▇▆▂▁▁\n\n\nWhat is the variation in my indicators?\nWhich values are most common (and most rare?)\nshortcuts\n\n\nlibrary(\"report\")\nlibrary(\"dplyr\")\nhead(nz)\n\n\n# A tibble: 6 x 66\n     Id  Wave years   Age Male  Gender   Edu Partner BornNZ BigDoms\n  <dbl> <dbl> <dbl> <dbl> <fct>  <dbl> <dbl>   <dbl>  <dbl> <fct>  \n1     1  2019 10.4   48.0 Male       1     3       1      1 Not_Rel\n2     1  2018  9.47  47   Male       1     3       1      1 Not_Rel\n3     2  2019 10.6   48.2 Male       1     7       1      1 Not_Rel\n4     2  2018  9.90  47   Male       1     7       1      1 Not_Rel\n5     3  2019 10.2   54.5 Male       1     4       0      1 Not_Rel\n6     3  2018  9.20  53   Male       1     4       0      1 Not_Rel\n# … with 56 more variables: TSCORE <dbl>, GenCohort <fct>,\n#   Religion.Church <dbl>, Religion.Believe.Cats <dbl>, Relid <dbl>,\n#   HLTH.Fatigue <dbl>, HLTH.SleepHours <dbl>, HomeOwner <dbl>,\n#   Pol.Orient <dbl>, PATRIOT <dbl>, Env.SatNZEnvironment <dbl>,\n#   Env.MotorwaySpend <dbl>, Env.PubTransSubs <dbl>,\n#   Env.ClimateChgConcern <dbl>, LIFEMEANING <dbl>,\n#   Hours.Internet <dbl>, Issue.GovtSurveillance <dbl>,\n#   Issue.RegulateAI <dbl>, Issue.IncomeRedistribution <dbl>,\n#   Hours.Exercise <dbl>, Hours.Work <dbl>, Hours.News <dbl>,\n#   CONSCIENTIOUSNESS <dbl>, EXTRAVERSION <dbl>, AGREEABLENESS <dbl>,\n#   OPENNESS <dbl>, Religious <fct>, Spiritual.Identification <dbl>,\n#   Believe.God <fct>, Believe.Spirit <fct>, HoursCharity <dbl>,\n#   CharityDonate <dbl>, Your.Personal.Relationships <dbl>,\n#   Your.Future.Security <dbl>, Standard.Living <dbl>,\n#   NZ.Economic.Situation <dbl>, NZ.Social.Conditions <dbl>,\n#   NZ.Business.Conditions <dbl>, Emp.JobSecure <dbl>,\n#   Issue.Food.GMO <dbl>, Env.SacMade <lgl>, KESSLER6sum <dbl>,\n#   SWB.Kessler01 <dbl>, SWB.Kessler02 <dbl>, SWB.Kessler03 <dbl>,\n#   SWB.Kessler04 <dbl>, SWB.Kessler05 <dbl>, SWB.Kessler06 <dbl>,\n#   FeelHopeless <fct>, FeelDepressed <fct>, FeelRestless <fct>,\n#   EverythingIsEffort <fct>, FeelWorthless <fct>, FeelNervous <fct>,\n#   k6cats <fct>, date <date>\n\nnz %>%\n  select(starts_with(\"KESSLER6sum\")) %>% \n  group_by(\"Wave\") %>% \n  report() %>% \n  summary()\n\n\nThe data contains 4126 observations, grouped by \"Wave\", of the following variables:\n- Wave (n = 4126):\n  - KESSLER6sum: Mean = 5.24, SD = 3.98, range = [0, 24], 39 missing\n\n\n\n\n",
    "preview": "posts/4_1/op.png",
    "last_modified": "2021-03-14T14:37:43+13:00",
    "input_file": "lecture_4.utf8.md",
    "preview_width": 10241,
    "preview_height": 8450
  },
  {
    "path": "posts/3_1/",
    "title": "Visualisation",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\n\nContents\nData visualisation with ggplot2\nIntroduction\nCreating a graph\nUsing ggplot 2 to highlight elements of interest.\nFacets\nUnderstanding your data through graphs\nTransforming data\nRevisiting logical operators\nCommand filter: keeps rows matching criteria\nTask\nCommand select: picks columns by column name\nCommand arrange reorders rows\nCommand mutate add new variable name\nCommand summarise reduce variables to values\nMultiple pipe operators\nOther functions\n\n\n\nData visualisation with ggplot2\n\n\n\nIntroduction\nIn this lecture we’ll first introduce you to the ggplot2 package, and vocabulary, for creating graphs in R. We’ll mostly follow the approach described in the book “R for data science,” which can be found here.\nWe’ll then turn to data-wrangling using the dplyr package.\nBoth ggplot2 and dplyr can be found in library(tidyverse)\nCreating a graph\nStep 1, load tidyverse:\n\n\nlibrary(\"tidyverse\")\n\n\n\nStep 2, Make sure your dataset is loaded. We’ll start with the mpg dataset\n\n\n#inspect the mpg dataset\nhead(mpg)\n\n\n# A tibble: 6 x 11\n  manufacturer model displ  year   cyl trans   drv     cty   hwy fl   \n  <chr>        <chr> <dbl> <int> <int> <chr>   <chr> <int> <int> <chr>\n1 audi         a4      1.8  1999     4 auto(l… f        18    29 p    \n2 audi         a4      1.8  1999     4 manual… f        21    29 p    \n3 audi         a4      2    2008     4 manual… f        20    31 p    \n4 audi         a4      2    2008     4 auto(a… f        21    30 p    \n5 audi         a4      2.8  1999     6 auto(l… f        16    26 p    \n6 audi         a4      2.8  1999     6 manual… f        18    26 p    \n# … with 1 more variable: class <chr>\n\nStep 3. Inspect the\"Negative relationship between highway fuel efficiency and a cars engine size (which is given by the variable displ).\n\n\n# Create graph\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Negative relationship between engine displacement and fuel efficiency.\")\n\n\n\n\nA basic problem with this graph is that we don’t know what it is representing. To avoid this problem, it is useful to get into the habit of adding titles to your graphs, and also of using informative axis labels. We do this by adding additional layers.\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title = \"Negative relationship between engine displacement and fuel efficiency.\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\")\n\n\n\n\nLet’s walk through the logic of the ggplot2 “grammar”:\nFirst we call the data\n\n\n# here we are calling up the data\nggplot(data = mpg)\n\n\n\nNext, we add a layer of points, by calling the relevant columns and rows of this dataset\n\n\n# Here, we add a layer of points, by calling the relevant columns and rows of this dataset\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\nThen we add the title\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title =  \"Negative relationship between engine displacement and fuel efficiency.\")\n\n\n\n\nThen we add the labels\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\")   + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\nWe can change the axis starting positions:\n\n\n# Create graph and add title\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\")   + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") + expand_limits(x = 0, y = 0)\n\n\n\n\nThe generic method for adding layers is as follows:\n\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\nUsing ggplot 2 to highlight elements of interest.\nHere we can use the “color =” option.1\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) \n\n\n\n\nHere’s a shape command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape = class)) \n\n\n\n\nHere’s a size command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, size = cty)) \n\n\n\n\nHere’s the fill command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, fill = cty)) \n\n\n\n\nHere’s the alpha command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, alpha  = .1)) \n\n\n\n\nHere’s the alpha command combined with the fill command\n\n\n# Which cases interest you in this graph?\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, alpha = cty, size = cty)) \n\n\n\n\nFacets\nWe can create multiple graphs using facets\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) + \n   facet_wrap(~ class, nrow = 2)\n\n\n\n\nWe use facet_grid for graphing the Negative relationship between two variables.\nNote the difference betwen these two graphs:\nHere the focus is on the negative relationship between class and the x variable, displacement\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_grid(class ~ .)  + theme(legend.position = \"none\") \n\n\n\n\nHere the focus is on the relationship betwen class and the y variable, highway milage.\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_grid(. ~ class) + theme(legend.position = \"none\") \n\n\n\n\nWe can focus on Negative relationship between class and the x and y variables simultaneously. Here we add the ’year` indicator and we do not see much of an improvement in highway milage for the different classes, adjusting for displacement:\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +\n  facet_grid(class ~ year) + theme(legend.position = \"bottom\") +\n  labs(title =\"Negative relationship between engine displacement and fuel efficiency by class.\") + \n  xlab(\"Engine displacement in (units)\") + \n  ylab(\"Highway miles per liter\") \n\n\n\n\nUnderstanding your data through graphs\nWe can create a graph of relationships:\n\n\n# set better theme\ntheme_set(theme_classic())\nggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy)) + \n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\") +\n  xlab(\"Engine displacement in (units)\") +\n  ylab(\"Highway miles per liter\") \n\n\n\n\nAdd points as a layer\n\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  geom_smooth(mapping = aes(x = displ, y = hwy)) +\n  theme(legend.position = \"bottom\") +\n  labs(title =\"Negative relationship between engine displacement and fuel efficiency.\") +\n  xlab(\"Engine displacement in (units)\") +\n  ylab(\"Highway miles per liter\") \n\n\n\n\nWe can write this more compactly, by including the mapping with the data layer\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\nThen we can include mappings for specific layers\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n\n\n\n\nWe can add a grouping factor e.g. for “drv”, thus creating multiple lines\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, group = drv)) + \n  geom_point(aes(color = class)) + \n  geom_smooth()\n\n\n\n\nWe can replace the smooths with linear models\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, linetype = drv)) + \n  geom_point(aes(color = class)) + \n  geom_smooth(method = \"lm\")\n\n\n\n\nTransforming data\nFirst we’ll get the flights data\n\n\nlibrary(nycflights13)\nhead(flights)\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\nNext we’ll create some data frames to help us illustrate points\n\n\ndf <- data.frame(\ncolour = c(\"blue\", \"black\", \"blue\", \"blue\", \"black\"), value = 1:5)\nhead(df)\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n5  black     5\n\nRevisiting logical operators\nRecall our logical operators. These will be essential for data wrangling\n\n\nknitr::include_graphics(\"logic.png\")\n\n\n\n\nCommand filter: keeps rows matching criteria\nKeep only blue rows:\n\n\ndf%>%\nfilter(colour == \"blue\")\n\n\n  colour value\n1   blue     1\n2   blue     3\n3   blue     4\n\nKeep only values 1 and 4\n\n\ndf%>%\n  filter (value %in% c(1,4))\n\n\n  colour value\n1   blue     1\n2   blue     4\n\nKeep values 1 through 4\n\n\ndf %>%\n  filter (value %in% c(1:4))\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n\nAnother way to do the same\n\n\ndf %>%\n  filter (value != 5)\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n\nTask\nHow can we find all flights that left in January?\n\n\nhead(flights)\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\n\nflights%>%\n  dplyr::filter(month ==1)\n\n\n# A tibble: 27,004 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 26,994 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nFlights delayed by more than 15 mintutes that arrived on time\n\n\nflights%>%\n  dplyr::filter (dep_delay >15 & arr_delay <=0)\n\n\n# A tibble: 4,314 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1     1025            951        34     1258\n 2  2013     1     1     1033           1017        16     1130\n 3  2013     1     1     2052           2029        23     2349\n 4  2013     1     1     2107           2040        27     2354\n 5  2013     1     2      727            645        42     1024\n 6  2013     1     2     1004            945        19     1251\n 7  2013     1     2     1031           1015        16     1135\n 8  2013     1     2     1500           1430        30     1741\n 9  2013     1     2     1737           1720        17     1908\n10  2013     1     2     1831           1815        16     2130\n# … with 4,304 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nCommand select: picks columns by column name\nSelect the colour column\n\n\ndf%>%\n  dplyr::select ( colour )\n\n\n  colour\n1   blue\n2  black\n3   blue\n4   blue\n5  black\n\nAnother way?\n\n\ndf%>%\n  dplyr::select ( !value )\n\n\n  colour\n1   blue\n2  black\n3   blue\n4   blue\n5  black\n\nor\n\n\ndf%>%\n  dplyr::select ( -c(value ))\n\n\n  colour\n1   blue\n2  black\n3   blue\n4   blue\n5  black\n\nCommand arrange reorders rows\n\n\ndf %>%\n  arrange(value)\n\n\n  colour value\n1   blue     1\n2  black     2\n3   blue     3\n4   blue     4\n5  black     5\n\n\n\ndf %>%\n  arrange(desc(value))\n\n\n  colour value\n1  black     5\n2   blue     4\n3   blue     3\n4  black     2\n5   blue     1\n\nTask: how would we order flights by departure data and time ?\n\n\nflights %>%\n  arrange(month, day, dep_time)\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nTask which flights have the greated difference between departure delay and arrival delay?\n\n\nflights%>%\n  arrange(desc(dep_delay - arr_delay))\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     6    13     1907           1512       235     2134\n 2  2013     2    26     1000            900        60     1513\n 3  2013     2    23     1226            900       206     1746\n 4  2013     5    13     1917           1900        17     2149\n 5  2013     2    27      924            900        24     1448\n 6  2013     7    14     1917           1829        48     2109\n 7  2013     7    17     2004           1930        34     2224\n 8  2013    12    27     1719           1648        31     1956\n 9  2013     5     2     1947           1949        -2     2209\n10  2013    11    13     2024           2015         9     2251\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nNot this could be written briefly as this:\n\n\narrange(flights, desc(dep_delay - arr_delay))\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     6    13     1907           1512       235     2134\n 2  2013     2    26     1000            900        60     1513\n 3  2013     2    23     1226            900       206     1746\n 4  2013     5    13     1917           1900        17     2149\n 5  2013     2    27      924            900        24     1448\n 6  2013     7    14     1917           1829        48     2109\n 7  2013     7    17     2004           1930        34     2224\n 8  2013    12    27     1719           1648        31     1956\n 9  2013     5     2     1947           1949        -2     2209\n10  2013    11    13     2024           2015         9     2251\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nCommand mutate add new variable name\n\n\ndf %>%\n  mutate(double_value = 2 * value)\n\n\n  colour value double_value\n1   blue     1            2\n2  black     2            4\n3   blue     3            6\n4   blue     4            8\n5  black     5           10\n\nOrder flights by greatest difference between departure delay and arrival delay?\n\n\nflights %>%\n  mutate(diff_dep_arr = dep_delay - arr_delay)%>%\n  select(flight,diff_dep_arr)%>%\n  arrange(desc(diff_dep_arr))\n\n\n# A tibble: 336,776 x 2\n   flight diff_dep_arr\n    <int>        <dbl>\n 1   4377          109\n 2     51           87\n 3     51           80\n 4   1465           79\n 5     51           76\n 6    673           74\n 7   1532           74\n 8   1284           73\n 9    612           73\n10    427           72\n# … with 336,766 more rows\n\nCommand summarise reduce variables to values\nSum all values in the df dataset\n\n\ndf %>%\n  summarise (total = sum(value))\n\n\n  total\n1    15\n\nSummaries the values by colour groups, and give the number of items per colour group\n\n\ndf %>%\n  group_by(colour) %>%\n  summarise(total = sum(value),\n            n = n())\n\n\n# A tibble: 2 x 3\n  colour total     n\n* <chr>  <int> <int>\n1 black      7     2\n2 blue       8     3\n\nUseful summary functions are:\nmin(x)\nmax(x)\nmean(x)\nn\nn_distinct\nsum(x)\nsum(x > 10)\nmean(x > 10)\nsd(x)\nvar(x)\nTask, how many flights flew on Christmas?\n\n\nhead(flights)\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\n\nflights %>%\n  filter( month == 12, day == 25)%>%\n  summarise (n = n())\n\n\n# A tibble: 1 x 1\n      n\n  <int>\n1   719\n\nCalculate average delay:\n\n\nflights %>%\n  summarise(delay = mean(dep_delay, na.rm = TRUE))\n\n\n# A tibble: 1 x 1\n  delay\n  <dbl>\n1  12.6\n\n\n\nsummarise(flights, delay = mean(dep_delay, na.rm = TRUE))\n\n\n# A tibble: 1 x 1\n  delay\n  <dbl>\n1  12.6\n\nMultiple pipe operators\nHere we:\nGroup flights by destination.\nSummarise to compute distance, average delay, and number of flights.\nRemove Honolulu airport, because it is so far away\n\n\ndelays <- flights %>% \n  group_by(dest) %>% \n  summarise(\n    count = n(),\n    dist = mean(distance, na.rm = TRUE),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) %>% \n  filter(dest != \"HNL\")\nhead(delays)\n\n\n# A tibble: 6 x 4\n  dest  count  dist delay\n  <chr> <int> <dbl> <dbl>\n1 ABQ     254 1826   4.38\n2 ACK     265  199   4.85\n3 ALB     439  143  14.4 \n4 ANC       8 3370  -2.5 \n5 ATL   17215  757. 11.3 \n6 AUS    2439 1514.  6.02\n\n\n\nflights %>% \n  filter(!is.na(dep_delay), !is.na(arr_delay)) %>% # not cancelled\n   group_by(tailnum) %>% # group by unique aircraft\n  summarise(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  ) %>%\n  ggplot(mapping = aes(x = n, y = delay)) + \n  geom_point(alpha = 1/10)  + \n  labs(title = \"Variation in average delay by tailnumber \") \n\n\n\n\nOther functions\nSuppose you only wanted to keep your mutated variables, in this case you can use transmute\n\n\nnew_flights <-transmute(flights,\n  gain = dep_delay - arr_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n)\nhead(new_flights)\n\n\n# A tibble: 6 x 3\n   gain hours gain_per_hour\n  <dbl> <dbl>         <dbl>\n1    -9  3.78         -2.38\n2   -16  3.78         -4.23\n3   -31  2.67        -11.6 \n4    17  3.05          5.57\n5    19  1.93          9.83\n6   -16  2.5          -6.4 \n\nTo learn more, go to https://dplyr.tidyverse.org/\n\nRemoving the axis and labels here just to keep the code compact↩︎\n",
    "preview": "posts/3_1/op.jpg",
    "last_modified": "2021-03-09T20:02:57+13:00",
    "input_file": {}
  },
  {
    "path": "posts/5_1/",
    "title": "Samples, paramaters, and elements of a linear model",
    "description": "\"What is a statistical model?\"",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\n\nContents\n\n\nSamples and parameters: observation and inference\n\n\n\nLinear relationships with one co-variate\nNon-linear relationships with one co-variate\n\n\n\n",
    "preview": "posts/5_1/distill-preview.png",
    "last_modified": "2021-02-23T15:04:48+13:00",
    "input_file": {},
    "preview_width": 2500,
    "preview_height": 2500
  },
  {
    "path": "posts/2_1/",
    "title": "Coding basics",
    "description": {},
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-03-02",
    "categories": [],
    "contents": "\n\nContents\nTo do:\nFundamentals of R\nHow to use R as calculator\nInspecting data\ncheck head of dataset sing head\ncheck data types using str\nnames\nview rownames\ntable (and use of $)\nchange column names\n\nBasic data structures\nvectors of numbers ( and use of c and i:x )\nvectors of characters\ndataframes (2 dimensional square arrays of vectors)\nrename columns of a data frame\nmatrix\nlists\n\nClasses in R\nnumeric and integer\nfactors\n\nIndexing in R\ncolumns\nrows\nrows and columns\nselection by negation\n\nBasic data wrangling in R\nuse of $ and [i:x]\n\nIndexing for logical operations\ndefinitions\nevaluation using logical operators\n\nThe basic structure of R commands\nmean\nsd\nsummary\nCoding\nInstalling package\n\nRolling your own code\nCoding etiquette\n\nusing R!\ndata summary\nmodel\nresults\ngraph predicted effects\nWhat is the advantage of this graph?\ntry another model\n\nto be continued\n\nTo do:\nA good, and free introduction to R for Data Science Read chapters: 2-8 (they are short chapters.)\nFundamentals of R\nA console runs all your code in R\nA source page runs all your code in R.\nA working directory is where R will look for raw data and other material.\nIn R-studio, (for starters) you can can use the file tab to import and save your material.\nSimilarly, in R-studio, (for starters) you can can use install packages by clicking the Install tab in the package pane (generally lower right pane) to install packagees.\nHow to use R as calculator\nWe can use R as a calculator. You can run any mathematical operation you would normally use by entering it into the console:\n\n\n## Addition\nx <- 3 + 2\nx\n\n\n[1] 5\n\n## Subtraction\nx <-  3 - 2\nx\n\n\n[1] 1\n\n## Multiplication\nx <-  3 * 2\nx\n\n\n[1] 6\n\n## Division\nx <-  3 / 2\nx\n\n\n[1] 1.5\n\n## Modulus (Remainder from division)\nx <-  3 %% 2\nx\n\n\n[1] 1\n\n## Exponentiation\nx = 3 ^ 2\nx\n\n\n[1] 9\n\n## Integer Division (Number of times denominator fits in numerator)\nx = 3 %/% 2\nx\n\n\n[1] 1\n\nInspecting data\nIn a moment, we’ll teach you how to import data into R. For now, let’s work with a dataset that is already present in your R environment, the iris dataset.\nHere are some useful commands for inspecting data\ncheck head of dataset sing head\n\n\n# the top rows and columns of the dataset\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ncheck data types using str\n\n\nstr(iris)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nnames\n\n\n#names of the columns\nnames(iris)\n\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n[5] \"Species\"     \n\nview rownames\n\n\n# view rownames\nrownames(iris)\n\n\n  [1] \"1\"   \"2\"   \"3\"   \"4\"   \"5\"   \"6\"   \"7\"   \"8\"   \"9\"   \"10\" \n [11] \"11\"  \"12\"  \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\"  \"19\"  \"20\" \n [21] \"21\"  \"22\"  \"23\"  \"24\"  \"25\"  \"26\"  \"27\"  \"28\"  \"29\"  \"30\" \n [31] \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\"  \"37\"  \"38\"  \"39\"  \"40\" \n [41] \"41\"  \"42\"  \"43\"  \"44\"  \"45\"  \"46\"  \"47\"  \"48\"  \"49\"  \"50\" \n [51] \"51\"  \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"60\" \n [61] \"61\"  \"62\"  \"63\"  \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"70\" \n [71] \"71\"  \"72\"  \"73\"  \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"80\" \n [81] \"81\"  \"82\"  \"83\"  \"84\"  \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"90\" \n [91] \"91\"  \"92\"  \"93\"  \"94\"  \"95\"  \"96\"  \"97\"  \"98\"  \"99\"  \"100\"\n[101] \"101\" \"102\" \"103\" \"104\" \"105\" \"106\" \"107\" \"108\" \"109\" \"110\"\n[111] \"111\" \"112\" \"113\" \"114\" \"115\" \"116\" \"117\" \"118\" \"119\" \"120\"\n[121] \"121\" \"122\" \"123\" \"124\" \"125\" \"126\" \"127\" \"128\" \"129\" \"130\"\n[131] \"131\" \"132\" \"133\" \"134\" \"135\" \"136\" \"137\" \"138\" \"139\" \"140\"\n[141] \"141\" \"142\" \"143\" \"144\" \"145\" \"146\" \"147\" \"148\" \"149\" \"150\"\n\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntable (and use of $)\n\n\n# create a table\ntable(iris$Species)\n\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\nchange column names\n\n\n# change column names\n# create new dataset for \nirisdat2 <- iris\n# chage names of columns\nnames(irisdat2)[] <- c(\"s_l\", \"s_w\", \"p_l\", \"p_w\", \"sp\")\n#inspect\nhead(irisdat2)\n\n\n  s_l s_w p_l p_w     sp\n1 5.1 3.5 1.4 0.2 setosa\n2 4.9 3.0 1.4 0.2 setosa\n3 4.7 3.2 1.3 0.2 setosa\n4 4.6 3.1 1.5 0.2 setosa\n5 5.0 3.6 1.4 0.2 setosa\n6 5.4 3.9 1.7 0.4 setosa\n\nBasic data structures\nvectors of numbers ( and use of c and i:x )\n\n\ngo_vector <- c(1:5)\ngo_vector\n\n\n[1] 1 2 3 4 5\n\nvectors of characters\n\n\ngo_vector2 <- c(\"hello\", \"world\")\ngo_vector2\n\n\n[1] \"hello\" \"world\"\n\n\n\nas.vector(irisdat2$s_l)\n\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7\n [17] 5.4 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4\n [33] 5.2 5.5 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6\n [49] 5.3 5.0 7.0 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1\n [65] 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7\n [81] 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7\n [97] 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4\n[113] 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1\n[129] 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8\n[145] 6.7 6.7 6.3 6.5 6.2 5.9\n\ndataframes (2 dimensional square arrays of vectors)\n2 x dimensional “square” array with equal column and row lengths. Can contain data with multiple formats characters, facotors, integers, etc.\n\n\nyuk <- data.frame(c(\"the\", \"enumeration\", \"of\", \"the\", \"constitution\"), 6:10)\nyuk\n\n\n  c..the....enumeration....of....the....constitution.. X6.10\n1                                                  the     6\n2                                          enumeration     7\n3                                                   of     8\n4                                                  the     9\n5                                         constitution    10\n\nrename columns of a data frame\n\n\nnames(yuk)[] <- c(\"short\", \"best\")\nyuk\n\n\n         short best\n1          the    6\n2  enumeration    7\n3           of    8\n4          the    9\n5 constitution   10\n\nmatrix\nSame as a dataframe but can only contain one format (e.g. numbers or characters)\n\n\nyok <- as.matrix(yuk)\nyok\n\n\n     short          best\n[1,] \"the\"          \" 6\"\n[2,] \"enumeration\"  \" 7\"\n[3,] \"of\"           \" 8\"\n[4,] \"the\"          \" 9\"\n[5,] \"constitution\" \"10\"\n\nlists\nArrays with constraints on “squareness” or data types.\n\n\nlok <- list(yok, yuk)\nlok\n\n\n[[1]]\n     short          best\n[1,] \"the\"          \" 6\"\n[2,] \"enumeration\"  \" 7\"\n[3,] \"of\"           \" 8\"\n[4,] \"the\"          \" 9\"\n[5,] \"constitution\" \"10\"\n\n[[2]]\n         short best\n1          the    6\n2  enumeration    7\n3           of    8\n4          the    9\n5 constitution   10\n\nClasses in R\nnumeric and integer\nnumeric means number\n\n\nis.numeric(4.2)\n\n\n[1] TRUE\n\ninteger means a number that is not a fraction\n\n\nis.integer(4.2)\n\n\n[1] FALSE\n\nNote the default here:\n\n\nis.integer(4)\n\n\n[1] FALSE\n\n\n\nis.integer(as.integer(4))\n\n\n[1] TRUE\n\nWe’ll need to ensure that certain numbers are integers later on, when we are estimating poisson models and/or doing bayesian data analysis.\ncharacters\nCharacters are strings:\n\n\n# this is a character\nis.character(\"chapeau\")\n\n\n[1] TRUE\n\n# this is not\nis.character(4)\n\n\n[1] FALSE\n\nfactors\nA factor is a category. It can be ordered (e.g. an ordinal scale) or unordered (say a participant in a study, or a wave in a longitidunal study)\n\n\nstr(iris)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nIt’s really important to check that ordered categories are really ordered categories in your dataset.\nThere was is a famous retraction recently where a group found that religion was associated with immorality, however the countries in the the study had been coded as numbers, not as factors. The study’s effect entirely disappeared once this error was corrected!\nIndexing in R\ncolumns\n\n\n# select second column of \"yuk\"\nyuk[, 2]\n\n\n[1]  6  7  8  9 10\n\nrows\n\n\n# select second row of yuk\nyuk[2, ]\n\n\n        short best\n2 enumeration    7\n\nrows and columns\n\n\n#select first row and first column of yuk\nyuk[1, 2]\n\n\n[1] 6\n\nselection by negation\n\n\n# negate the first column of yuk\nyuk[, -1]\n\n\n[1]  6  7  8  9 10\n\n\n\n# negate the second column of yuk\nyuk[,-2]\n\n\n[1] \"the\"          \"enumeration\"  \"of\"           \"the\"         \n[5] \"constitution\"\n\nBasic data wrangling in R\nc\n\n\n# select only the first and second cols of iris\niris_short2 <- iris[ ,c( 1, 2 ) ]\nhead(iris_short2)\n\n\n  Sepal.Length Sepal.Width\n1          5.1         3.5\n2          4.9         3.0\n3          4.7         3.2\n4          4.6         3.1\n5          5.0         3.6\n6          5.4         3.9\n\n-c\n\n\n# select all columns but the first and second of iris\niris_short <- iris[ ,-c( 1, 2 ) ]\nhead(iris_short)\n\n\n  Petal.Length Petal.Width Species\n1          1.4         0.2  setosa\n2          1.4         0.2  setosa\n3          1.3         0.2  setosa\n4          1.5         0.2  setosa\n5          1.4         0.2  setosa\n6          1.7         0.4  setosa\n\ncbind\n\n\n# for use with dataframes and matrices -- note that srings a are c\nyokyuk<-cbind(yok,yuk)\nyokyuk\n\n\n         short best        short best\n1          the    6          the    6\n2  enumeration    7  enumeration    7\n3           of    8           of    8\n4          the    9          the    9\n5 constitution   10 constitution   10\n\nstr(yokyuk)\n\n\n'data.frame':   5 obs. of  4 variables:\n $ short: chr  \"the\" \"enumeration\" \"of\" \"the\" ...\n $ best : chr  \" 6\" \" 7\" \" 8\" \" 9\" ...\n $ short: chr  \"the\" \"enumeration\" \"of\" \"the\" ...\n $ best : int  6 7 8 9 10\n\nrbind\n\n\nrbind(yuk[,],yok[2:3])\n\n\n         short best\n1          the    6\n2  enumeration    7\n3           of    8\n4          the    9\n5 constitution   10\n6  enumeration   of\n\nuse of $ and []\n\n\n#select the fifth row of the column\niris_short$Petal.Width[5]\n\n\n[1] 0.2\n\nuse of $ and [i:x]\n\n\n#select the 5th-25th row of the column\niris_short$Petal.Width[5:25]\n\n\n [1] 0.2 0.4 0.3 0.2 0.2 0.1 0.2 0.2 0.1 0.1 0.2 0.4 0.4 0.3 0.3 0.3\n[17] 0.2 0.4 0.2 0.5 0.2\n\nIndexing for logical operations\ndefinitions\n== means “equals to”\n!= means “not equals to”\n> means “greater than”\n< means “less than”\n>=means “greater than or equal”\n<= means “less than or equal”\n! means “not”\n& means “and”\n| means “or”!\nis.na means “is missing” (missing values are coded in R as NA)\n> -9999 == 666 > TRUE !!! :)\nevaluation using logical operators\ncreate dataframe\n\n\n# create data frame\ndf<-data.frame( x = c(1:10),y = c(11:20) )\n\n\n\nevaluate cases\n\n\n#evaluate cases in y that greater  than 15\ndf[,\"y\"] > 15\n\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nsum instances\n\n\n# count these cases\nsum(df[,\"y\"] > 15)\n\n\n[1] 5\n\nsum instances with a different operator\n\n\n# count cases greater than or equal to 15\nsum(df[,\"y\"] >= 15)\n\n\n[1] 6\n\nanother methods\n\n\n# another approach\nsum(df$y >= 15)\n\n\n[1] 6\n\nusing the or function\n\n\n# combine operators\nsum(df$y >= 15 | df$y <=11)\n\n\n[1] 7\n\ngo meta\n\n\n# go \"meta\"\nsum(df$y >= 15 | df$y <= 11) == sum(df$x >=5 | df$x <=1 )\n\n\n[1] TRUE\n\ngo meta-meta!\n\n\n# go meta-meta\nsum(sum(df$y >= 15 | df$y <= 11) == sum(df$x >=5 | df$x <=1 ))\n\n\n[1] 1\n\nuse operators to modify data\n\n\n# using assignment to modify data\ndf$x[df$x >=5 ] <- NA\ndf\n\n\n    x  y\n1   1 11\n2   2 12\n3   3 13\n4   4 14\n5  NA 15\n6  NA 16\n7  NA 17\n8  NA 18\n9  NA 19\n10 NA 20\n\nusing is.na and !is.na\n\n\nsum(is.na(df$x))\n\n\n[1] 6\n\n\n\nsum(!is.na(df$x))\n\n\n[1] 4\n\n\n\nsum(is.na(df$x)) + sum(!is.na(df$x)) \n\n\n[1] 10\n\nThe basic structure of R commands\nThe four main elements of every R code are:\nobjects,\nfunctions,\narguments\noperators.\nFigure 1 provides a simple example, that produces a new object which contains the mean of variable x.\n\n\n\nFigure 1: The Basic Syntax of R\n\n\n\nmean\nThe function mean generates the arithmetic mean of an input object:\n\n\n# a function to assess the mean of a Sepal.Length\nmean(iris$Sepal.Length)\n\n\n[1] 5.843333\n\nsd\nThe function sd gives us the standard deviation:\n\n\n# standard deviation of Sepal.Length\nsd(iris$Sepal.Length)\n\n\n[1] 0.8280661\n\nsummary\n\n\n# summary of the \"Sepal Length\" column\nsummary(iris$Sepal.Length)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.300   5.100   5.800   5.843   6.400   7.900 \n\n\n\n# summary of the Iris data set\nsummary(iris)\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\nCoding\nThe object needs to be specified inside of the function brackets as the x argument, in this case we define x = df .\nLast, we assign the result of this function for later us via the <- operator to an object which we decided to call object.\nIn other words, we create a new object that can be further manipulated and contains information about the mean of a previously created object “x”. This structure represents the foundation of most operations in R.\nInstead calling the mean function as above one could manually add all values of x dividing it by the number of x values.\nNevertheless, this would be very cumbersome.\nFor this reason, functions (pre-assembled lines of code) exist to reduce the amount of coding necessary.\nThese functions can be bundled into packages. R’s capacity for creating packages is main appeal of R as a statistical tool because community developed functions are available from a central repository called CRAN in the form of packages.\nThese packages can be installed in R with the command install.packages(“package name”).\nIt is important that you only need to install a package once on your machine, expect if you want to upgrade the package. Generally speaking you regularly want to upgrade your packages, but keep a permanent note in your code which version of a package you used when it was initially written.\nInstalling package\ninstall these packages (we’ll be using them later)\n\n\ninstall.packages(\"devtools\") # installing packages\n\n\n\n\n\ninstall.packages(\"remotes\") # installing packages\n\n\n\n\n\ninstall.packages(\"tidyverse\") ## data wrangling and visualisation\n\n\n\n\n\ninstall.packages(\"lme4\") # multilevel modelling\n\n\n\n\n\ninstall.packages(\"patchwork\") # arranging multiple graphs\n\n\n\n\n\nlibrary(devtools)\ndevtools::install_github(\"strengejacke/sjPlot\") # plots and tables\n\n\n\n\n\ninstall.packages(\"papaja\")  # writing APA documents\n\n\n\n\n\ninstall.packages(\"table1\") # summary tables\n\n\n\nextra credit\n\n\ndevtools::install_github(\"easystats/easystats\")\n\n\n\n\n\ndevtools::install_github(\"strengejacke/ggeffects\")\n\n\n\nsuper extra credit\n\n\nif (!requireNamespace(\"remotes\")) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"paul-buerkner/brms\")\n\n\n\n\n\ndevtools::install_github(\"stan-dev/cmdstanr\")\n\n\n\nRolling your own code\nLet’s use R to write a function. Recall that a factorial for a number \\(n\\) is the product of all positive inters less than or equal to \\(n\\). Thus the factorial for 5 = \\[1 \\times 2 \\times 3 \\times 4 \\times 5\\]\nIn R we can write a function:\n\n\n# create a function to perform the factorial operation \ngo_factorial <- function(x) {\n  y <- 1\n  for (i in 1:x) {\n    y <- y * ((1:x)[i])\n  }\n  print(y)\n}\n\n\n\nLet’s try it out\n\n\n# test of the `go_factorial` function\ngo_factorial(5)\n\n\n[1] 120\n\nLet’s see if this is the number that R’s factorial function produces:\n\n\n# R's native factorial function\nfactorial(5)\n\n\n[1] 120\n\nWe can use R’s == relational operator to evaluate whether the two functions are the same\n\n\n# are the two functions equivalent for factorial five\ngo_factorial(5) == factorial(5)\n\n\n[1] 120\n[1] TRUE\n\nFor more information about relational operators type the following into your console:\n\n\n?`==`\n\n\n\nWe can make more complicated functions:\n\n\n# function for factorial that throws warnings when the data that are entered are not appropriate. \ngo_bayes_factorial <- function (x) {\n  # check is the number is negative, positive or zero\n  if (x  < 0) {\n    print(\"not even Ashley Bloomfield could make a factorial for a negative number\")\n  } else if (x == 0) {\n    print(\"the factorial of zero is defined as 1\")\n  } else {\n    for (i in 1:x)\n      y <- 1\n    for (i in 1:x) {\n      y <- y * ((1:x)[i])\n    }\n    print(y)\n  }\n}\n\n\n\nWe’ll come back to functions later. It’s useful to look at an example of a function so that you can see that R is much more than a calcultor. It is a tool to empower you for doing data anlysis in new and creative ways.\nCoding etiquette\nKeep your code legible and annotate\nWhy is this bad code?\n\n\ndf1<-data.frame(a=rnorm(10,1,1),b=rnorm(10,4,8),c=rnorm(10,8,1),d=rnorm(10,7,2))\n\n\n\nWhy is this better code?\n\n\n# Create a data frame with four columns of randomly generated numbers specifying different means and standard deviations \ndf1 <- data.frame(\n  a = rnorm( 10, mean = 1, sd = 1 ),\n  b = rnorm( 10, mean = 4, sd = 8 ),\n  c = rnorm( 10, mean = 8, sd = 1 ),\n  d = rnorm( 10, mean = 7, sd = 2 )\n)\n\n\n\nusing R!\ndata summary\n\n\n# basic summary\nsummary(iris)\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\n\ntable1::table1(~ Sepal.Length   + Sepal.Width   + Petal.Length  + Petal.Width |Species, data = iris  )\n\n\n\nsetosa(N=50)\nversicolor(N=50)\nvirginica(N=50)\nOverall(N=150)\nSepal.Length\n\n\n\n\nMean (SD)\n5.01 (0.352)\n5.94 (0.516)\n6.59 (0.636)\n5.84 (0.828)\nMedian [Min, Max]\n5.00 [4.30, 5.80]\n5.90 [4.90, 7.00]\n6.50 [4.90, 7.90]\n5.80 [4.30, 7.90]\nSepal.Width\n\n\n\n\nMean (SD)\n3.43 (0.379)\n2.77 (0.314)\n2.97 (0.322)\n3.06 (0.436)\nMedian [Min, Max]\n3.40 [2.30, 4.40]\n2.80 [2.00, 3.40]\n3.00 [2.20, 3.80]\n3.00 [2.00, 4.40]\nPetal.Length\n\n\n\n\nMean (SD)\n1.46 (0.174)\n4.26 (0.470)\n5.55 (0.552)\n3.76 (1.77)\nMedian [Min, Max]\n1.50 [1.00, 1.90]\n4.35 [3.00, 5.10]\n5.55 [4.50, 6.90]\n4.35 [1.00, 6.90]\nPetal.Width\n\n\n\n\nMean (SD)\n0.246 (0.105)\n1.33 (0.198)\n2.03 (0.275)\n1.20 (0.762)\nMedian [Min, Max]\n0.200 [0.100, 0.600]\n1.30 [1.00, 1.80]\n2.00 [1.40, 2.50]\n1.30 [0.100, 2.50]\n\n\n\n\n# plot relationship (what is happening here? )\nplot( Sepal.Length   ~ Sepal.Width , data = iris )\n\n\n\n\nmodel\n\n\nlibrary(\"tidyverse\")  # plotting\nlibrary(\"ggeffects\")  # plotting\nlibrary(\"ggplot2\")  # plotting\nlibrary(\"patchwork\") # arrange multiple plots\nlibrary(\"sjPlot\")  # tables and plots\n\n# basic model\nm1<- lm(Sepal.Length ~ Sepal.Width, data = iris)\nsummary(m1)\n\n\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5561 -0.6333 -0.1120  0.5579  2.2226 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   6.5262     0.4789   13.63   <2e-16 ***\nSepal.Width  -0.2234     0.1551   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8251 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\nresults\n\n\n# better summary method\nsjPlot::tab_model(m1)\n\n\n\n \n\n\nSepal.Length\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n6.53\n\n\n5.58 – 7.47\n\n\n<0.001\n\n\nSepal.Width\n\n\n-0.22\n\n\n-0.53 – 0.08\n\n\n0.152\n\n\nObservations\n\n\n150\n\n\nR2 / R2 adjusted\n\n\n0.014 / 0.007\n\n\n\n\n# plot the coefficients\nsjPlot::plot_model(m1)\n\n\n\n\ngraph predicted effects\n\n\n# plot the predicted relationship of Sepal Width on Sepal Length\np1 <- ggeffects::ggpredict(m1, terms = \"Sepal.Width\")\nplot(p1)\n\n\n\n\nWhat is the advantage of this graph?\n\n\npp1 <- plot(p1,\n            add.data = TRUE,\n            dot.alpha = .8,\n            jitter = .2)\npp1\n\n\n\n\ntry another model\n\n\nhead(iris)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nsummary (m2 <- lm(Petal.Length ~ Petal.Width, data = iris)) \n\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.33542 -0.30347 -0.02955  0.25776  1.39453 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.08356    0.07297   14.85   <2e-16 ***\nPetal.Width  2.22994    0.05140   43.39   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4782 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\n\npp2<-plot(\n  p2 <- ggeffects::ggpredict(m2, terms = \"Petal.Width\"),\n  add.data = TRUE,\n  dot.alpha = .8,\n  jitter = .2\n) \npp2\n\n\n\n\n\n\n## arange plots\nlibrary(patchwork)\npp1 / pp2 + plot_annotation(title = \"two plots\", tag_levels = \"i\")\n\n\n\n\nto be continued\n\n\n\n",
    "preview": "posts/2_1/syntax.png",
    "last_modified": "2021-03-02T19:39:22+13:00",
    "input_file": {},
    "preview_width": 1285,
    "preview_height": 478
  },
  {
    "path": "posts/1_1/",
    "title": "Course basics",
    "description": "Goals: (1) Download Rstudio (2) Get Git (3) Teach you the essentials of Rmarkdown (4) Integrate (1)-(3).",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-02-23",
    "categories": [],
    "contents": "\n\nContents\nTo do:\nDownload R\nDownload R-Studio desktop\nRead about Rmarkdown\nRecommended reading\nWho is this course for?\nHow will you benefit from this course?\n\nOur approach to teaching and learning\nWhat is R?\nIn a nutshell\nHistory\nPurpose\n\nWhat is R Studio?\nThe IDE\nA quick walk through R Studio\n\nGitHub\nWhat is Git/GitHub?\nInstall Git\nCreate a repository\nNext copy the location\nThen open a new project in Rstudio as a GitHub project\n\nThen open an Rmarkdown document, write and save it\nThen commit your Rmarkdown document by including a message, and “pushing” to GitHub\nNext commit!\nExtra information\n\nRMarkdown\nWhat is Rmarkdown?\nWhy is Rmarkdown useful?\nExtra information\nTips and tricks (JB)\nFinal thought: why quantitative psychology has to change (Why can’t I use SPSS, JK)\n\nFinal though\nLecture slides\nWeekly problem sets\nTask 1 Apply for a github education account\nTask 2 Download R for free here:\nTask 3 Download Rstudio for free here:\nTask 4 Create your first repository\nTask 5 Commit your first Rmarkdown journal\nGrading points\nSubmit a link to your journal on Blackboard.\n\n\nTo do:\nDownload R\nR is freely available for download at: https://www.r-project.org/ Please make sure that you have a current version of R-studio desktop on your machine\n\n\n\nDownload R-Studio desktop\nRstudio desktop is also freely available for download at:\nhttps://rstudio.com/products/rstudio/download/#download\nPlease make sure that you have the current version of R-studio desktop on your machine\n\n\n\nRead about Rmarkdown\nRead Danielle Navarro’s brief account of Rmarkdown here\nRecommended reading\nThis is a thorough rmarkdown workshop. You might quickly feel lost. Don’t worry about. We only expect you to acquire the basics this week.\nRmarkdown workshop\n\n\n\n\n\n\n\n\n\nWho is this course for?\nFor those of you who always wanted to learn R but never thought they could, this course is for you.\nHow will you benefit from this course?\nYou’ll learn how to use R, and Github, and we’ll teach you the fundamentals of statistics with a focus thinking not on rules.\nYou’ll learn how to learn – that is, how to obtain the resources you need to address a problem at hand.\nBy the end of the course, you’ll know how to:\nData skills:\nperform your data analysis in R\ndocument your analysis and collaborate in GitHub\ncreate a publication-ready article, with tables and graphs\ncreate a free personal website on github.\nStatistical skills:\nlearn the importance of knowing your question\nlearn how to collect data that bears on your question\nlearn how to explore your data visually\nlearn how to avoid common modelling pitfalls\nlearn how to improve your inference using multi-level models\nOur approach to teaching and learning\n\n\nThis course is designed to provide you with basic understanding, useful tips, and some guide rails for learning. Our main task is to give you the confidence, and the inspiration, for independent learning.\nWhat is R?\nIn a nutshell\nR is a free programming language and software environment for statistical computing (for download links see here: Windows, Mac).\nHistory\nR is the brainchild of Ross Ihaka and [Robert Gentleman](https://en.wikipedia.org/wiki/Robert_Gentleman_(statistician). It was created at the University of Auckland, where Ross Ihaka remains a professor of statistics.\nPurpose\nR was conceived to be a flexible language for data analysis usable by researchers. Since the initial beta release of R in 2000 the language has gained substantial popularity inside and outside of academia (have a look at this blog post for an interesting analysis). New versions of R are released periodically and can be downloaded and installed to replace the older R version.\nWhat is R Studio?\nThe IDE\nThe are many ways for using R on your computer. For the purposes of getting started, we will be using the Integrated Development Environment (IDE): R Studio.\nR Studio provides an interface with a number of user-friendly options, including a separate console and editor that has various help and syntax-auto-complete functions, and various tools for plotting, history, data visualization, debugging and work space management. It is important to remember that R and R Studio are not the same thing.\n\nA quick walk through R Studio\n\n\nGitHub\nWhat is Git/GitHub?\nGithub is a version control system. It is similar to Google docs, though for code. It is useful for collaboration because code easily breaks. It is only rarely possible to simultaneously work in real time on the same code because it will eventually break. Where and how is not easy to assess.\nA second function of GitHub is that it allows us to reconstruct histories of analysis. This is critical for open and reproducible science. This is the main function that we will be examining here.\nA third function, which pertains to single users, is that when writing code you can rewind and recover from your mistakes. This will save you a whole lot of time in the end.\nNote that GitHub has an interface with Rstudio. You will be using GitHub with Rstudio throughout this course.\nInstall Git\nWe suggested installing the educational version because this will allow you to have private repositories.\nIf you haven’t done that, but want to get started you can open a free account and retrospectively add your educational account later.\nPRO TIP Pick a user name that will be OK for professional purposes. If in doubt use your name.\nDirections for installing Git can be found http://github.com\nCreate a repository\nFirst create a repository on GitHub\n\n\n\nNext copy the location\n\n\n\nThen open a new project in Rstudio as a GitHub project\n\n\n\nThen open an Rmarkdown document, write and save it\nFirst, make sure that Rmarkdown is installed:\n\n\n# run this code\nif (!requireNamespace(\"devtools\"))\n  install.packages('devtools')\ndevtools::install_github('rstudio/rmarkdown')\n\n\n\nNext, create a document\n\n\n\nMake sure you save your document\nPress ⌘ + S  is the command for “save”\nThen commit your Rmarkdown document by including a message, and “pushing” to GitHub\nNote that we don’t want want to push .Rproj files to GitHub (this will mess up your collaborations), so I edited my .gitignore file.\nTypically you won’t want to be pushing large html files back and forth to GitHub (that can cause GitHub to freeze).\nYou can edit your gitingore file by adding a * like so:\n/*.html\nsee: https://git-scm.com/docs/gitig\nNext commit!\n\n\n\nVoila!\nExtra information\nJB’s recommendations for using Git and Rstudio\nThis is a very good tutorial on github and Rstudio: link\nVideo link\nA very brief setup video for Mac Users Link\nJK’s recommendations for using Git and Rstuio\nsetup\nRMarkdown\nWhat is Rmarkdown?\nIn the example above we breezed through Rmarkdown without exampling it. What is Rmarkdown?\nRmarkdown is a format for combining data-analysis with ordinary writing using a simple markup language.\nThe Rmarkdown code we used to write the opening paragraph looks like this.\n\ndiv.blue{ background-color:#e6f0ff}\n\n## To do\nRead Danielle Navarro's brief account of \nRmarkdown [here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThe:\n\n##\n\nmakes a heading. Then we write as we ordinarily would write:\n\nRead Danielle Navarro’s brief account of Rmarkdown\n\nand we include a link by typing\n\n[here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThink of rmarkdown as writing in word but without having to use your mouse all the time, and you can write up your analysis while writing in a one0stop shop.\nRmarkdown is just an efficient method for composing text without having to reach for your mouse, and a way of documenting and reporting your analysis\nWhy is Rmarkdown useful?\nRmarkdown merges two very powerful ideas: 1. R as a coding based tool to make your analysis repeatable; 2. markdown an approach to writing text that allows for the direct embedding of code output.\nThis is an immensely powerful approach that can be used for everything, from writing research papers, to writing complex technical documentation. This website is written Rmarkdown.\nYou will be creating a website similar to Johannes Karl’swebsite and Joseph Bulbulia’s websiteand you will do this by written using Rmarkdown).\nYou might think that writing in R markdown is only a nice technical trick for people really into coding, but in reality it addresses a central problem of statistical analysis.\nThe majority of errors in quantitative research papers (some meta-researchers indicate values as high as 80%) are human errors in transcribing values from the statistical software they are using to the final document (for a marginally entertaining story around this issue see this post).\nExtra information\ncoding tips Rmarkdown website\nJB’s recommendation for a very short introduction to Rmarkdown: https://rpubs.com/bpbond/626346\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips and tricks (JB)\nOne day, someone might ask you to collaborate in \\(\\LaTeX\\) (pronounced “Lay-Tek”). \\(\\LaTeX\\) is a document preparation system developed by Leslie Lamport in the 1980s that uses \\(\\TeX\\), a typesetting system that Donald Knuth developed in the 1970s to create mathematical documents. Writing in LaTeX is only a little more complicated than writing in markdown. For example, instead of writing # Heading, ## Subheading, ## Subsubheading, you would write \\section{Heading} \\subsection{Subheading}, \\subsubsection{Subsubheading}. However, the principles of mouseless composition that make Rmarkdown so nice, also make LaTeX nice. Rmarkdown shares features for bibliographic referencing with LaTeX that we’ll cover in later weeks. For now, since we are teaching you about Rmarkdown, we thought it’d be useful to teach about LaTeX too. Stay tuned for more.\nFinal thought: why quantitative psychology has to change (Why can’t I use SPSS, JK)\nQuantitative psychology has long struggled with replicability of it’s results both in substantive and also statistical areas. Concerns around these topics have already been raised on works by authors such as Joseph Banks Rhine the founder of modern parapsychology in the 1930s. Numerous authors, even at the time criticized both methods of the experiment and of the analysis [@gulliksenExtraSensoryPerceptionWhat1938]. In modern times, Deryl Bem’s article “Feeling the Future” that reported evidence in favor of Extra Sensory Perception revived this debate and led to an increased uptake of Open Science methods. Importantly, this is not only an issue in psychology, but instead affects all quantitative fields such as biology, chemistry, and physics. Out of the many issues that are addressed as part of the open science movement (if you are interested in getting active in it have a look at ANZORN) we will focus mostly on aspects of reproducability in analysis.\nUntil recently IBMs SPSS (Statistical Package for the Social Sciences), which originally launched in 1968 dominated the research space in psychology. If you never had the fortune of working with SPSS this is what it looked like:\n\n\n\nSPSS presented the user with a GUI (Graphical User Interface) through which they could run tests on their data. The big issue was that each statistical test has many different options researchers can choose (you will often hear people talk about researcher degrees of freedom) and a GUI makes it very difficult to accurately record every small setting a researcher has chosen. As a work around researchers could either store their output of the analysis which recorded some settings, but even for moderately complex analysis this output could stretch in the hundredth of pages. Alternatively, researchers could save the underlying code that SPSS used, but this was also very clunky and extremely arcane to understand. To give you a sense of scope below you see a snippet from a widely used analysis in SPSS aimed at examining the similarity of factor structures across groups. This code has a total of 130 lines that researchers would have needed to largely enter by hand and double check for any potential coding errors.\n\n\n\nAdditionally, some changes made by researchers were extremely difficult to account for. For example, when a researcher re-coded a variable say reversing its direction there was no way of knowing that this had taken place if you later looked at the data set. Together with the rise in complex analysis in psychology this has led to a steady decline in the use of SPSS and most psychology departments, as well as private, and governmental stake holder now require a certain fluency in R or similar coding based languages.\nFinal though\nR, Rstudio, GitHub, R-markdown \\(\\dots\\) these are just tools that fit our task\nOur main task in this course is to develop statistical intuition and workflows that will enable you to do better science.\nLecture slides\nClick here to go to the lecture slides.\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nWeekly problem sets\n** Note we will be working with you in the first week’s workshop to get you through this assessment task. We don’t expect that you will be able to do any of this before the first week.\nTask 1 Apply for a github education account\napply here: https://education.github.com/pack/offers\nset up a github account.\nchoose a username that will be OK for professional purposes (e.g. your name)\nTask 2 Download R for free here:\nhttps://www.r-project.org\nTask 3 Download Rstudio for free here:\nhttps://rstudio.com/products/rstudio/download/#download\nUpdate R and Rstudio to the latest versions. Don’t worry if you have problems, we’ll be working through your problems in the first week, and throughout the course. We assume no prior experience with R or Git/GitHub\nTask 4 Create your first repository\nTask 5 Commit your first Rmarkdown journal\nCreate your first Rmarkdown document, and title it “My Journal”\nRecord your impressions of the process of getting set up with RStudio and GitHub\nRecord any help you have offered to other people, and any help you have sought from other people, along the way.\nSave the document and commit it to GitHub.\nNote: set your github repository to private if you wish the account to be private\nGrading points\nYour Rmarkdown document must include each of the following\n\nHeading\n\n\ncrossed out text\n\n\nlink to a website\n\n\nA footnote 1\n\n\nAn inspirational quote\n\n\nBONUS: figure out how to add a bibliographic citation, such as [@darwin1964origin]\n\n\nA record of help you sought and offered.\n\nSubmit a link to your journal on Blackboard.\nBlackboard isn’t our favourite tool, however, we need to respect the norms of our community. For this reason, presently all of your assessments need to be uploaded through Blackboard.\nOn Blackboard, pass us a link to your GitHub document on Blackboard for weekly assessments\nThis journal will count as both your weekly journal entry and your weekly workbook – double credits!\n\nHumanities scholars love footnotes.↩︎\n",
    "preview": "posts/1_1/Rlogo.png",
    "last_modified": "2021-02-23T17:44:16+13:00",
    "input_file": {},
    "preview_width": 724,
    "preview_height": 561
  },
  {
    "path": "posts/1_2/",
    "title": "R basics",
    "description": "Some fundamentals",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-02-23",
    "categories": [],
    "contents": "\n\nContents\nFundamentals of RUsing R as calculator\nThe basic structure of R commands\nRecommended reading\n\n\nFundamentals of R\nA consoleruns all your code in R\nA source page runs all your code in R.\nA working directory is where R will look for raw data and other material.\nIn R-studio, (for starters) you can can use the file tab to import and save your material.\nSimilarly, in R-studio, (for starters) you can can use install packages by clicking the Install tab in the package pane (generally lower right pane) to install packagees.\nUsing R as calculator\nWe can use R as a calculator. You can run any mathematical operation you would normally use by entering it into the console:\n\n\n## Addition\nx = 3 + 2\nx\n\n\n[1] 5\n\n## Subtraction\nx = 3 - 2\nx\n\n\n[1] 1\n\n## Multiplication\nx = 3 * 2\nx\n\n\n[1] 6\n\n## Division\nx = 3 / 2\nx\n\n\n[1] 1.5\n\n## Modulus (Remainder from division)\nx = 3 %% 2\nx\n\n\n[1] 1\n\n## Exponentiation\nx = 3 ^ 2\nx\n\n\n[1] 9\n\n## Integer Division (Number of times denominator fits in numerator)\nx = 3 %/% 2\nx\n\n\n[1] 1\n\nThe basic structure of R commands\nWhile using R as calculator might be interesting, it does not get us very far in analysing our data.\nTo really unlock the full potential of R we first need to understand the basic structure of most R code and learn some terms.\nThe four main elements of every R code are:\nobjects,\nfunctions,\narguments\noperators.\nFigure 1 provides a simple example, that produces a new object which contains the mean of variable x.\n\n\n\nFigure 1: The Basic Syntax of R\n\n\n\nThe function mean generates the arithmetic mean of an input object.\nThe object needs to be specified inside of the function brackets as the x argument, in this case we define x = df .\nLast, we assign the result of this function for later us via the <- operator to an object which we decided to call object.\nIn other words, we create a new object that can be further manipulated and contains information about the mean of a previously created object “x”. This structure represents the foundation of most operations in R.\nInstead calling the mean function as above one could manually add all values of x dividing it by the number of x values.\nNevertheless, this would be very cumbersome.\nFor this reason, functions (pre-assembled lines of code) exist to reduce the amount of coding necessary.\nThese functions can be bundled into packages. R’s capacity for creating packages is main appeal of R as a statistical tool because community developed functions are available from a central repository called CRAN in the form of packages.\nThese packages can be installed in R with the command install.packages(“package name”).\nIt is important that you only need to install a package once on your machine, expect if you want to upgrade the package. Generally speaking you regularly want to upgrade your packages, but keep a permanent note in your code which version of a package you used when it was initially written.\nRecommended reading\nAn introduction to R and Rstudio\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-19T18:35:55+13:00",
    "input_file": {}
  },
  {
    "path": "posts/1_3/",
    "title": "Set up",
    "description": "The purpose of this week is to get you set up to write in R markdown",
    "author": [
      {
        "name": "Joseph Bulbulia",
        "url": "https://josephbulbulia.netlify.app"
      },
      {
        "name": "Johannes Karl",
        "url": "https://johannes-karl.com"
      }
    ],
    "date": "2021-02-23",
    "categories": [],
    "contents": "\n\nContents\nTO DO\nMarkdown is a format for writing\nRmarkdown and You\nHow to keep track of everything\nSome useful sites\n\nTO DO\nRead Daniell Navarro’s brief account of Rmarkdown here\nMarkdown is a format for writing\nRmarkdown is a format for combing code with ordinary writing.\nThe Rmarkdown code we used to write the opening paragraph looks like this.\n\ndiv.blue{ background-color:#e6f0ff}\n\n## TO DO\nRead Danielle Navarro's brief account of \nRmarkdown [here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThe:\n\n##\n\nmakes a heading. We write as ordinary,\n\nRead Danielle Navarro’s brief account of Rmarkdown\n\nand we include a link by typing\n\n[here](https://slides.djnavarro.net/starting-rmarkdown/#8)\n\nThink of rmarkdown as writing in word but without having to use your mouse all the time. The really great thing about Rmarkdown is that you can write document and do the analysis in a single stop shop. Figure 1 shows Rmarkdown in the rconsole (upper left).\n\n\n\nFigure 1: Screenshot of Rmarkdown document (upper left)\n\n\n\nRmarkdown and You\nRmarkdown merges two very powerful ideas: 1. R as a coding based tool to make your analysis repeatable; 2. markdown an approach to writing text that allows for the direct embedding of code output.\nThis is an immensely powerful approach that can be used for everything, from writing research papers, to writing complex technical documentation. This website is written Rmarkdown.\nYou will be creating a website similar to Johannes Karl’swebsite and Joseph Bulbulia’s websiteand you will do this by written using Rmarkdown).\nYou might think that writing in R markdown is only a nice technical trick for people really into coding, but in reality it addresses a central problem of statistical analysis.\nThe majority of errors in quantitative research papers (some meta-researchers indiate values as high as 80%) are human errors in transcribing values from the statistical software they are using to the final document (for a marginally entertaining story around this issue see this post).\nHow to keep track of everything\nNow that we have our repeatable code, our repeatable document, the last thing we need is a transparent way to document what we are doing and share with others. For that we come to our last tool that in a similar confusing way to R and Rstudio is split in to parts; git and Github.\nSome useful sites\ncoding tips Rmarkdown website\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-19T18:36:56+13:00",
    "input_file": {}
  }
]
